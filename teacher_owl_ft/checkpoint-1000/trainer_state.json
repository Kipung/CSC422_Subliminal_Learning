{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.8726591760299627,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0018726591760299626,
      "grad_norm": 1.3026269674301147,
      "learning_rate": 0.0,
      "loss": 1.2554,
      "step": 1
    },
    {
      "epoch": 0.003745318352059925,
      "grad_norm": 1.4449772834777832,
      "learning_rate": 3.1055900621118013e-07,
      "loss": 1.1554,
      "step": 2
    },
    {
      "epoch": 0.0056179775280898875,
      "grad_norm": 1.4774997234344482,
      "learning_rate": 6.211180124223603e-07,
      "loss": 1.1756,
      "step": 3
    },
    {
      "epoch": 0.00749063670411985,
      "grad_norm": 1.5316052436828613,
      "learning_rate": 9.316770186335405e-07,
      "loss": 1.2872,
      "step": 4
    },
    {
      "epoch": 0.009363295880149813,
      "grad_norm": 1.4776418209075928,
      "learning_rate": 1.2422360248447205e-06,
      "loss": 1.1251,
      "step": 5
    },
    {
      "epoch": 0.011235955056179775,
      "grad_norm": 1.3048932552337646,
      "learning_rate": 1.5527950310559006e-06,
      "loss": 1.195,
      "step": 6
    },
    {
      "epoch": 0.013108614232209739,
      "grad_norm": 1.3724702596664429,
      "learning_rate": 1.863354037267081e-06,
      "loss": 1.1439,
      "step": 7
    },
    {
      "epoch": 0.0149812734082397,
      "grad_norm": 1.438194990158081,
      "learning_rate": 2.173913043478261e-06,
      "loss": 1.1821,
      "step": 8
    },
    {
      "epoch": 0.016853932584269662,
      "grad_norm": 1.3487805128097534,
      "learning_rate": 2.484472049689441e-06,
      "loss": 1.1439,
      "step": 9
    },
    {
      "epoch": 0.018726591760299626,
      "grad_norm": 1.4586468935012817,
      "learning_rate": 2.7950310559006214e-06,
      "loss": 1.1292,
      "step": 10
    },
    {
      "epoch": 0.020599250936329586,
      "grad_norm": 1.3570560216903687,
      "learning_rate": 3.1055900621118013e-06,
      "loss": 1.1956,
      "step": 11
    },
    {
      "epoch": 0.02247191011235955,
      "grad_norm": 1.2898938655853271,
      "learning_rate": 3.4161490683229816e-06,
      "loss": 1.2871,
      "step": 12
    },
    {
      "epoch": 0.024344569288389514,
      "grad_norm": 1.3221269845962524,
      "learning_rate": 3.726708074534162e-06,
      "loss": 1.1836,
      "step": 13
    },
    {
      "epoch": 0.026217228464419477,
      "grad_norm": 1.3561272621154785,
      "learning_rate": 4.037267080745342e-06,
      "loss": 1.0835,
      "step": 14
    },
    {
      "epoch": 0.028089887640449437,
      "grad_norm": 1.2948007583618164,
      "learning_rate": 4.347826086956522e-06,
      "loss": 1.0778,
      "step": 15
    },
    {
      "epoch": 0.0299625468164794,
      "grad_norm": 1.3581706285476685,
      "learning_rate": 4.658385093167702e-06,
      "loss": 1.2007,
      "step": 16
    },
    {
      "epoch": 0.031835205992509365,
      "grad_norm": 1.5045963525772095,
      "learning_rate": 4.968944099378882e-06,
      "loss": 1.2374,
      "step": 17
    },
    {
      "epoch": 0.033707865168539325,
      "grad_norm": 1.688643217086792,
      "learning_rate": 5.279503105590062e-06,
      "loss": 1.1205,
      "step": 18
    },
    {
      "epoch": 0.035580524344569285,
      "grad_norm": 1.3835513591766357,
      "learning_rate": 5.590062111801243e-06,
      "loss": 1.2287,
      "step": 19
    },
    {
      "epoch": 0.03745318352059925,
      "grad_norm": 1.4592604637145996,
      "learning_rate": 5.900621118012423e-06,
      "loss": 1.1921,
      "step": 20
    },
    {
      "epoch": 0.03932584269662921,
      "grad_norm": 1.3836994171142578,
      "learning_rate": 6.2111801242236025e-06,
      "loss": 1.1536,
      "step": 21
    },
    {
      "epoch": 0.04119850187265917,
      "grad_norm": 1.3439030647277832,
      "learning_rate": 6.521739130434783e-06,
      "loss": 1.1643,
      "step": 22
    },
    {
      "epoch": 0.04307116104868914,
      "grad_norm": 1.428924798965454,
      "learning_rate": 6.832298136645963e-06,
      "loss": 1.144,
      "step": 23
    },
    {
      "epoch": 0.0449438202247191,
      "grad_norm": 1.4606187343597412,
      "learning_rate": 7.142857142857143e-06,
      "loss": 1.0173,
      "step": 24
    },
    {
      "epoch": 0.04681647940074907,
      "grad_norm": 1.5080071687698364,
      "learning_rate": 7.453416149068324e-06,
      "loss": 1.2091,
      "step": 25
    },
    {
      "epoch": 0.04868913857677903,
      "grad_norm": 1.290763020515442,
      "learning_rate": 7.763975155279503e-06,
      "loss": 1.0059,
      "step": 26
    },
    {
      "epoch": 0.05056179775280899,
      "grad_norm": 1.4387352466583252,
      "learning_rate": 8.074534161490684e-06,
      "loss": 1.0903,
      "step": 27
    },
    {
      "epoch": 0.052434456928838954,
      "grad_norm": 1.3046436309814453,
      "learning_rate": 8.385093167701864e-06,
      "loss": 1.1316,
      "step": 28
    },
    {
      "epoch": 0.054307116104868915,
      "grad_norm": 1.3761404752731323,
      "learning_rate": 8.695652173913044e-06,
      "loss": 1.2076,
      "step": 29
    },
    {
      "epoch": 0.056179775280898875,
      "grad_norm": 1.3914835453033447,
      "learning_rate": 9.006211180124225e-06,
      "loss": 1.1758,
      "step": 30
    },
    {
      "epoch": 0.05805243445692884,
      "grad_norm": 1.5163521766662598,
      "learning_rate": 9.316770186335403e-06,
      "loss": 1.175,
      "step": 31
    },
    {
      "epoch": 0.0599250936329588,
      "grad_norm": 1.4430779218673706,
      "learning_rate": 9.627329192546584e-06,
      "loss": 1.1942,
      "step": 32
    },
    {
      "epoch": 0.06179775280898876,
      "grad_norm": 1.3843475580215454,
      "learning_rate": 9.937888198757764e-06,
      "loss": 1.2199,
      "step": 33
    },
    {
      "epoch": 0.06367041198501873,
      "grad_norm": 1.3691433668136597,
      "learning_rate": 1.0248447204968944e-05,
      "loss": 1.2084,
      "step": 34
    },
    {
      "epoch": 0.06554307116104868,
      "grad_norm": 1.3402478694915771,
      "learning_rate": 1.0559006211180125e-05,
      "loss": 1.2541,
      "step": 35
    },
    {
      "epoch": 0.06741573033707865,
      "grad_norm": 1.270963191986084,
      "learning_rate": 1.0869565217391305e-05,
      "loss": 1.2303,
      "step": 36
    },
    {
      "epoch": 0.06928838951310862,
      "grad_norm": 1.4909944534301758,
      "learning_rate": 1.1180124223602485e-05,
      "loss": 1.0853,
      "step": 37
    },
    {
      "epoch": 0.07116104868913857,
      "grad_norm": 1.5540560483932495,
      "learning_rate": 1.1490683229813664e-05,
      "loss": 1.1197,
      "step": 38
    },
    {
      "epoch": 0.07303370786516854,
      "grad_norm": 1.4371951818466187,
      "learning_rate": 1.1801242236024846e-05,
      "loss": 1.1924,
      "step": 39
    },
    {
      "epoch": 0.0749063670411985,
      "grad_norm": 1.4621436595916748,
      "learning_rate": 1.2111801242236026e-05,
      "loss": 1.191,
      "step": 40
    },
    {
      "epoch": 0.07677902621722846,
      "grad_norm": 1.5579149723052979,
      "learning_rate": 1.2422360248447205e-05,
      "loss": 1.1832,
      "step": 41
    },
    {
      "epoch": 0.07865168539325842,
      "grad_norm": 1.2884191274642944,
      "learning_rate": 1.2732919254658385e-05,
      "loss": 1.0879,
      "step": 42
    },
    {
      "epoch": 0.08052434456928839,
      "grad_norm": 1.4170233011245728,
      "learning_rate": 1.3043478260869566e-05,
      "loss": 1.1784,
      "step": 43
    },
    {
      "epoch": 0.08239700374531835,
      "grad_norm": 1.6299399137496948,
      "learning_rate": 1.3354037267080746e-05,
      "loss": 1.0794,
      "step": 44
    },
    {
      "epoch": 0.08426966292134831,
      "grad_norm": 1.499181866645813,
      "learning_rate": 1.3664596273291926e-05,
      "loss": 1.2622,
      "step": 45
    },
    {
      "epoch": 0.08614232209737828,
      "grad_norm": 1.492141604423523,
      "learning_rate": 1.3975155279503105e-05,
      "loss": 1.2493,
      "step": 46
    },
    {
      "epoch": 0.08801498127340825,
      "grad_norm": 1.4679068326950073,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 1.0848,
      "step": 47
    },
    {
      "epoch": 0.0898876404494382,
      "grad_norm": 1.5417423248291016,
      "learning_rate": 1.4596273291925466e-05,
      "loss": 1.1791,
      "step": 48
    },
    {
      "epoch": 0.09176029962546817,
      "grad_norm": 1.5521010160446167,
      "learning_rate": 1.4906832298136648e-05,
      "loss": 1.1683,
      "step": 49
    },
    {
      "epoch": 0.09363295880149813,
      "grad_norm": 1.5508254766464233,
      "learning_rate": 1.5217391304347828e-05,
      "loss": 1.2034,
      "step": 50
    },
    {
      "epoch": 0.09550561797752809,
      "grad_norm": 1.54826021194458,
      "learning_rate": 1.5527950310559007e-05,
      "loss": 1.0715,
      "step": 51
    },
    {
      "epoch": 0.09737827715355805,
      "grad_norm": 1.5275466442108154,
      "learning_rate": 1.5838509316770185e-05,
      "loss": 1.2104,
      "step": 52
    },
    {
      "epoch": 0.09925093632958802,
      "grad_norm": 1.7163398265838623,
      "learning_rate": 1.6149068322981367e-05,
      "loss": 1.2068,
      "step": 53
    },
    {
      "epoch": 0.10112359550561797,
      "grad_norm": 1.574834942817688,
      "learning_rate": 1.645962732919255e-05,
      "loss": 1.2286,
      "step": 54
    },
    {
      "epoch": 0.10299625468164794,
      "grad_norm": 1.4743047952651978,
      "learning_rate": 1.6770186335403728e-05,
      "loss": 1.2446,
      "step": 55
    },
    {
      "epoch": 0.10486891385767791,
      "grad_norm": 1.3758811950683594,
      "learning_rate": 1.7080745341614907e-05,
      "loss": 1.1601,
      "step": 56
    },
    {
      "epoch": 0.10674157303370786,
      "grad_norm": 1.4019018411636353,
      "learning_rate": 1.739130434782609e-05,
      "loss": 1.1465,
      "step": 57
    },
    {
      "epoch": 0.10861423220973783,
      "grad_norm": 1.414319396018982,
      "learning_rate": 1.7701863354037267e-05,
      "loss": 1.0981,
      "step": 58
    },
    {
      "epoch": 0.1104868913857678,
      "grad_norm": 1.4188764095306396,
      "learning_rate": 1.801242236024845e-05,
      "loss": 1.1728,
      "step": 59
    },
    {
      "epoch": 0.11235955056179775,
      "grad_norm": 1.4120042324066162,
      "learning_rate": 1.8322981366459628e-05,
      "loss": 1.1279,
      "step": 60
    },
    {
      "epoch": 0.11423220973782772,
      "grad_norm": 1.5767236948013306,
      "learning_rate": 1.8633540372670807e-05,
      "loss": 1.1731,
      "step": 61
    },
    {
      "epoch": 0.11610486891385768,
      "grad_norm": 1.616288185119629,
      "learning_rate": 1.894409937888199e-05,
      "loss": 1.1932,
      "step": 62
    },
    {
      "epoch": 0.11797752808988764,
      "grad_norm": 1.5308669805526733,
      "learning_rate": 1.9254658385093167e-05,
      "loss": 1.1789,
      "step": 63
    },
    {
      "epoch": 0.1198501872659176,
      "grad_norm": 1.4565783739089966,
      "learning_rate": 1.956521739130435e-05,
      "loss": 1.2051,
      "step": 64
    },
    {
      "epoch": 0.12172284644194757,
      "grad_norm": 1.4824532270431519,
      "learning_rate": 1.9875776397515528e-05,
      "loss": 1.0331,
      "step": 65
    },
    {
      "epoch": 0.12359550561797752,
      "grad_norm": 1.6640753746032715,
      "learning_rate": 2.0186335403726707e-05,
      "loss": 1.2792,
      "step": 66
    },
    {
      "epoch": 0.1254681647940075,
      "grad_norm": 1.5573160648345947,
      "learning_rate": 2.049689440993789e-05,
      "loss": 1.0555,
      "step": 67
    },
    {
      "epoch": 0.12734082397003746,
      "grad_norm": 1.4493117332458496,
      "learning_rate": 2.080745341614907e-05,
      "loss": 1.2052,
      "step": 68
    },
    {
      "epoch": 0.12921348314606743,
      "grad_norm": 1.5892958641052246,
      "learning_rate": 2.111801242236025e-05,
      "loss": 1.1007,
      "step": 69
    },
    {
      "epoch": 0.13108614232209737,
      "grad_norm": 1.5649909973144531,
      "learning_rate": 2.1428571428571428e-05,
      "loss": 1.169,
      "step": 70
    },
    {
      "epoch": 0.13295880149812733,
      "grad_norm": 1.4347357749938965,
      "learning_rate": 2.173913043478261e-05,
      "loss": 1.1051,
      "step": 71
    },
    {
      "epoch": 0.1348314606741573,
      "grad_norm": 1.397554636001587,
      "learning_rate": 2.204968944099379e-05,
      "loss": 1.1832,
      "step": 72
    },
    {
      "epoch": 0.13670411985018727,
      "grad_norm": 1.6099340915679932,
      "learning_rate": 2.236024844720497e-05,
      "loss": 1.0834,
      "step": 73
    },
    {
      "epoch": 0.13857677902621723,
      "grad_norm": 1.5594242811203003,
      "learning_rate": 2.2670807453416153e-05,
      "loss": 1.1483,
      "step": 74
    },
    {
      "epoch": 0.1404494382022472,
      "grad_norm": 1.5925356149673462,
      "learning_rate": 2.2981366459627328e-05,
      "loss": 1.1906,
      "step": 75
    },
    {
      "epoch": 0.14232209737827714,
      "grad_norm": 1.4679605960845947,
      "learning_rate": 2.329192546583851e-05,
      "loss": 1.2342,
      "step": 76
    },
    {
      "epoch": 0.1441947565543071,
      "grad_norm": 1.3723108768463135,
      "learning_rate": 2.3602484472049692e-05,
      "loss": 1.091,
      "step": 77
    },
    {
      "epoch": 0.14606741573033707,
      "grad_norm": 1.3349108695983887,
      "learning_rate": 2.391304347826087e-05,
      "loss": 1.125,
      "step": 78
    },
    {
      "epoch": 0.14794007490636704,
      "grad_norm": 1.4154669046401978,
      "learning_rate": 2.4223602484472053e-05,
      "loss": 1.223,
      "step": 79
    },
    {
      "epoch": 0.149812734082397,
      "grad_norm": 1.4821884632110596,
      "learning_rate": 2.453416149068323e-05,
      "loss": 1.168,
      "step": 80
    },
    {
      "epoch": 0.15168539325842698,
      "grad_norm": 1.5733778476715088,
      "learning_rate": 2.484472049689441e-05,
      "loss": 1.0796,
      "step": 81
    },
    {
      "epoch": 0.15355805243445692,
      "grad_norm": 1.6458569765090942,
      "learning_rate": 2.515527950310559e-05,
      "loss": 1.1422,
      "step": 82
    },
    {
      "epoch": 0.15543071161048688,
      "grad_norm": 1.3965171575546265,
      "learning_rate": 2.546583850931677e-05,
      "loss": 1.129,
      "step": 83
    },
    {
      "epoch": 0.15730337078651685,
      "grad_norm": 1.5425337553024292,
      "learning_rate": 2.577639751552795e-05,
      "loss": 1.2018,
      "step": 84
    },
    {
      "epoch": 0.15917602996254682,
      "grad_norm": 1.3868314027786255,
      "learning_rate": 2.608695652173913e-05,
      "loss": 1.257,
      "step": 85
    },
    {
      "epoch": 0.16104868913857678,
      "grad_norm": 1.5391466617584229,
      "learning_rate": 2.639751552795031e-05,
      "loss": 1.1584,
      "step": 86
    },
    {
      "epoch": 0.16292134831460675,
      "grad_norm": 1.5018256902694702,
      "learning_rate": 2.6708074534161492e-05,
      "loss": 1.1513,
      "step": 87
    },
    {
      "epoch": 0.1647940074906367,
      "grad_norm": 1.3616445064544678,
      "learning_rate": 2.7018633540372674e-05,
      "loss": 1.0657,
      "step": 88
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.514312982559204,
      "learning_rate": 2.7329192546583853e-05,
      "loss": 1.2021,
      "step": 89
    },
    {
      "epoch": 0.16853932584269662,
      "grad_norm": 1.4081169366836548,
      "learning_rate": 2.7639751552795035e-05,
      "loss": 1.161,
      "step": 90
    },
    {
      "epoch": 0.1704119850187266,
      "grad_norm": 1.6131020784378052,
      "learning_rate": 2.795031055900621e-05,
      "loss": 1.1177,
      "step": 91
    },
    {
      "epoch": 0.17228464419475656,
      "grad_norm": 1.5070279836654663,
      "learning_rate": 2.826086956521739e-05,
      "loss": 1.1867,
      "step": 92
    },
    {
      "epoch": 0.17415730337078653,
      "grad_norm": 1.6042367219924927,
      "learning_rate": 2.857142857142857e-05,
      "loss": 1.1774,
      "step": 93
    },
    {
      "epoch": 0.1760299625468165,
      "grad_norm": 1.6082099676132202,
      "learning_rate": 2.8881987577639753e-05,
      "loss": 1.216,
      "step": 94
    },
    {
      "epoch": 0.17790262172284643,
      "grad_norm": 1.5297681093215942,
      "learning_rate": 2.919254658385093e-05,
      "loss": 1.1589,
      "step": 95
    },
    {
      "epoch": 0.1797752808988764,
      "grad_norm": 1.487549901008606,
      "learning_rate": 2.9503105590062114e-05,
      "loss": 1.1931,
      "step": 96
    },
    {
      "epoch": 0.18164794007490637,
      "grad_norm": 1.5287202596664429,
      "learning_rate": 2.9813664596273296e-05,
      "loss": 1.124,
      "step": 97
    },
    {
      "epoch": 0.18352059925093633,
      "grad_norm": 1.8432048559188843,
      "learning_rate": 3.0124223602484474e-05,
      "loss": 1.1663,
      "step": 98
    },
    {
      "epoch": 0.1853932584269663,
      "grad_norm": 1.6610244512557983,
      "learning_rate": 3.0434782608695656e-05,
      "loss": 1.1668,
      "step": 99
    },
    {
      "epoch": 0.18726591760299627,
      "grad_norm": 1.5565853118896484,
      "learning_rate": 3.074534161490684e-05,
      "loss": 1.2489,
      "step": 100
    },
    {
      "epoch": 0.1891385767790262,
      "grad_norm": 1.394884705543518,
      "learning_rate": 3.1055900621118014e-05,
      "loss": 1.0734,
      "step": 101
    },
    {
      "epoch": 0.19101123595505617,
      "grad_norm": 1.5688456296920776,
      "learning_rate": 3.136645962732919e-05,
      "loss": 1.1009,
      "step": 102
    },
    {
      "epoch": 0.19288389513108614,
      "grad_norm": 1.5593910217285156,
      "learning_rate": 3.167701863354037e-05,
      "loss": 1.1859,
      "step": 103
    },
    {
      "epoch": 0.1947565543071161,
      "grad_norm": 1.3805791139602661,
      "learning_rate": 3.198757763975155e-05,
      "loss": 1.1447,
      "step": 104
    },
    {
      "epoch": 0.19662921348314608,
      "grad_norm": 1.4909158945083618,
      "learning_rate": 3.2298136645962735e-05,
      "loss": 1.2199,
      "step": 105
    },
    {
      "epoch": 0.19850187265917604,
      "grad_norm": 1.3227345943450928,
      "learning_rate": 3.260869565217392e-05,
      "loss": 1.2239,
      "step": 106
    },
    {
      "epoch": 0.20037453183520598,
      "grad_norm": 1.3647398948669434,
      "learning_rate": 3.29192546583851e-05,
      "loss": 1.2074,
      "step": 107
    },
    {
      "epoch": 0.20224719101123595,
      "grad_norm": 1.5704591274261475,
      "learning_rate": 3.3229813664596274e-05,
      "loss": 1.216,
      "step": 108
    },
    {
      "epoch": 0.20411985018726592,
      "grad_norm": 1.520886778831482,
      "learning_rate": 3.3540372670807456e-05,
      "loss": 1.0867,
      "step": 109
    },
    {
      "epoch": 0.20599250936329588,
      "grad_norm": 1.2778754234313965,
      "learning_rate": 3.385093167701863e-05,
      "loss": 1.0954,
      "step": 110
    },
    {
      "epoch": 0.20786516853932585,
      "grad_norm": 1.5520150661468506,
      "learning_rate": 3.4161490683229814e-05,
      "loss": 1.2193,
      "step": 111
    },
    {
      "epoch": 0.20973782771535582,
      "grad_norm": 1.5812016725540161,
      "learning_rate": 3.4472049689440996e-05,
      "loss": 1.1769,
      "step": 112
    },
    {
      "epoch": 0.21161048689138576,
      "grad_norm": 1.5317151546478271,
      "learning_rate": 3.478260869565218e-05,
      "loss": 1.211,
      "step": 113
    },
    {
      "epoch": 0.21348314606741572,
      "grad_norm": 1.4736088514328003,
      "learning_rate": 3.509316770186335e-05,
      "loss": 1.2138,
      "step": 114
    },
    {
      "epoch": 0.2153558052434457,
      "grad_norm": 1.4762110710144043,
      "learning_rate": 3.5403726708074535e-05,
      "loss": 1.1394,
      "step": 115
    },
    {
      "epoch": 0.21722846441947566,
      "grad_norm": 1.3830227851867676,
      "learning_rate": 3.571428571428572e-05,
      "loss": 1.1338,
      "step": 116
    },
    {
      "epoch": 0.21910112359550563,
      "grad_norm": 1.4477558135986328,
      "learning_rate": 3.60248447204969e-05,
      "loss": 1.1757,
      "step": 117
    },
    {
      "epoch": 0.2209737827715356,
      "grad_norm": 1.396102786064148,
      "learning_rate": 3.633540372670808e-05,
      "loss": 1.0905,
      "step": 118
    },
    {
      "epoch": 0.22284644194756553,
      "grad_norm": 1.4930384159088135,
      "learning_rate": 3.6645962732919256e-05,
      "loss": 1.1597,
      "step": 119
    },
    {
      "epoch": 0.2247191011235955,
      "grad_norm": 1.614054560661316,
      "learning_rate": 3.695652173913043e-05,
      "loss": 1.2301,
      "step": 120
    },
    {
      "epoch": 0.22659176029962547,
      "grad_norm": 1.5976333618164062,
      "learning_rate": 3.7267080745341614e-05,
      "loss": 1.1827,
      "step": 121
    },
    {
      "epoch": 0.22846441947565543,
      "grad_norm": 1.5226333141326904,
      "learning_rate": 3.7577639751552796e-05,
      "loss": 1.1788,
      "step": 122
    },
    {
      "epoch": 0.2303370786516854,
      "grad_norm": 1.5830849409103394,
      "learning_rate": 3.788819875776398e-05,
      "loss": 1.2305,
      "step": 123
    },
    {
      "epoch": 0.23220973782771537,
      "grad_norm": 1.415382981300354,
      "learning_rate": 3.819875776397516e-05,
      "loss": 1.2432,
      "step": 124
    },
    {
      "epoch": 0.2340823970037453,
      "grad_norm": 1.511003851890564,
      "learning_rate": 3.8509316770186335e-05,
      "loss": 1.189,
      "step": 125
    },
    {
      "epoch": 0.23595505617977527,
      "grad_norm": 1.4866615533828735,
      "learning_rate": 3.881987577639752e-05,
      "loss": 1.2673,
      "step": 126
    },
    {
      "epoch": 0.23782771535580524,
      "grad_norm": 1.470435380935669,
      "learning_rate": 3.91304347826087e-05,
      "loss": 1.0441,
      "step": 127
    },
    {
      "epoch": 0.2397003745318352,
      "grad_norm": 1.383588194847107,
      "learning_rate": 3.944099378881988e-05,
      "loss": 1.1723,
      "step": 128
    },
    {
      "epoch": 0.24157303370786518,
      "grad_norm": 1.2656447887420654,
      "learning_rate": 3.9751552795031056e-05,
      "loss": 1.1381,
      "step": 129
    },
    {
      "epoch": 0.24344569288389514,
      "grad_norm": 1.4384419918060303,
      "learning_rate": 4.006211180124224e-05,
      "loss": 1.1325,
      "step": 130
    },
    {
      "epoch": 0.24531835205992508,
      "grad_norm": 1.4772992134094238,
      "learning_rate": 4.0372670807453414e-05,
      "loss": 1.2289,
      "step": 131
    },
    {
      "epoch": 0.24719101123595505,
      "grad_norm": 1.5507723093032837,
      "learning_rate": 4.0683229813664596e-05,
      "loss": 1.149,
      "step": 132
    },
    {
      "epoch": 0.24906367041198502,
      "grad_norm": 1.5904213190078735,
      "learning_rate": 4.099378881987578e-05,
      "loss": 1.1118,
      "step": 133
    },
    {
      "epoch": 0.250936329588015,
      "grad_norm": 1.471490502357483,
      "learning_rate": 4.130434782608696e-05,
      "loss": 1.1528,
      "step": 134
    },
    {
      "epoch": 0.25280898876404495,
      "grad_norm": 1.5965917110443115,
      "learning_rate": 4.161490683229814e-05,
      "loss": 1.1586,
      "step": 135
    },
    {
      "epoch": 0.2546816479400749,
      "grad_norm": 1.514224648475647,
      "learning_rate": 4.192546583850932e-05,
      "loss": 1.1026,
      "step": 136
    },
    {
      "epoch": 0.2565543071161049,
      "grad_norm": 1.6960163116455078,
      "learning_rate": 4.22360248447205e-05,
      "loss": 1.187,
      "step": 137
    },
    {
      "epoch": 0.25842696629213485,
      "grad_norm": 1.7132759094238281,
      "learning_rate": 4.254658385093168e-05,
      "loss": 1.24,
      "step": 138
    },
    {
      "epoch": 0.2602996254681648,
      "grad_norm": 1.7442761659622192,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 1.1824,
      "step": 139
    },
    {
      "epoch": 0.26217228464419473,
      "grad_norm": 1.5273418426513672,
      "learning_rate": 4.316770186335404e-05,
      "loss": 1.2839,
      "step": 140
    },
    {
      "epoch": 0.2640449438202247,
      "grad_norm": 1.551336407661438,
      "learning_rate": 4.347826086956522e-05,
      "loss": 1.2002,
      "step": 141
    },
    {
      "epoch": 0.26591760299625467,
      "grad_norm": 1.4246649742126465,
      "learning_rate": 4.3788819875776396e-05,
      "loss": 1.0359,
      "step": 142
    },
    {
      "epoch": 0.26779026217228463,
      "grad_norm": 1.5037660598754883,
      "learning_rate": 4.409937888198758e-05,
      "loss": 1.1637,
      "step": 143
    },
    {
      "epoch": 0.2696629213483146,
      "grad_norm": 1.5252467393875122,
      "learning_rate": 4.440993788819876e-05,
      "loss": 1.2384,
      "step": 144
    },
    {
      "epoch": 0.27153558052434457,
      "grad_norm": 1.5221097469329834,
      "learning_rate": 4.472049689440994e-05,
      "loss": 1.2156,
      "step": 145
    },
    {
      "epoch": 0.27340823970037453,
      "grad_norm": 1.450116753578186,
      "learning_rate": 4.5031055900621124e-05,
      "loss": 1.1505,
      "step": 146
    },
    {
      "epoch": 0.2752808988764045,
      "grad_norm": 1.3867359161376953,
      "learning_rate": 4.5341614906832306e-05,
      "loss": 1.2125,
      "step": 147
    },
    {
      "epoch": 0.27715355805243447,
      "grad_norm": 1.4758037328720093,
      "learning_rate": 4.565217391304348e-05,
      "loss": 1.3001,
      "step": 148
    },
    {
      "epoch": 0.27902621722846443,
      "grad_norm": 1.4616174697875977,
      "learning_rate": 4.5962732919254656e-05,
      "loss": 1.1267,
      "step": 149
    },
    {
      "epoch": 0.2808988764044944,
      "grad_norm": 1.4771562814712524,
      "learning_rate": 4.627329192546584e-05,
      "loss": 1.0386,
      "step": 150
    },
    {
      "epoch": 0.28277153558052437,
      "grad_norm": 1.504596471786499,
      "learning_rate": 4.658385093167702e-05,
      "loss": 1.119,
      "step": 151
    },
    {
      "epoch": 0.2846441947565543,
      "grad_norm": 1.5628637075424194,
      "learning_rate": 4.68944099378882e-05,
      "loss": 1.1706,
      "step": 152
    },
    {
      "epoch": 0.28651685393258425,
      "grad_norm": 1.522675633430481,
      "learning_rate": 4.7204968944099384e-05,
      "loss": 1.0485,
      "step": 153
    },
    {
      "epoch": 0.2883895131086142,
      "grad_norm": 1.6288747787475586,
      "learning_rate": 4.751552795031056e-05,
      "loss": 1.2206,
      "step": 154
    },
    {
      "epoch": 0.2902621722846442,
      "grad_norm": 1.5163813829421997,
      "learning_rate": 4.782608695652174e-05,
      "loss": 1.0293,
      "step": 155
    },
    {
      "epoch": 0.29213483146067415,
      "grad_norm": 1.5933566093444824,
      "learning_rate": 4.8136645962732924e-05,
      "loss": 1.1337,
      "step": 156
    },
    {
      "epoch": 0.2940074906367041,
      "grad_norm": 1.5962384939193726,
      "learning_rate": 4.8447204968944106e-05,
      "loss": 1.0383,
      "step": 157
    },
    {
      "epoch": 0.2958801498127341,
      "grad_norm": 1.5109846591949463,
      "learning_rate": 4.875776397515528e-05,
      "loss": 1.2023,
      "step": 158
    },
    {
      "epoch": 0.29775280898876405,
      "grad_norm": 1.4496876001358032,
      "learning_rate": 4.906832298136646e-05,
      "loss": 1.194,
      "step": 159
    },
    {
      "epoch": 0.299625468164794,
      "grad_norm": 1.579806923866272,
      "learning_rate": 4.937888198757764e-05,
      "loss": 1.0874,
      "step": 160
    },
    {
      "epoch": 0.301498127340824,
      "grad_norm": 1.4519844055175781,
      "learning_rate": 4.968944099378882e-05,
      "loss": 1.1927,
      "step": 161
    },
    {
      "epoch": 0.30337078651685395,
      "grad_norm": 1.636043906211853,
      "learning_rate": 5e-05,
      "loss": 1.1534,
      "step": 162
    },
    {
      "epoch": 0.3052434456928839,
      "grad_norm": 1.4884446859359741,
      "learning_rate": 4.9999940586980495e-05,
      "loss": 1.2634,
      "step": 163
    },
    {
      "epoch": 0.30711610486891383,
      "grad_norm": 1.3654574155807495,
      "learning_rate": 4.999976234820439e-05,
      "loss": 1.1988,
      "step": 164
    },
    {
      "epoch": 0.3089887640449438,
      "grad_norm": 1.5280197858810425,
      "learning_rate": 4.999946528451884e-05,
      "loss": 1.1026,
      "step": 165
    },
    {
      "epoch": 0.31086142322097376,
      "grad_norm": 1.415433645248413,
      "learning_rate": 4.999904939733581e-05,
      "loss": 1.1093,
      "step": 166
    },
    {
      "epoch": 0.31273408239700373,
      "grad_norm": 1.2785145044326782,
      "learning_rate": 4.999851468863204e-05,
      "loss": 1.1535,
      "step": 167
    },
    {
      "epoch": 0.3146067415730337,
      "grad_norm": 1.4721038341522217,
      "learning_rate": 4.9997861160949e-05,
      "loss": 1.118,
      "step": 168
    },
    {
      "epoch": 0.31647940074906367,
      "grad_norm": 1.4718084335327148,
      "learning_rate": 4.999708881739296e-05,
      "loss": 1.1831,
      "step": 169
    },
    {
      "epoch": 0.31835205992509363,
      "grad_norm": 1.3739901781082153,
      "learning_rate": 4.9996197661634876e-05,
      "loss": 1.1029,
      "step": 170
    },
    {
      "epoch": 0.3202247191011236,
      "grad_norm": 1.5754553079605103,
      "learning_rate": 4.999518769791046e-05,
      "loss": 1.1467,
      "step": 171
    },
    {
      "epoch": 0.32209737827715357,
      "grad_norm": 1.605102300643921,
      "learning_rate": 4.999405893102012e-05,
      "loss": 1.1755,
      "step": 172
    },
    {
      "epoch": 0.32397003745318353,
      "grad_norm": 1.5127618312835693,
      "learning_rate": 4.9992811366328926e-05,
      "loss": 1.2515,
      "step": 173
    },
    {
      "epoch": 0.3258426966292135,
      "grad_norm": 1.4650321006774902,
      "learning_rate": 4.9991445009766595e-05,
      "loss": 1.1219,
      "step": 174
    },
    {
      "epoch": 0.32771535580524347,
      "grad_norm": 1.5768160820007324,
      "learning_rate": 4.9989959867827486e-05,
      "loss": 1.112,
      "step": 175
    },
    {
      "epoch": 0.3295880149812734,
      "grad_norm": 1.6900323629379272,
      "learning_rate": 4.998835594757054e-05,
      "loss": 1.162,
      "step": 176
    },
    {
      "epoch": 0.33146067415730335,
      "grad_norm": 1.7240861654281616,
      "learning_rate": 4.998663325661926e-05,
      "loss": 1.2133,
      "step": 177
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.5012314319610596,
      "learning_rate": 4.998479180316166e-05,
      "loss": 1.2095,
      "step": 178
    },
    {
      "epoch": 0.3352059925093633,
      "grad_norm": 1.446341872215271,
      "learning_rate": 4.998283159595024e-05,
      "loss": 1.1529,
      "step": 179
    },
    {
      "epoch": 0.33707865168539325,
      "grad_norm": 1.448501467704773,
      "learning_rate": 4.9980752644301964e-05,
      "loss": 1.1221,
      "step": 180
    },
    {
      "epoch": 0.3389513108614232,
      "grad_norm": 1.5665380954742432,
      "learning_rate": 4.997855495809816e-05,
      "loss": 1.1492,
      "step": 181
    },
    {
      "epoch": 0.3408239700374532,
      "grad_norm": 1.481780767440796,
      "learning_rate": 4.997623854778453e-05,
      "loss": 1.2011,
      "step": 182
    },
    {
      "epoch": 0.34269662921348315,
      "grad_norm": 1.5529239177703857,
      "learning_rate": 4.997380342437107e-05,
      "loss": 1.2144,
      "step": 183
    },
    {
      "epoch": 0.3445692883895131,
      "grad_norm": 1.4905798435211182,
      "learning_rate": 4.997124959943201e-05,
      "loss": 1.1335,
      "step": 184
    },
    {
      "epoch": 0.3464419475655431,
      "grad_norm": 1.5457500219345093,
      "learning_rate": 4.99685770851058e-05,
      "loss": 1.1206,
      "step": 185
    },
    {
      "epoch": 0.34831460674157305,
      "grad_norm": 1.4430296421051025,
      "learning_rate": 4.996578589409501e-05,
      "loss": 1.0037,
      "step": 186
    },
    {
      "epoch": 0.350187265917603,
      "grad_norm": 1.579960584640503,
      "learning_rate": 4.996287603966627e-05,
      "loss": 1.1179,
      "step": 187
    },
    {
      "epoch": 0.352059925093633,
      "grad_norm": 1.5079766511917114,
      "learning_rate": 4.995984753565027e-05,
      "loss": 1.1796,
      "step": 188
    },
    {
      "epoch": 0.3539325842696629,
      "grad_norm": 1.7241826057434082,
      "learning_rate": 4.995670039644159e-05,
      "loss": 1.1697,
      "step": 189
    },
    {
      "epoch": 0.35580524344569286,
      "grad_norm": 1.461499571800232,
      "learning_rate": 4.995343463699872e-05,
      "loss": 1.2091,
      "step": 190
    },
    {
      "epoch": 0.35767790262172283,
      "grad_norm": 1.615537166595459,
      "learning_rate": 4.9950050272843954e-05,
      "loss": 1.1336,
      "step": 191
    },
    {
      "epoch": 0.3595505617977528,
      "grad_norm": 1.469224452972412,
      "learning_rate": 4.994654732006331e-05,
      "loss": 1.1584,
      "step": 192
    },
    {
      "epoch": 0.36142322097378277,
      "grad_norm": 1.5374778509140015,
      "learning_rate": 4.9942925795306476e-05,
      "loss": 1.0903,
      "step": 193
    },
    {
      "epoch": 0.36329588014981273,
      "grad_norm": 1.4772615432739258,
      "learning_rate": 4.993918571578671e-05,
      "loss": 0.976,
      "step": 194
    },
    {
      "epoch": 0.3651685393258427,
      "grad_norm": 1.5688837766647339,
      "learning_rate": 4.993532709928075e-05,
      "loss": 1.1375,
      "step": 195
    },
    {
      "epoch": 0.36704119850187267,
      "grad_norm": 1.70148503780365,
      "learning_rate": 4.993134996412877e-05,
      "loss": 1.2268,
      "step": 196
    },
    {
      "epoch": 0.36891385767790263,
      "grad_norm": 1.453991174697876,
      "learning_rate": 4.9927254329234266e-05,
      "loss": 1.064,
      "step": 197
    },
    {
      "epoch": 0.3707865168539326,
      "grad_norm": 1.5822319984436035,
      "learning_rate": 4.9923040214063954e-05,
      "loss": 1.1093,
      "step": 198
    },
    {
      "epoch": 0.37265917602996257,
      "grad_norm": 1.6120829582214355,
      "learning_rate": 4.99187076386477e-05,
      "loss": 1.1677,
      "step": 199
    },
    {
      "epoch": 0.37453183520599254,
      "grad_norm": 1.4933550357818604,
      "learning_rate": 4.991425662357841e-05,
      "loss": 1.1727,
      "step": 200
    },
    {
      "epoch": 0.37640449438202245,
      "grad_norm": 1.5150635242462158,
      "learning_rate": 4.990968719001195e-05,
      "loss": 1.1404,
      "step": 201
    },
    {
      "epoch": 0.3782771535580524,
      "grad_norm": 1.5226224660873413,
      "learning_rate": 4.990499935966702e-05,
      "loss": 1.2232,
      "step": 202
    },
    {
      "epoch": 0.3801498127340824,
      "grad_norm": 1.493480920791626,
      "learning_rate": 4.9900193154825095e-05,
      "loss": 1.2115,
      "step": 203
    },
    {
      "epoch": 0.38202247191011235,
      "grad_norm": 1.565292477607727,
      "learning_rate": 4.989526859833024e-05,
      "loss": 1.1222,
      "step": 204
    },
    {
      "epoch": 0.3838951310861423,
      "grad_norm": 1.5033292770385742,
      "learning_rate": 4.989022571358908e-05,
      "loss": 1.1121,
      "step": 205
    },
    {
      "epoch": 0.3857677902621723,
      "grad_norm": 1.5071288347244263,
      "learning_rate": 4.9885064524570665e-05,
      "loss": 1.1408,
      "step": 206
    },
    {
      "epoch": 0.38764044943820225,
      "grad_norm": 1.5469200611114502,
      "learning_rate": 4.987978505580634e-05,
      "loss": 1.2503,
      "step": 207
    },
    {
      "epoch": 0.3895131086142322,
      "grad_norm": 1.4516538381576538,
      "learning_rate": 4.987438733238963e-05,
      "loss": 1.1788,
      "step": 208
    },
    {
      "epoch": 0.3913857677902622,
      "grad_norm": 1.3073605298995972,
      "learning_rate": 4.986887137997615e-05,
      "loss": 1.1472,
      "step": 209
    },
    {
      "epoch": 0.39325842696629215,
      "grad_norm": 1.541106104850769,
      "learning_rate": 4.986323722478345e-05,
      "loss": 1.1623,
      "step": 210
    },
    {
      "epoch": 0.3951310861423221,
      "grad_norm": 1.499010443687439,
      "learning_rate": 4.98574848935909e-05,
      "loss": 1.1277,
      "step": 211
    },
    {
      "epoch": 0.3970037453183521,
      "grad_norm": 1.397747278213501,
      "learning_rate": 4.9851614413739566e-05,
      "loss": 0.9774,
      "step": 212
    },
    {
      "epoch": 0.398876404494382,
      "grad_norm": 1.5046184062957764,
      "learning_rate": 4.984562581313209e-05,
      "loss": 1.1546,
      "step": 213
    },
    {
      "epoch": 0.40074906367041196,
      "grad_norm": 1.6133068799972534,
      "learning_rate": 4.9839519120232534e-05,
      "loss": 1.1194,
      "step": 214
    },
    {
      "epoch": 0.40262172284644193,
      "grad_norm": 1.6497281789779663,
      "learning_rate": 4.9833294364066266e-05,
      "loss": 1.1066,
      "step": 215
    },
    {
      "epoch": 0.4044943820224719,
      "grad_norm": 1.5671818256378174,
      "learning_rate": 4.982695157421982e-05,
      "loss": 1.1764,
      "step": 216
    },
    {
      "epoch": 0.40636704119850187,
      "grad_norm": 1.3684931993484497,
      "learning_rate": 4.982049078084071e-05,
      "loss": 0.9828,
      "step": 217
    },
    {
      "epoch": 0.40823970037453183,
      "grad_norm": 1.680830717086792,
      "learning_rate": 4.981391201463739e-05,
      "loss": 1.1215,
      "step": 218
    },
    {
      "epoch": 0.4101123595505618,
      "grad_norm": 1.5590721368789673,
      "learning_rate": 4.980721530687899e-05,
      "loss": 1.2404,
      "step": 219
    },
    {
      "epoch": 0.41198501872659177,
      "grad_norm": 1.534775733947754,
      "learning_rate": 4.980040068939524e-05,
      "loss": 1.0333,
      "step": 220
    },
    {
      "epoch": 0.41385767790262173,
      "grad_norm": 1.5796840190887451,
      "learning_rate": 4.979346819457631e-05,
      "loss": 1.2076,
      "step": 221
    },
    {
      "epoch": 0.4157303370786517,
      "grad_norm": 1.464978814125061,
      "learning_rate": 4.978641785537264e-05,
      "loss": 1.1224,
      "step": 222
    },
    {
      "epoch": 0.41760299625468167,
      "grad_norm": 1.6233384609222412,
      "learning_rate": 4.9779249705294764e-05,
      "loss": 1.1246,
      "step": 223
    },
    {
      "epoch": 0.41947565543071164,
      "grad_norm": 1.5799139738082886,
      "learning_rate": 4.977196377841321e-05,
      "loss": 1.2446,
      "step": 224
    },
    {
      "epoch": 0.42134831460674155,
      "grad_norm": 1.829337477684021,
      "learning_rate": 4.9764560109358295e-05,
      "loss": 1.204,
      "step": 225
    },
    {
      "epoch": 0.4232209737827715,
      "grad_norm": 1.6836844682693481,
      "learning_rate": 4.975703873331996e-05,
      "loss": 1.0616,
      "step": 226
    },
    {
      "epoch": 0.4250936329588015,
      "grad_norm": 1.6359386444091797,
      "learning_rate": 4.974939968604761e-05,
      "loss": 1.105,
      "step": 227
    },
    {
      "epoch": 0.42696629213483145,
      "grad_norm": 1.3627440929412842,
      "learning_rate": 4.974164300384998e-05,
      "loss": 1.0648,
      "step": 228
    },
    {
      "epoch": 0.4288389513108614,
      "grad_norm": 1.5270311832427979,
      "learning_rate": 4.973376872359488e-05,
      "loss": 1.1372,
      "step": 229
    },
    {
      "epoch": 0.4307116104868914,
      "grad_norm": 1.46063232421875,
      "learning_rate": 4.972577688270909e-05,
      "loss": 1.2022,
      "step": 230
    },
    {
      "epoch": 0.43258426966292135,
      "grad_norm": 1.6172986030578613,
      "learning_rate": 4.9717667519178176e-05,
      "loss": 1.0959,
      "step": 231
    },
    {
      "epoch": 0.4344569288389513,
      "grad_norm": 1.516430377960205,
      "learning_rate": 4.970944067154627e-05,
      "loss": 1.197,
      "step": 232
    },
    {
      "epoch": 0.4363295880149813,
      "grad_norm": 1.6371095180511475,
      "learning_rate": 4.970109637891593e-05,
      "loss": 1.0851,
      "step": 233
    },
    {
      "epoch": 0.43820224719101125,
      "grad_norm": 1.6148850917816162,
      "learning_rate": 4.969263468094791e-05,
      "loss": 1.1575,
      "step": 234
    },
    {
      "epoch": 0.4400749063670412,
      "grad_norm": 1.7453510761260986,
      "learning_rate": 4.968405561786103e-05,
      "loss": 1.2035,
      "step": 235
    },
    {
      "epoch": 0.4419475655430712,
      "grad_norm": 1.5782212018966675,
      "learning_rate": 4.967535923043192e-05,
      "loss": 1.0934,
      "step": 236
    },
    {
      "epoch": 0.4438202247191011,
      "grad_norm": 1.7206964492797852,
      "learning_rate": 4.966654555999488e-05,
      "loss": 1.1595,
      "step": 237
    },
    {
      "epoch": 0.44569288389513106,
      "grad_norm": 1.4365622997283936,
      "learning_rate": 4.965761464844165e-05,
      "loss": 1.1366,
      "step": 238
    },
    {
      "epoch": 0.44756554307116103,
      "grad_norm": 1.4344583749771118,
      "learning_rate": 4.964856653822122e-05,
      "loss": 1.0841,
      "step": 239
    },
    {
      "epoch": 0.449438202247191,
      "grad_norm": 1.4658355712890625,
      "learning_rate": 4.963940127233963e-05,
      "loss": 1.1874,
      "step": 240
    },
    {
      "epoch": 0.45131086142322097,
      "grad_norm": 1.5206516981124878,
      "learning_rate": 4.9630118894359775e-05,
      "loss": 1.1591,
      "step": 241
    },
    {
      "epoch": 0.45318352059925093,
      "grad_norm": 1.517268180847168,
      "learning_rate": 4.962071944840119e-05,
      "loss": 1.2242,
      "step": 242
    },
    {
      "epoch": 0.4550561797752809,
      "grad_norm": 1.501020908355713,
      "learning_rate": 4.961120297913983e-05,
      "loss": 1.0809,
      "step": 243
    },
    {
      "epoch": 0.45692883895131087,
      "grad_norm": 1.5617141723632812,
      "learning_rate": 4.960156953180786e-05,
      "loss": 1.2216,
      "step": 244
    },
    {
      "epoch": 0.45880149812734083,
      "grad_norm": 1.5764578580856323,
      "learning_rate": 4.959181915219345e-05,
      "loss": 1.1861,
      "step": 245
    },
    {
      "epoch": 0.4606741573033708,
      "grad_norm": 1.482728123664856,
      "learning_rate": 4.958195188664058e-05,
      "loss": 0.9889,
      "step": 246
    },
    {
      "epoch": 0.46254681647940077,
      "grad_norm": 1.87996244430542,
      "learning_rate": 4.9571967782048765e-05,
      "loss": 1.2252,
      "step": 247
    },
    {
      "epoch": 0.46441947565543074,
      "grad_norm": 1.6032559871673584,
      "learning_rate": 4.956186688587287e-05,
      "loss": 1.1643,
      "step": 248
    },
    {
      "epoch": 0.46629213483146065,
      "grad_norm": 1.5622271299362183,
      "learning_rate": 4.9551649246122854e-05,
      "loss": 1.0723,
      "step": 249
    },
    {
      "epoch": 0.4681647940074906,
      "grad_norm": 1.6316447257995605,
      "learning_rate": 4.954131491136362e-05,
      "loss": 1.0304,
      "step": 250
    },
    {
      "epoch": 0.4700374531835206,
      "grad_norm": 1.7002456188201904,
      "learning_rate": 4.953086393071466e-05,
      "loss": 1.1973,
      "step": 251
    },
    {
      "epoch": 0.47191011235955055,
      "grad_norm": 1.4778872728347778,
      "learning_rate": 4.9520296353849935e-05,
      "loss": 1.0214,
      "step": 252
    },
    {
      "epoch": 0.4737827715355805,
      "grad_norm": 1.5268747806549072,
      "learning_rate": 4.950961223099757e-05,
      "loss": 1.0433,
      "step": 253
    },
    {
      "epoch": 0.4756554307116105,
      "grad_norm": 1.728524088859558,
      "learning_rate": 4.9498811612939656e-05,
      "loss": 1.1828,
      "step": 254
    },
    {
      "epoch": 0.47752808988764045,
      "grad_norm": 1.4926018714904785,
      "learning_rate": 4.948789455101196e-05,
      "loss": 1.1214,
      "step": 255
    },
    {
      "epoch": 0.4794007490636704,
      "grad_norm": 1.5727952718734741,
      "learning_rate": 4.947686109710375e-05,
      "loss": 1.1685,
      "step": 256
    },
    {
      "epoch": 0.4812734082397004,
      "grad_norm": 1.5863584280014038,
      "learning_rate": 4.946571130365748e-05,
      "loss": 1.1242,
      "step": 257
    },
    {
      "epoch": 0.48314606741573035,
      "grad_norm": 1.4327436685562134,
      "learning_rate": 4.9454445223668586e-05,
      "loss": 1.1638,
      "step": 258
    },
    {
      "epoch": 0.4850187265917603,
      "grad_norm": 1.4657065868377686,
      "learning_rate": 4.944306291068522e-05,
      "loss": 1.109,
      "step": 259
    },
    {
      "epoch": 0.4868913857677903,
      "grad_norm": 1.7462222576141357,
      "learning_rate": 4.943156441880797e-05,
      "loss": 1.3181,
      "step": 260
    },
    {
      "epoch": 0.4887640449438202,
      "grad_norm": 1.6143985986709595,
      "learning_rate": 4.9419949802689666e-05,
      "loss": 1.0208,
      "step": 261
    },
    {
      "epoch": 0.49063670411985016,
      "grad_norm": 1.4593861103057861,
      "learning_rate": 4.940821911753505e-05,
      "loss": 1.1035,
      "step": 262
    },
    {
      "epoch": 0.49250936329588013,
      "grad_norm": 1.574257731437683,
      "learning_rate": 4.939637241910056e-05,
      "loss": 1.1071,
      "step": 263
    },
    {
      "epoch": 0.4943820224719101,
      "grad_norm": 1.3996013402938843,
      "learning_rate": 4.9384409763694045e-05,
      "loss": 1.1275,
      "step": 264
    },
    {
      "epoch": 0.49625468164794007,
      "grad_norm": 1.5150666236877441,
      "learning_rate": 4.937233120817451e-05,
      "loss": 1.0768,
      "step": 265
    },
    {
      "epoch": 0.49812734082397003,
      "grad_norm": 1.361235499382019,
      "learning_rate": 4.936013680995181e-05,
      "loss": 1.1034,
      "step": 266
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.5145703554153442,
      "learning_rate": 4.9347826626986445e-05,
      "loss": 1.0783,
      "step": 267
    },
    {
      "epoch": 0.50187265917603,
      "grad_norm": 1.5136126279830933,
      "learning_rate": 4.9335400717789233e-05,
      "loss": 1.0618,
      "step": 268
    },
    {
      "epoch": 0.5037453183520599,
      "grad_norm": 1.4927247762680054,
      "learning_rate": 4.932285914142102e-05,
      "loss": 1.1156,
      "step": 269
    },
    {
      "epoch": 0.5056179775280899,
      "grad_norm": 1.65762460231781,
      "learning_rate": 4.9310201957492444e-05,
      "loss": 1.1186,
      "step": 270
    },
    {
      "epoch": 0.5074906367041199,
      "grad_norm": 1.7279539108276367,
      "learning_rate": 4.929742922616363e-05,
      "loss": 1.11,
      "step": 271
    },
    {
      "epoch": 0.5093632958801498,
      "grad_norm": 1.5478657484054565,
      "learning_rate": 4.92845410081439e-05,
      "loss": 1.1016,
      "step": 272
    },
    {
      "epoch": 0.5112359550561798,
      "grad_norm": 1.5291787385940552,
      "learning_rate": 4.927153736469149e-05,
      "loss": 1.1079,
      "step": 273
    },
    {
      "epoch": 0.5131086142322098,
      "grad_norm": 1.4760980606079102,
      "learning_rate": 4.9258418357613257e-05,
      "loss": 1.179,
      "step": 274
    },
    {
      "epoch": 0.5149812734082397,
      "grad_norm": 1.7023591995239258,
      "learning_rate": 4.924518404926438e-05,
      "loss": 1.0133,
      "step": 275
    },
    {
      "epoch": 0.5168539325842697,
      "grad_norm": 1.5599968433380127,
      "learning_rate": 4.923183450254809e-05,
      "loss": 1.1677,
      "step": 276
    },
    {
      "epoch": 0.5187265917602997,
      "grad_norm": 1.588911771774292,
      "learning_rate": 4.921836978091533e-05,
      "loss": 1.1216,
      "step": 277
    },
    {
      "epoch": 0.5205992509363296,
      "grad_norm": 1.4291163682937622,
      "learning_rate": 4.9204789948364485e-05,
      "loss": 1.0281,
      "step": 278
    },
    {
      "epoch": 0.5224719101123596,
      "grad_norm": 1.7413066625595093,
      "learning_rate": 4.919109506944106e-05,
      "loss": 1.1404,
      "step": 279
    },
    {
      "epoch": 0.5243445692883895,
      "grad_norm": 1.4292179346084595,
      "learning_rate": 4.917728520923738e-05,
      "loss": 1.0465,
      "step": 280
    },
    {
      "epoch": 0.5262172284644194,
      "grad_norm": 1.6999690532684326,
      "learning_rate": 4.916336043339228e-05,
      "loss": 1.1552,
      "step": 281
    },
    {
      "epoch": 0.5280898876404494,
      "grad_norm": 1.6006282567977905,
      "learning_rate": 4.9149320808090826e-05,
      "loss": 1.0855,
      "step": 282
    },
    {
      "epoch": 0.5299625468164794,
      "grad_norm": 1.5259249210357666,
      "learning_rate": 4.913516640006392e-05,
      "loss": 1.1003,
      "step": 283
    },
    {
      "epoch": 0.5318352059925093,
      "grad_norm": 1.57192063331604,
      "learning_rate": 4.912089727658804e-05,
      "loss": 1.172,
      "step": 284
    },
    {
      "epoch": 0.5337078651685393,
      "grad_norm": 1.631603479385376,
      "learning_rate": 4.910651350548494e-05,
      "loss": 1.1915,
      "step": 285
    },
    {
      "epoch": 0.5355805243445693,
      "grad_norm": 1.6010798215866089,
      "learning_rate": 4.909201515512128e-05,
      "loss": 1.0756,
      "step": 286
    },
    {
      "epoch": 0.5374531835205992,
      "grad_norm": 1.4932098388671875,
      "learning_rate": 4.907740229440832e-05,
      "loss": 0.974,
      "step": 287
    },
    {
      "epoch": 0.5393258426966292,
      "grad_norm": 1.7492554187774658,
      "learning_rate": 4.9062674992801594e-05,
      "loss": 1.1777,
      "step": 288
    },
    {
      "epoch": 0.5411985018726592,
      "grad_norm": 1.5781502723693848,
      "learning_rate": 4.904783332030058e-05,
      "loss": 1.1718,
      "step": 289
    },
    {
      "epoch": 0.5430711610486891,
      "grad_norm": 1.5765377283096313,
      "learning_rate": 4.903287734744836e-05,
      "loss": 1.1215,
      "step": 290
    },
    {
      "epoch": 0.5449438202247191,
      "grad_norm": 1.6574699878692627,
      "learning_rate": 4.90178071453313e-05,
      "loss": 1.1899,
      "step": 291
    },
    {
      "epoch": 0.5468164794007491,
      "grad_norm": 1.3999654054641724,
      "learning_rate": 4.900262278557869e-05,
      "loss": 1.1339,
      "step": 292
    },
    {
      "epoch": 0.548689138576779,
      "grad_norm": 1.5854759216308594,
      "learning_rate": 4.898732434036244e-05,
      "loss": 1.1994,
      "step": 293
    },
    {
      "epoch": 0.550561797752809,
      "grad_norm": 1.5346934795379639,
      "learning_rate": 4.897191188239667e-05,
      "loss": 1.1763,
      "step": 294
    },
    {
      "epoch": 0.552434456928839,
      "grad_norm": 1.5394537448883057,
      "learning_rate": 4.895638548493745e-05,
      "loss": 1.0584,
      "step": 295
    },
    {
      "epoch": 0.5543071161048689,
      "grad_norm": 1.5062048435211182,
      "learning_rate": 4.894074522178239e-05,
      "loss": 1.0034,
      "step": 296
    },
    {
      "epoch": 0.5561797752808989,
      "grad_norm": 1.5169084072113037,
      "learning_rate": 4.892499116727032e-05,
      "loss": 1.0402,
      "step": 297
    },
    {
      "epoch": 0.5580524344569289,
      "grad_norm": 1.480250597000122,
      "learning_rate": 4.8909123396280894e-05,
      "loss": 1.048,
      "step": 298
    },
    {
      "epoch": 0.5599250936329588,
      "grad_norm": 1.5190110206604004,
      "learning_rate": 4.88931419842343e-05,
      "loss": 1.0521,
      "step": 299
    },
    {
      "epoch": 0.5617977528089888,
      "grad_norm": 1.6043379306793213,
      "learning_rate": 4.8877047007090847e-05,
      "loss": 1.1077,
      "step": 300
    },
    {
      "epoch": 0.5636704119850188,
      "grad_norm": 1.7554877996444702,
      "learning_rate": 4.8860838541350644e-05,
      "loss": 1.1897,
      "step": 301
    },
    {
      "epoch": 0.5655430711610487,
      "grad_norm": 1.581669807434082,
      "learning_rate": 4.88445166640532e-05,
      "loss": 1.054,
      "step": 302
    },
    {
      "epoch": 0.5674157303370787,
      "grad_norm": 1.6239006519317627,
      "learning_rate": 4.882808145277705e-05,
      "loss": 1.1241,
      "step": 303
    },
    {
      "epoch": 0.5692883895131086,
      "grad_norm": 1.4883763790130615,
      "learning_rate": 4.881153298563947e-05,
      "loss": 1.1219,
      "step": 304
    },
    {
      "epoch": 0.5711610486891385,
      "grad_norm": 1.698545217514038,
      "learning_rate": 4.8794871341296e-05,
      "loss": 1.1122,
      "step": 305
    },
    {
      "epoch": 0.5730337078651685,
      "grad_norm": 1.6264686584472656,
      "learning_rate": 4.877809659894012e-05,
      "loss": 1.1618,
      "step": 306
    },
    {
      "epoch": 0.5749063670411985,
      "grad_norm": 1.6276663541793823,
      "learning_rate": 4.876120883830288e-05,
      "loss": 1.0859,
      "step": 307
    },
    {
      "epoch": 0.5767790262172284,
      "grad_norm": 1.635825514793396,
      "learning_rate": 4.8744208139652526e-05,
      "loss": 1.1565,
      "step": 308
    },
    {
      "epoch": 0.5786516853932584,
      "grad_norm": 1.790785789489746,
      "learning_rate": 4.872709458379407e-05,
      "loss": 1.0935,
      "step": 309
    },
    {
      "epoch": 0.5805243445692884,
      "grad_norm": 1.5771825313568115,
      "learning_rate": 4.8709868252068947e-05,
      "loss": 1.1073,
      "step": 310
    },
    {
      "epoch": 0.5823970037453183,
      "grad_norm": 1.51578688621521,
      "learning_rate": 4.8692529226354635e-05,
      "loss": 1.1092,
      "step": 311
    },
    {
      "epoch": 0.5842696629213483,
      "grad_norm": 1.376161813735962,
      "learning_rate": 4.8675077589064247e-05,
      "loss": 1.0628,
      "step": 312
    },
    {
      "epoch": 0.5861423220973783,
      "grad_norm": 1.4921092987060547,
      "learning_rate": 4.865751342314614e-05,
      "loss": 0.9707,
      "step": 313
    },
    {
      "epoch": 0.5880149812734082,
      "grad_norm": 1.419815182685852,
      "learning_rate": 4.863983681208352e-05,
      "loss": 1.0434,
      "step": 314
    },
    {
      "epoch": 0.5898876404494382,
      "grad_norm": 1.7696516513824463,
      "learning_rate": 4.862204783989406e-05,
      "loss": 1.1411,
      "step": 315
    },
    {
      "epoch": 0.5917602996254682,
      "grad_norm": 1.4691334962844849,
      "learning_rate": 4.8604146591129485e-05,
      "loss": 1.0518,
      "step": 316
    },
    {
      "epoch": 0.5936329588014981,
      "grad_norm": 1.6589725017547607,
      "learning_rate": 4.8586133150875165e-05,
      "loss": 1.0912,
      "step": 317
    },
    {
      "epoch": 0.5955056179775281,
      "grad_norm": 1.5359841585159302,
      "learning_rate": 4.856800760474973e-05,
      "loss": 1.0159,
      "step": 318
    },
    {
      "epoch": 0.5973782771535581,
      "grad_norm": 1.5885412693023682,
      "learning_rate": 4.8549770038904676e-05,
      "loss": 1.118,
      "step": 319
    },
    {
      "epoch": 0.599250936329588,
      "grad_norm": 1.6005066633224487,
      "learning_rate": 4.853142054002388e-05,
      "loss": 1.1348,
      "step": 320
    },
    {
      "epoch": 0.601123595505618,
      "grad_norm": 1.5607479810714722,
      "learning_rate": 4.851295919532329e-05,
      "loss": 1.1106,
      "step": 321
    },
    {
      "epoch": 0.602996254681648,
      "grad_norm": 1.5515291690826416,
      "learning_rate": 4.849438609255045e-05,
      "loss": 1.0757,
      "step": 322
    },
    {
      "epoch": 0.6048689138576779,
      "grad_norm": 1.7061604261398315,
      "learning_rate": 4.847570131998408e-05,
      "loss": 1.1435,
      "step": 323
    },
    {
      "epoch": 0.6067415730337079,
      "grad_norm": 1.5443345308303833,
      "learning_rate": 4.845690496643368e-05,
      "loss": 1.1227,
      "step": 324
    },
    {
      "epoch": 0.6086142322097379,
      "grad_norm": 1.5258502960205078,
      "learning_rate": 4.8437997121239097e-05,
      "loss": 0.9021,
      "step": 325
    },
    {
      "epoch": 0.6104868913857678,
      "grad_norm": 1.4665710926055908,
      "learning_rate": 4.8418977874270114e-05,
      "loss": 1.0564,
      "step": 326
    },
    {
      "epoch": 0.6123595505617978,
      "grad_norm": 1.663405179977417,
      "learning_rate": 4.8399847315926e-05,
      "loss": 1.1565,
      "step": 327
    },
    {
      "epoch": 0.6142322097378277,
      "grad_norm": 1.6374868154525757,
      "learning_rate": 4.838060553713509e-05,
      "loss": 1.0452,
      "step": 328
    },
    {
      "epoch": 0.6161048689138576,
      "grad_norm": 1.6134765148162842,
      "learning_rate": 4.836125262935436e-05,
      "loss": 0.9966,
      "step": 329
    },
    {
      "epoch": 0.6179775280898876,
      "grad_norm": 1.4466798305511475,
      "learning_rate": 4.834178868456899e-05,
      "loss": 1.0768,
      "step": 330
    },
    {
      "epoch": 0.6198501872659176,
      "grad_norm": 1.6012035608291626,
      "learning_rate": 4.832221379529191e-05,
      "loss": 1.082,
      "step": 331
    },
    {
      "epoch": 0.6217228464419475,
      "grad_norm": 1.5244425535202026,
      "learning_rate": 4.830252805456339e-05,
      "loss": 1.0294,
      "step": 332
    },
    {
      "epoch": 0.6235955056179775,
      "grad_norm": 1.6361243724822998,
      "learning_rate": 4.8282731555950565e-05,
      "loss": 1.1562,
      "step": 333
    },
    {
      "epoch": 0.6254681647940075,
      "grad_norm": 1.5838738679885864,
      "learning_rate": 4.8262824393547025e-05,
      "loss": 1.0588,
      "step": 334
    },
    {
      "epoch": 0.6273408239700374,
      "grad_norm": 1.9189035892486572,
      "learning_rate": 4.824280666197233e-05,
      "loss": 1.1648,
      "step": 335
    },
    {
      "epoch": 0.6292134831460674,
      "grad_norm": 1.5704110860824585,
      "learning_rate": 4.82226784563716e-05,
      "loss": 1.1095,
      "step": 336
    },
    {
      "epoch": 0.6310861423220974,
      "grad_norm": 1.574375867843628,
      "learning_rate": 4.8202439872415026e-05,
      "loss": 1.1566,
      "step": 337
    },
    {
      "epoch": 0.6329588014981273,
      "grad_norm": 1.6069117784500122,
      "learning_rate": 4.818209100629745e-05,
      "loss": 1.1732,
      "step": 338
    },
    {
      "epoch": 0.6348314606741573,
      "grad_norm": 1.5431451797485352,
      "learning_rate": 4.8161631954737863e-05,
      "loss": 1.0653,
      "step": 339
    },
    {
      "epoch": 0.6367041198501873,
      "grad_norm": 1.5428498983383179,
      "learning_rate": 4.814106281497899e-05,
      "loss": 1.0314,
      "step": 340
    },
    {
      "epoch": 0.6385767790262172,
      "grad_norm": 1.601715326309204,
      "learning_rate": 4.8120383684786816e-05,
      "loss": 1.1075,
      "step": 341
    },
    {
      "epoch": 0.6404494382022472,
      "grad_norm": 1.695381999015808,
      "learning_rate": 4.809959466245011e-05,
      "loss": 1.0042,
      "step": 342
    },
    {
      "epoch": 0.6423220973782772,
      "grad_norm": 1.9092988967895508,
      "learning_rate": 4.807869584677994e-05,
      "loss": 1.1025,
      "step": 343
    },
    {
      "epoch": 0.6441947565543071,
      "grad_norm": 1.6228171586990356,
      "learning_rate": 4.805768733710926e-05,
      "loss": 1.1357,
      "step": 344
    },
    {
      "epoch": 0.6460674157303371,
      "grad_norm": 1.6147143840789795,
      "learning_rate": 4.8036569233292385e-05,
      "loss": 1.1814,
      "step": 345
    },
    {
      "epoch": 0.6479400749063671,
      "grad_norm": 1.393559455871582,
      "learning_rate": 4.8015341635704535e-05,
      "loss": 1.0114,
      "step": 346
    },
    {
      "epoch": 0.649812734082397,
      "grad_norm": 1.6626986265182495,
      "learning_rate": 4.7994004645241374e-05,
      "loss": 1.167,
      "step": 347
    },
    {
      "epoch": 0.651685393258427,
      "grad_norm": 1.660546064376831,
      "learning_rate": 4.79725583633185e-05,
      "loss": 1.125,
      "step": 348
    },
    {
      "epoch": 0.653558052434457,
      "grad_norm": 1.4520858526229858,
      "learning_rate": 4.795100289187099e-05,
      "loss": 0.9763,
      "step": 349
    },
    {
      "epoch": 0.6554307116104869,
      "grad_norm": 1.5276495218276978,
      "learning_rate": 4.792933833335287e-05,
      "loss": 1.0948,
      "step": 350
    },
    {
      "epoch": 0.6573033707865169,
      "grad_norm": 1.476569652557373,
      "learning_rate": 4.790756479073672e-05,
      "loss": 1.0611,
      "step": 351
    },
    {
      "epoch": 0.6591760299625468,
      "grad_norm": 1.5661776065826416,
      "learning_rate": 4.788568236751307e-05,
      "loss": 0.987,
      "step": 352
    },
    {
      "epoch": 0.6610486891385767,
      "grad_norm": 1.6556824445724487,
      "learning_rate": 4.786369116769e-05,
      "loss": 1.0858,
      "step": 353
    },
    {
      "epoch": 0.6629213483146067,
      "grad_norm": 1.4795435667037964,
      "learning_rate": 4.784159129579259e-05,
      "loss": 1.1415,
      "step": 354
    },
    {
      "epoch": 0.6647940074906367,
      "grad_norm": 1.5093092918395996,
      "learning_rate": 4.781938285686245e-05,
      "loss": 1.0005,
      "step": 355
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.7989087104797363,
      "learning_rate": 4.779706595645721e-05,
      "loss": 0.9165,
      "step": 356
    },
    {
      "epoch": 0.6685393258426966,
      "grad_norm": 1.6038930416107178,
      "learning_rate": 4.777464070065004e-05,
      "loss": 1.085,
      "step": 357
    },
    {
      "epoch": 0.6704119850187266,
      "grad_norm": 1.6552350521087646,
      "learning_rate": 4.775210719602909e-05,
      "loss": 1.1868,
      "step": 358
    },
    {
      "epoch": 0.6722846441947565,
      "grad_norm": 1.659806489944458,
      "learning_rate": 4.772946554969706e-05,
      "loss": 1.0912,
      "step": 359
    },
    {
      "epoch": 0.6741573033707865,
      "grad_norm": 1.6058727502822876,
      "learning_rate": 4.7706715869270635e-05,
      "loss": 1.1489,
      "step": 360
    },
    {
      "epoch": 0.6760299625468165,
      "grad_norm": 1.6421173810958862,
      "learning_rate": 4.768385826287999e-05,
      "loss": 1.1469,
      "step": 361
    },
    {
      "epoch": 0.6779026217228464,
      "grad_norm": 1.8495490550994873,
      "learning_rate": 4.766089283916828e-05,
      "loss": 0.9188,
      "step": 362
    },
    {
      "epoch": 0.6797752808988764,
      "grad_norm": 1.6859833002090454,
      "learning_rate": 4.763781970729111e-05,
      "loss": 1.0929,
      "step": 363
    },
    {
      "epoch": 0.6816479400749064,
      "grad_norm": 1.5425212383270264,
      "learning_rate": 4.761463897691604e-05,
      "loss": 1.0448,
      "step": 364
    },
    {
      "epoch": 0.6835205992509363,
      "grad_norm": 1.7280806303024292,
      "learning_rate": 4.759135075822204e-05,
      "loss": 1.1144,
      "step": 365
    },
    {
      "epoch": 0.6853932584269663,
      "grad_norm": 1.689196228981018,
      "learning_rate": 4.756795516189899e-05,
      "loss": 1.2129,
      "step": 366
    },
    {
      "epoch": 0.6872659176029963,
      "grad_norm": 1.5086220502853394,
      "learning_rate": 4.7544452299147135e-05,
      "loss": 1.1034,
      "step": 367
    },
    {
      "epoch": 0.6891385767790262,
      "grad_norm": 1.5630439519882202,
      "learning_rate": 4.752084228167654e-05,
      "loss": 1.1605,
      "step": 368
    },
    {
      "epoch": 0.6910112359550562,
      "grad_norm": 1.5696303844451904,
      "learning_rate": 4.7497125221706616e-05,
      "loss": 0.9959,
      "step": 369
    },
    {
      "epoch": 0.6928838951310862,
      "grad_norm": 1.4605532884597778,
      "learning_rate": 4.747330123196553e-05,
      "loss": 1.0566,
      "step": 370
    },
    {
      "epoch": 0.6947565543071161,
      "grad_norm": 1.3735911846160889,
      "learning_rate": 4.74493704256897e-05,
      "loss": 1.0433,
      "step": 371
    },
    {
      "epoch": 0.6966292134831461,
      "grad_norm": 1.6587042808532715,
      "learning_rate": 4.7425332916623225e-05,
      "loss": 0.9328,
      "step": 372
    },
    {
      "epoch": 0.6985018726591761,
      "grad_norm": 1.6691465377807617,
      "learning_rate": 4.7401188819017404e-05,
      "loss": 1.0803,
      "step": 373
    },
    {
      "epoch": 0.700374531835206,
      "grad_norm": 1.6182940006256104,
      "learning_rate": 4.737693824763013e-05,
      "loss": 1.2174,
      "step": 374
    },
    {
      "epoch": 0.702247191011236,
      "grad_norm": 1.8840125799179077,
      "learning_rate": 4.735258131772538e-05,
      "loss": 1.0311,
      "step": 375
    },
    {
      "epoch": 0.704119850187266,
      "grad_norm": 1.4873160123825073,
      "learning_rate": 4.732811814507264e-05,
      "loss": 0.9644,
      "step": 376
    },
    {
      "epoch": 0.7059925093632958,
      "grad_norm": 1.5592941045761108,
      "learning_rate": 4.73035488459464e-05,
      "loss": 1.0724,
      "step": 377
    },
    {
      "epoch": 0.7078651685393258,
      "grad_norm": 1.8579198122024536,
      "learning_rate": 4.727887353712556e-05,
      "loss": 1.0128,
      "step": 378
    },
    {
      "epoch": 0.7097378277153558,
      "grad_norm": 1.6077895164489746,
      "learning_rate": 4.725409233589288e-05,
      "loss": 1.0008,
      "step": 379
    },
    {
      "epoch": 0.7116104868913857,
      "grad_norm": 1.80449378490448,
      "learning_rate": 4.722920536003444e-05,
      "loss": 1.176,
      "step": 380
    },
    {
      "epoch": 0.7134831460674157,
      "grad_norm": 1.789223551750183,
      "learning_rate": 4.7204212727839085e-05,
      "loss": 0.968,
      "step": 381
    },
    {
      "epoch": 0.7153558052434457,
      "grad_norm": 1.5405093431472778,
      "learning_rate": 4.717911455809782e-05,
      "loss": 1.0982,
      "step": 382
    },
    {
      "epoch": 0.7172284644194756,
      "grad_norm": 1.7638291120529175,
      "learning_rate": 4.7153910970103294e-05,
      "loss": 0.9635,
      "step": 383
    },
    {
      "epoch": 0.7191011235955056,
      "grad_norm": 1.6764631271362305,
      "learning_rate": 4.7128602083649206e-05,
      "loss": 1.0696,
      "step": 384
    },
    {
      "epoch": 0.7209737827715356,
      "grad_norm": 1.5016858577728271,
      "learning_rate": 4.710318801902974e-05,
      "loss": 1.1009,
      "step": 385
    },
    {
      "epoch": 0.7228464419475655,
      "grad_norm": 1.5475585460662842,
      "learning_rate": 4.707766889703902e-05,
      "loss": 0.8943,
      "step": 386
    },
    {
      "epoch": 0.7247191011235955,
      "grad_norm": 1.8044363260269165,
      "learning_rate": 4.705204483897048e-05,
      "loss": 1.0045,
      "step": 387
    },
    {
      "epoch": 0.7265917602996255,
      "grad_norm": 1.8302942514419556,
      "learning_rate": 4.702631596661632e-05,
      "loss": 1.1079,
      "step": 388
    },
    {
      "epoch": 0.7284644194756554,
      "grad_norm": 1.8008815050125122,
      "learning_rate": 4.700048240226697e-05,
      "loss": 1.1204,
      "step": 389
    },
    {
      "epoch": 0.7303370786516854,
      "grad_norm": 1.5046863555908203,
      "learning_rate": 4.697454426871041e-05,
      "loss": 1.0613,
      "step": 390
    },
    {
      "epoch": 0.7322097378277154,
      "grad_norm": 1.4354548454284668,
      "learning_rate": 4.6948501689231674e-05,
      "loss": 1.0056,
      "step": 391
    },
    {
      "epoch": 0.7340823970037453,
      "grad_norm": 1.7457107305526733,
      "learning_rate": 4.6922354787612235e-05,
      "loss": 1.0263,
      "step": 392
    },
    {
      "epoch": 0.7359550561797753,
      "grad_norm": 1.5688825845718384,
      "learning_rate": 4.6896103688129385e-05,
      "loss": 0.9844,
      "step": 393
    },
    {
      "epoch": 0.7378277153558053,
      "grad_norm": 1.5871200561523438,
      "learning_rate": 4.68697485155557e-05,
      "loss": 1.1718,
      "step": 394
    },
    {
      "epoch": 0.7397003745318352,
      "grad_norm": 1.6620075702667236,
      "learning_rate": 4.6843289395158416e-05,
      "loss": 1.1409,
      "step": 395
    },
    {
      "epoch": 0.7415730337078652,
      "grad_norm": 1.647120714187622,
      "learning_rate": 4.6816726452698825e-05,
      "loss": 1.2754,
      "step": 396
    },
    {
      "epoch": 0.7434456928838952,
      "grad_norm": 1.4554756879806519,
      "learning_rate": 4.679005981443169e-05,
      "loss": 1.0255,
      "step": 397
    },
    {
      "epoch": 0.7453183520599251,
      "grad_norm": 1.5727430582046509,
      "learning_rate": 4.676328960710467e-05,
      "loss": 1.061,
      "step": 398
    },
    {
      "epoch": 0.7471910112359551,
      "grad_norm": 1.6953654289245605,
      "learning_rate": 4.673641595795766e-05,
      "loss": 1.0195,
      "step": 399
    },
    {
      "epoch": 0.7490636704119851,
      "grad_norm": 1.6274147033691406,
      "learning_rate": 4.670943899472222e-05,
      "loss": 1.0475,
      "step": 400
    },
    {
      "epoch": 0.7509363295880149,
      "grad_norm": 1.5732604265213013,
      "learning_rate": 4.6682358845621e-05,
      "loss": 1.0513,
      "step": 401
    },
    {
      "epoch": 0.7528089887640449,
      "grad_norm": 1.6868889331817627,
      "learning_rate": 4.6655175639367064e-05,
      "loss": 1.1051,
      "step": 402
    },
    {
      "epoch": 0.7546816479400749,
      "grad_norm": 1.5216459035873413,
      "learning_rate": 4.6627889505163326e-05,
      "loss": 1.1043,
      "step": 403
    },
    {
      "epoch": 0.7565543071161048,
      "grad_norm": 1.797364354133606,
      "learning_rate": 4.660050057270191e-05,
      "loss": 1.0954,
      "step": 404
    },
    {
      "epoch": 0.7584269662921348,
      "grad_norm": 1.6266460418701172,
      "learning_rate": 4.657300897216355e-05,
      "loss": 1.0303,
      "step": 405
    },
    {
      "epoch": 0.7602996254681648,
      "grad_norm": 1.8808412551879883,
      "learning_rate": 4.6545414834216974e-05,
      "loss": 1.017,
      "step": 406
    },
    {
      "epoch": 0.7621722846441947,
      "grad_norm": 1.701244592666626,
      "learning_rate": 4.6517718290018246e-05,
      "loss": 1.0323,
      "step": 407
    },
    {
      "epoch": 0.7640449438202247,
      "grad_norm": 1.6596214771270752,
      "learning_rate": 4.648991947121022e-05,
      "loss": 1.0573,
      "step": 408
    },
    {
      "epoch": 0.7659176029962547,
      "grad_norm": 1.4940226078033447,
      "learning_rate": 4.646201850992181e-05,
      "loss": 1.0868,
      "step": 409
    },
    {
      "epoch": 0.7677902621722846,
      "grad_norm": 1.5109360218048096,
      "learning_rate": 4.643401553876747e-05,
      "loss": 1.0178,
      "step": 410
    },
    {
      "epoch": 0.7696629213483146,
      "grad_norm": 1.823403239250183,
      "learning_rate": 4.6405910690846465e-05,
      "loss": 1.0791,
      "step": 411
    },
    {
      "epoch": 0.7715355805243446,
      "grad_norm": 1.610405445098877,
      "learning_rate": 4.6377704099742316e-05,
      "loss": 1.0537,
      "step": 412
    },
    {
      "epoch": 0.7734082397003745,
      "grad_norm": 1.541967749595642,
      "learning_rate": 4.634939589952212e-05,
      "loss": 1.0371,
      "step": 413
    },
    {
      "epoch": 0.7752808988764045,
      "grad_norm": 1.4686362743377686,
      "learning_rate": 4.632098622473593e-05,
      "loss": 1.0344,
      "step": 414
    },
    {
      "epoch": 0.7771535580524345,
      "grad_norm": 1.6143155097961426,
      "learning_rate": 4.6292475210416106e-05,
      "loss": 1.0489,
      "step": 415
    },
    {
      "epoch": 0.7790262172284644,
      "grad_norm": 1.4856438636779785,
      "learning_rate": 4.62638629920767e-05,
      "loss": 1.1411,
      "step": 416
    },
    {
      "epoch": 0.7808988764044944,
      "grad_norm": 1.5338101387023926,
      "learning_rate": 4.623514970571275e-05,
      "loss": 1.0695,
      "step": 417
    },
    {
      "epoch": 0.7827715355805244,
      "grad_norm": 1.8551791906356812,
      "learning_rate": 4.620633548779972e-05,
      "loss": 0.9719,
      "step": 418
    },
    {
      "epoch": 0.7846441947565543,
      "grad_norm": 1.511245608329773,
      "learning_rate": 4.6177420475292776e-05,
      "loss": 1.0255,
      "step": 419
    },
    {
      "epoch": 0.7865168539325843,
      "grad_norm": 1.7427936792373657,
      "learning_rate": 4.614840480562618e-05,
      "loss": 1.0822,
      "step": 420
    },
    {
      "epoch": 0.7883895131086143,
      "grad_norm": 1.63080632686615,
      "learning_rate": 4.611928861671261e-05,
      "loss": 1.0141,
      "step": 421
    },
    {
      "epoch": 0.7902621722846442,
      "grad_norm": 1.5692390203475952,
      "learning_rate": 4.609007204694252e-05,
      "loss": 1.0613,
      "step": 422
    },
    {
      "epoch": 0.7921348314606742,
      "grad_norm": 1.432518720626831,
      "learning_rate": 4.606075523518348e-05,
      "loss": 0.8665,
      "step": 423
    },
    {
      "epoch": 0.7940074906367042,
      "grad_norm": 1.7015092372894287,
      "learning_rate": 4.6031338320779534e-05,
      "loss": 1.1502,
      "step": 424
    },
    {
      "epoch": 0.795880149812734,
      "grad_norm": 1.6967726945877075,
      "learning_rate": 4.600182144355048e-05,
      "loss": 1.1109,
      "step": 425
    },
    {
      "epoch": 0.797752808988764,
      "grad_norm": 1.6699614524841309,
      "learning_rate": 4.597220474379125e-05,
      "loss": 1.0586,
      "step": 426
    },
    {
      "epoch": 0.799625468164794,
      "grad_norm": 1.8999767303466797,
      "learning_rate": 4.594248836227128e-05,
      "loss": 1.0509,
      "step": 427
    },
    {
      "epoch": 0.8014981273408239,
      "grad_norm": 1.5948760509490967,
      "learning_rate": 4.591267244023375e-05,
      "loss": 1.0471,
      "step": 428
    },
    {
      "epoch": 0.8033707865168539,
      "grad_norm": 1.4416248798370361,
      "learning_rate": 4.588275711939497e-05,
      "loss": 1.0822,
      "step": 429
    },
    {
      "epoch": 0.8052434456928839,
      "grad_norm": 1.6852812767028809,
      "learning_rate": 4.585274254194372e-05,
      "loss": 1.0269,
      "step": 430
    },
    {
      "epoch": 0.8071161048689138,
      "grad_norm": 1.7095532417297363,
      "learning_rate": 4.582262885054052e-05,
      "loss": 1.0377,
      "step": 431
    },
    {
      "epoch": 0.8089887640449438,
      "grad_norm": 1.8382697105407715,
      "learning_rate": 4.5792416188317e-05,
      "loss": 0.9847,
      "step": 432
    },
    {
      "epoch": 0.8108614232209738,
      "grad_norm": 1.7580546140670776,
      "learning_rate": 4.57621046988752e-05,
      "loss": 1.0357,
      "step": 433
    },
    {
      "epoch": 0.8127340823970037,
      "grad_norm": 1.6905148029327393,
      "learning_rate": 4.573169452628689e-05,
      "loss": 0.9799,
      "step": 434
    },
    {
      "epoch": 0.8146067415730337,
      "grad_norm": 1.4204102754592896,
      "learning_rate": 4.5701185815092894e-05,
      "loss": 1.0737,
      "step": 435
    },
    {
      "epoch": 0.8164794007490637,
      "grad_norm": 1.5896222591400146,
      "learning_rate": 4.567057871030237e-05,
      "loss": 1.0387,
      "step": 436
    },
    {
      "epoch": 0.8183520599250936,
      "grad_norm": 1.6951297521591187,
      "learning_rate": 4.563987335739216e-05,
      "loss": 1.0803,
      "step": 437
    },
    {
      "epoch": 0.8202247191011236,
      "grad_norm": 1.8428187370300293,
      "learning_rate": 4.560906990230609e-05,
      "loss": 1.1107,
      "step": 438
    },
    {
      "epoch": 0.8220973782771536,
      "grad_norm": 1.7049366235733032,
      "learning_rate": 4.557816849145425e-05,
      "loss": 1.0204,
      "step": 439
    },
    {
      "epoch": 0.8239700374531835,
      "grad_norm": 1.5920213460922241,
      "learning_rate": 4.5547169271712345e-05,
      "loss": 1.0093,
      "step": 440
    },
    {
      "epoch": 0.8258426966292135,
      "grad_norm": 1.6030118465423584,
      "learning_rate": 4.551607239042095e-05,
      "loss": 0.9408,
      "step": 441
    },
    {
      "epoch": 0.8277153558052435,
      "grad_norm": 1.4931598901748657,
      "learning_rate": 4.5484877995384824e-05,
      "loss": 0.9885,
      "step": 442
    },
    {
      "epoch": 0.8295880149812734,
      "grad_norm": 1.4005173444747925,
      "learning_rate": 4.545358623487224e-05,
      "loss": 1.1007,
      "step": 443
    },
    {
      "epoch": 0.8314606741573034,
      "grad_norm": 1.727854609489441,
      "learning_rate": 4.542219725761422e-05,
      "loss": 0.9969,
      "step": 444
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.672741413116455,
      "learning_rate": 4.539071121280389e-05,
      "loss": 1.0898,
      "step": 445
    },
    {
      "epoch": 0.8352059925093633,
      "grad_norm": 1.6663293838500977,
      "learning_rate": 4.535912825009573e-05,
      "loss": 1.1491,
      "step": 446
    },
    {
      "epoch": 0.8370786516853933,
      "grad_norm": 1.7204681634902954,
      "learning_rate": 4.5327448519604854e-05,
      "loss": 1.1566,
      "step": 447
    },
    {
      "epoch": 0.8389513108614233,
      "grad_norm": 1.68721604347229,
      "learning_rate": 4.5295672171906364e-05,
      "loss": 0.9671,
      "step": 448
    },
    {
      "epoch": 0.8408239700374532,
      "grad_norm": 1.73160982131958,
      "learning_rate": 4.526379935803455e-05,
      "loss": 1.127,
      "step": 449
    },
    {
      "epoch": 0.8426966292134831,
      "grad_norm": 1.6722755432128906,
      "learning_rate": 4.5231830229482216e-05,
      "loss": 0.9966,
      "step": 450
    },
    {
      "epoch": 0.8445692883895131,
      "grad_norm": 1.7836792469024658,
      "learning_rate": 4.519976493819996e-05,
      "loss": 1.0832,
      "step": 451
    },
    {
      "epoch": 0.846441947565543,
      "grad_norm": 1.5940388441085815,
      "learning_rate": 4.516760363659546e-05,
      "loss": 1.0323,
      "step": 452
    },
    {
      "epoch": 0.848314606741573,
      "grad_norm": 1.6814219951629639,
      "learning_rate": 4.5135346477532694e-05,
      "loss": 1.066,
      "step": 453
    },
    {
      "epoch": 0.850187265917603,
      "grad_norm": 1.8259578943252563,
      "learning_rate": 4.510299361433129e-05,
      "loss": 1.172,
      "step": 454
    },
    {
      "epoch": 0.8520599250936329,
      "grad_norm": 1.6291422843933105,
      "learning_rate": 4.507054520076576e-05,
      "loss": 1.0449,
      "step": 455
    },
    {
      "epoch": 0.8539325842696629,
      "grad_norm": 1.8515766859054565,
      "learning_rate": 4.503800139106475e-05,
      "loss": 1.0569,
      "step": 456
    },
    {
      "epoch": 0.8558052434456929,
      "grad_norm": 1.54253089427948,
      "learning_rate": 4.500536233991036e-05,
      "loss": 1.0169,
      "step": 457
    },
    {
      "epoch": 0.8576779026217228,
      "grad_norm": 1.539723515510559,
      "learning_rate": 4.497262820243733e-05,
      "loss": 1.0568,
      "step": 458
    },
    {
      "epoch": 0.8595505617977528,
      "grad_norm": 1.8949477672576904,
      "learning_rate": 4.49397991342324e-05,
      "loss": 1.1646,
      "step": 459
    },
    {
      "epoch": 0.8614232209737828,
      "grad_norm": 1.834831953048706,
      "learning_rate": 4.490687529133347e-05,
      "loss": 1.0207,
      "step": 460
    },
    {
      "epoch": 0.8632958801498127,
      "grad_norm": 1.5849862098693848,
      "learning_rate": 4.4873856830228956e-05,
      "loss": 1.01,
      "step": 461
    },
    {
      "epoch": 0.8651685393258427,
      "grad_norm": 1.7013742923736572,
      "learning_rate": 4.4840743907856964e-05,
      "loss": 1.0533,
      "step": 462
    },
    {
      "epoch": 0.8670411985018727,
      "grad_norm": 1.644546627998352,
      "learning_rate": 4.48075366816046e-05,
      "loss": 0.9573,
      "step": 463
    },
    {
      "epoch": 0.8689138576779026,
      "grad_norm": 1.7327362298965454,
      "learning_rate": 4.477423530930718e-05,
      "loss": 1.0258,
      "step": 464
    },
    {
      "epoch": 0.8707865168539326,
      "grad_norm": 1.8457634449005127,
      "learning_rate": 4.474083994924751e-05,
      "loss": 1.1828,
      "step": 465
    },
    {
      "epoch": 0.8726591760299626,
      "grad_norm": 1.7080919742584229,
      "learning_rate": 4.470735076015513e-05,
      "loss": 1.0078,
      "step": 466
    },
    {
      "epoch": 0.8745318352059925,
      "grad_norm": 1.9950729608535767,
      "learning_rate": 4.467376790120555e-05,
      "loss": 1.1748,
      "step": 467
    },
    {
      "epoch": 0.8764044943820225,
      "grad_norm": 1.627723217010498,
      "learning_rate": 4.464009153201949e-05,
      "loss": 1.0458,
      "step": 468
    },
    {
      "epoch": 0.8782771535580525,
      "grad_norm": 2.0230979919433594,
      "learning_rate": 4.460632181266213e-05,
      "loss": 1.1525,
      "step": 469
    },
    {
      "epoch": 0.8801498127340824,
      "grad_norm": 1.6221990585327148,
      "learning_rate": 4.4572458903642354e-05,
      "loss": 0.996,
      "step": 470
    },
    {
      "epoch": 0.8820224719101124,
      "grad_norm": 1.9461476802825928,
      "learning_rate": 4.4538502965911974e-05,
      "loss": 1.0291,
      "step": 471
    },
    {
      "epoch": 0.8838951310861424,
      "grad_norm": 1.7200851440429688,
      "learning_rate": 4.450445416086498e-05,
      "loss": 1.15,
      "step": 472
    },
    {
      "epoch": 0.8857677902621723,
      "grad_norm": 1.5953515768051147,
      "learning_rate": 4.447031265033675e-05,
      "loss": 1.0185,
      "step": 473
    },
    {
      "epoch": 0.8876404494382022,
      "grad_norm": 1.6676887273788452,
      "learning_rate": 4.4436078596603305e-05,
      "loss": 1.0666,
      "step": 474
    },
    {
      "epoch": 0.8895131086142322,
      "grad_norm": 1.4061825275421143,
      "learning_rate": 4.440175216238052e-05,
      "loss": 0.9712,
      "step": 475
    },
    {
      "epoch": 0.8913857677902621,
      "grad_norm": 1.7256637811660767,
      "learning_rate": 4.436733351082336e-05,
      "loss": 1.1157,
      "step": 476
    },
    {
      "epoch": 0.8932584269662921,
      "grad_norm": 1.8103817701339722,
      "learning_rate": 4.433282280552512e-05,
      "loss": 1.0177,
      "step": 477
    },
    {
      "epoch": 0.8951310861423221,
      "grad_norm": 1.789918065071106,
      "learning_rate": 4.429822021051662e-05,
      "loss": 1.0042,
      "step": 478
    },
    {
      "epoch": 0.897003745318352,
      "grad_norm": 1.8155372142791748,
      "learning_rate": 4.426352589026541e-05,
      "loss": 1.0779,
      "step": 479
    },
    {
      "epoch": 0.898876404494382,
      "grad_norm": 1.7328506708145142,
      "learning_rate": 4.422874000967505e-05,
      "loss": 0.9908,
      "step": 480
    },
    {
      "epoch": 0.900749063670412,
      "grad_norm": 1.9106333255767822,
      "learning_rate": 4.419386273408428e-05,
      "loss": 0.9457,
      "step": 481
    },
    {
      "epoch": 0.9026217228464419,
      "grad_norm": 1.8273078203201294,
      "learning_rate": 4.415889422926623e-05,
      "loss": 0.997,
      "step": 482
    },
    {
      "epoch": 0.9044943820224719,
      "grad_norm": 1.679417610168457,
      "learning_rate": 4.4123834661427665e-05,
      "loss": 1.0906,
      "step": 483
    },
    {
      "epoch": 0.9063670411985019,
      "grad_norm": 1.8032982349395752,
      "learning_rate": 4.408868419720816e-05,
      "loss": 1.0413,
      "step": 484
    },
    {
      "epoch": 0.9082397003745318,
      "grad_norm": 1.6805667877197266,
      "learning_rate": 4.405344300367934e-05,
      "loss": 0.9574,
      "step": 485
    },
    {
      "epoch": 0.9101123595505618,
      "grad_norm": 1.5782074928283691,
      "learning_rate": 4.4018111248344065e-05,
      "loss": 1.0039,
      "step": 486
    },
    {
      "epoch": 0.9119850187265918,
      "grad_norm": 1.6246662139892578,
      "learning_rate": 4.398268909913562e-05,
      "loss": 0.9786,
      "step": 487
    },
    {
      "epoch": 0.9138576779026217,
      "grad_norm": 1.813031792640686,
      "learning_rate": 4.394717672441697e-05,
      "loss": 1.0026,
      "step": 488
    },
    {
      "epoch": 0.9157303370786517,
      "grad_norm": 1.552275538444519,
      "learning_rate": 4.39115742929799e-05,
      "loss": 0.9646,
      "step": 489
    },
    {
      "epoch": 0.9176029962546817,
      "grad_norm": 1.6873841285705566,
      "learning_rate": 4.3875881974044256e-05,
      "loss": 1.0055,
      "step": 490
    },
    {
      "epoch": 0.9194756554307116,
      "grad_norm": 1.6861865520477295,
      "learning_rate": 4.384009993725709e-05,
      "loss": 1.0454,
      "step": 491
    },
    {
      "epoch": 0.9213483146067416,
      "grad_norm": 1.8031034469604492,
      "learning_rate": 4.3804228352691935e-05,
      "loss": 1.0282,
      "step": 492
    },
    {
      "epoch": 0.9232209737827716,
      "grad_norm": 1.7844208478927612,
      "learning_rate": 4.3768267390847906e-05,
      "loss": 1.1077,
      "step": 493
    },
    {
      "epoch": 0.9250936329588015,
      "grad_norm": 1.9212076663970947,
      "learning_rate": 4.373221722264896e-05,
      "loss": 1.0092,
      "step": 494
    },
    {
      "epoch": 0.9269662921348315,
      "grad_norm": 1.8501081466674805,
      "learning_rate": 4.369607801944304e-05,
      "loss": 1.0734,
      "step": 495
    },
    {
      "epoch": 0.9288389513108615,
      "grad_norm": 1.669869065284729,
      "learning_rate": 4.365984995300129e-05,
      "loss": 1.108,
      "step": 496
    },
    {
      "epoch": 0.9307116104868914,
      "grad_norm": 1.663673996925354,
      "learning_rate": 4.36235331955172e-05,
      "loss": 1.0407,
      "step": 497
    },
    {
      "epoch": 0.9325842696629213,
      "grad_norm": 1.7344704866409302,
      "learning_rate": 4.358712791960583e-05,
      "loss": 1.043,
      "step": 498
    },
    {
      "epoch": 0.9344569288389513,
      "grad_norm": 2.310483932495117,
      "learning_rate": 4.355063429830298e-05,
      "loss": 1.1223,
      "step": 499
    },
    {
      "epoch": 0.9363295880149812,
      "grad_norm": 1.6897557973861694,
      "learning_rate": 4.351405250506434e-05,
      "loss": 1.0652,
      "step": 500
    },
    {
      "epoch": 0.9382022471910112,
      "grad_norm": 1.7188814878463745,
      "learning_rate": 4.347738271376469e-05,
      "loss": 0.9759,
      "step": 501
    },
    {
      "epoch": 0.9400749063670412,
      "grad_norm": 1.824223279953003,
      "learning_rate": 4.34406250986971e-05,
      "loss": 1.0197,
      "step": 502
    },
    {
      "epoch": 0.9419475655430711,
      "grad_norm": 1.7260512113571167,
      "learning_rate": 4.3403779834572004e-05,
      "loss": 0.9848,
      "step": 503
    },
    {
      "epoch": 0.9438202247191011,
      "grad_norm": 1.697952389717102,
      "learning_rate": 4.336684709651649e-05,
      "loss": 0.8997,
      "step": 504
    },
    {
      "epoch": 0.9456928838951311,
      "grad_norm": 1.7912168502807617,
      "learning_rate": 4.33298270600734e-05,
      "loss": 1.064,
      "step": 505
    },
    {
      "epoch": 0.947565543071161,
      "grad_norm": 1.699842929840088,
      "learning_rate": 4.3292719901200506e-05,
      "loss": 1.0075,
      "step": 506
    },
    {
      "epoch": 0.949438202247191,
      "grad_norm": 1.7212971448898315,
      "learning_rate": 4.325552579626967e-05,
      "loss": 1.0191,
      "step": 507
    },
    {
      "epoch": 0.951310861423221,
      "grad_norm": 1.8068803548812866,
      "learning_rate": 4.321824492206602e-05,
      "loss": 1.0454,
      "step": 508
    },
    {
      "epoch": 0.9531835205992509,
      "grad_norm": 1.8014535903930664,
      "learning_rate": 4.318087745578711e-05,
      "loss": 1.035,
      "step": 509
    },
    {
      "epoch": 0.9550561797752809,
      "grad_norm": 1.7793548107147217,
      "learning_rate": 4.314342357504205e-05,
      "loss": 1.0225,
      "step": 510
    },
    {
      "epoch": 0.9569288389513109,
      "grad_norm": 1.8011503219604492,
      "learning_rate": 4.3105883457850694e-05,
      "loss": 1.0953,
      "step": 511
    },
    {
      "epoch": 0.9588014981273408,
      "grad_norm": 1.7338863611221313,
      "learning_rate": 4.3068257282642777e-05,
      "loss": 0.9953,
      "step": 512
    },
    {
      "epoch": 0.9606741573033708,
      "grad_norm": 1.8439340591430664,
      "learning_rate": 4.303054522825708e-05,
      "loss": 1.051,
      "step": 513
    },
    {
      "epoch": 0.9625468164794008,
      "grad_norm": 1.7267389297485352,
      "learning_rate": 4.2992747473940556e-05,
      "loss": 0.9943,
      "step": 514
    },
    {
      "epoch": 0.9644194756554307,
      "grad_norm": 1.7753161191940308,
      "learning_rate": 4.295486419934751e-05,
      "loss": 1.0512,
      "step": 515
    },
    {
      "epoch": 0.9662921348314607,
      "grad_norm": 1.5941411256790161,
      "learning_rate": 4.291689558453871e-05,
      "loss": 1.0779,
      "step": 516
    },
    {
      "epoch": 0.9681647940074907,
      "grad_norm": 1.687220811843872,
      "learning_rate": 4.287884180998058e-05,
      "loss": 1.0951,
      "step": 517
    },
    {
      "epoch": 0.9700374531835206,
      "grad_norm": 1.5222551822662354,
      "learning_rate": 4.2840703056544273e-05,
      "loss": 0.9775,
      "step": 518
    },
    {
      "epoch": 0.9719101123595506,
      "grad_norm": 1.7129426002502441,
      "learning_rate": 4.280247950550488e-05,
      "loss": 1.0237,
      "step": 519
    },
    {
      "epoch": 0.9737827715355806,
      "grad_norm": 1.7602370977401733,
      "learning_rate": 4.2764171338540516e-05,
      "loss": 1.1009,
      "step": 520
    },
    {
      "epoch": 0.9756554307116105,
      "grad_norm": 1.7526400089263916,
      "learning_rate": 4.27257787377315e-05,
      "loss": 1.084,
      "step": 521
    },
    {
      "epoch": 0.9775280898876404,
      "grad_norm": 1.734044075012207,
      "learning_rate": 4.2687301885559465e-05,
      "loss": 0.9989,
      "step": 522
    },
    {
      "epoch": 0.9794007490636704,
      "grad_norm": 2.0714521408081055,
      "learning_rate": 4.264874096490647e-05,
      "loss": 1.2199,
      "step": 523
    },
    {
      "epoch": 0.9812734082397003,
      "grad_norm": 2.1099956035614014,
      "learning_rate": 4.26100961590542e-05,
      "loss": 1.0812,
      "step": 524
    },
    {
      "epoch": 0.9831460674157303,
      "grad_norm": 1.688125491142273,
      "learning_rate": 4.2571367651683e-05,
      "loss": 1.0456,
      "step": 525
    },
    {
      "epoch": 0.9850187265917603,
      "grad_norm": 1.7595692873001099,
      "learning_rate": 4.2532555626871077e-05,
      "loss": 0.9801,
      "step": 526
    },
    {
      "epoch": 0.9868913857677902,
      "grad_norm": 1.7835627794265747,
      "learning_rate": 4.249366026909361e-05,
      "loss": 1.0635,
      "step": 527
    },
    {
      "epoch": 0.9887640449438202,
      "grad_norm": 1.9305663108825684,
      "learning_rate": 4.245468176322184e-05,
      "loss": 1.0856,
      "step": 528
    },
    {
      "epoch": 0.9906367041198502,
      "grad_norm": 1.84221351146698,
      "learning_rate": 4.2415620294522235e-05,
      "loss": 1.0592,
      "step": 529
    },
    {
      "epoch": 0.9925093632958801,
      "grad_norm": 1.5723772048950195,
      "learning_rate": 4.2376476048655576e-05,
      "loss": 1.051,
      "step": 530
    },
    {
      "epoch": 0.9943820224719101,
      "grad_norm": 1.654529094696045,
      "learning_rate": 4.233724921167609e-05,
      "loss": 0.9851,
      "step": 531
    },
    {
      "epoch": 0.9962546816479401,
      "grad_norm": 1.6773353815078735,
      "learning_rate": 4.2297939970030554e-05,
      "loss": 1.0517,
      "step": 532
    },
    {
      "epoch": 0.99812734082397,
      "grad_norm": 1.5636121034622192,
      "learning_rate": 4.225854851055744e-05,
      "loss": 0.9805,
      "step": 533
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.8314659595489502,
      "learning_rate": 4.2219075020485975e-05,
      "loss": 1.0412,
      "step": 534
    },
    {
      "epoch": 1.0018726591760299,
      "grad_norm": 1.687831163406372,
      "learning_rate": 4.217951968743533e-05,
      "loss": 0.9112,
      "step": 535
    },
    {
      "epoch": 1.00374531835206,
      "grad_norm": 1.7152167558670044,
      "learning_rate": 4.213988269941362e-05,
      "loss": 1.038,
      "step": 536
    },
    {
      "epoch": 1.0056179775280898,
      "grad_norm": 1.7820637226104736,
      "learning_rate": 4.2100164244817095e-05,
      "loss": 0.901,
      "step": 537
    },
    {
      "epoch": 1.0074906367041199,
      "grad_norm": 1.7385212182998657,
      "learning_rate": 4.206036451242924e-05,
      "loss": 0.9733,
      "step": 538
    },
    {
      "epoch": 1.0093632958801497,
      "grad_norm": 1.6926887035369873,
      "learning_rate": 4.2020483691419824e-05,
      "loss": 0.8587,
      "step": 539
    },
    {
      "epoch": 1.0112359550561798,
      "grad_norm": 2.0280916690826416,
      "learning_rate": 4.1980521971344044e-05,
      "loss": 0.9001,
      "step": 540
    },
    {
      "epoch": 1.0131086142322097,
      "grad_norm": 2.077990770339966,
      "learning_rate": 4.194047954214163e-05,
      "loss": 0.8724,
      "step": 541
    },
    {
      "epoch": 1.0149812734082397,
      "grad_norm": 2.005837917327881,
      "learning_rate": 4.1900356594135895e-05,
      "loss": 0.9792,
      "step": 542
    },
    {
      "epoch": 1.0168539325842696,
      "grad_norm": 1.8844318389892578,
      "learning_rate": 4.1860153318032893e-05,
      "loss": 0.888,
      "step": 543
    },
    {
      "epoch": 1.0187265917602997,
      "grad_norm": 1.7155941724777222,
      "learning_rate": 4.181986990492045e-05,
      "loss": 0.9391,
      "step": 544
    },
    {
      "epoch": 1.0205992509363295,
      "grad_norm": 1.7761414051055908,
      "learning_rate": 4.177950654626732e-05,
      "loss": 0.8041,
      "step": 545
    },
    {
      "epoch": 1.0224719101123596,
      "grad_norm": 1.882714867591858,
      "learning_rate": 4.173906343392221e-05,
      "loss": 0.8733,
      "step": 546
    },
    {
      "epoch": 1.0243445692883895,
      "grad_norm": 1.8269325494766235,
      "learning_rate": 4.169854076011292e-05,
      "loss": 0.9379,
      "step": 547
    },
    {
      "epoch": 1.0262172284644195,
      "grad_norm": 1.8602226972579956,
      "learning_rate": 4.1657938717445414e-05,
      "loss": 0.8153,
      "step": 548
    },
    {
      "epoch": 1.0280898876404494,
      "grad_norm": 2.014211416244507,
      "learning_rate": 4.1617257498902875e-05,
      "loss": 0.9633,
      "step": 549
    },
    {
      "epoch": 1.0299625468164795,
      "grad_norm": 1.6967893838882446,
      "learning_rate": 4.157649729784483e-05,
      "loss": 0.8158,
      "step": 550
    },
    {
      "epoch": 1.0318352059925093,
      "grad_norm": 1.5895038843154907,
      "learning_rate": 4.15356583080062e-05,
      "loss": 0.9025,
      "step": 551
    },
    {
      "epoch": 1.0337078651685394,
      "grad_norm": 1.9513355493545532,
      "learning_rate": 4.149474072349641e-05,
      "loss": 0.9674,
      "step": 552
    },
    {
      "epoch": 1.0355805243445693,
      "grad_norm": 1.997674584388733,
      "learning_rate": 4.145374473879843e-05,
      "loss": 0.8938,
      "step": 553
    },
    {
      "epoch": 1.0374531835205993,
      "grad_norm": 2.004856824874878,
      "learning_rate": 4.1412670548767895e-05,
      "loss": 0.9188,
      "step": 554
    },
    {
      "epoch": 1.0393258426966292,
      "grad_norm": 1.9221094846725464,
      "learning_rate": 4.137151834863213e-05,
      "loss": 0.8536,
      "step": 555
    },
    {
      "epoch": 1.0411985018726593,
      "grad_norm": 2.084178924560547,
      "learning_rate": 4.1330288333989245e-05,
      "loss": 1.0585,
      "step": 556
    },
    {
      "epoch": 1.0430711610486891,
      "grad_norm": 1.8300937414169312,
      "learning_rate": 4.128898070080722e-05,
      "loss": 0.891,
      "step": 557
    },
    {
      "epoch": 1.0449438202247192,
      "grad_norm": 2.0271317958831787,
      "learning_rate": 4.124759564542295e-05,
      "loss": 0.8638,
      "step": 558
    },
    {
      "epoch": 1.046816479400749,
      "grad_norm": 1.9429879188537598,
      "learning_rate": 4.120613336454133e-05,
      "loss": 0.8638,
      "step": 559
    },
    {
      "epoch": 1.048689138576779,
      "grad_norm": 1.570739507675171,
      "learning_rate": 4.1164594055234306e-05,
      "loss": 0.8787,
      "step": 560
    },
    {
      "epoch": 1.050561797752809,
      "grad_norm": 1.8611810207366943,
      "learning_rate": 4.112297791493993e-05,
      "loss": 0.7978,
      "step": 561
    },
    {
      "epoch": 1.0524344569288389,
      "grad_norm": 1.8811849355697632,
      "learning_rate": 4.108128514146144e-05,
      "loss": 0.9201,
      "step": 562
    },
    {
      "epoch": 1.054307116104869,
      "grad_norm": 1.8634980916976929,
      "learning_rate": 4.103951593296634e-05,
      "loss": 0.9523,
      "step": 563
    },
    {
      "epoch": 1.0561797752808988,
      "grad_norm": 1.9492840766906738,
      "learning_rate": 4.09976704879854e-05,
      "loss": 0.8885,
      "step": 564
    },
    {
      "epoch": 1.0580524344569289,
      "grad_norm": 2.1936943531036377,
      "learning_rate": 4.095574900541177e-05,
      "loss": 0.8726,
      "step": 565
    },
    {
      "epoch": 1.0599250936329587,
      "grad_norm": 2.0287249088287354,
      "learning_rate": 4.09137516845e-05,
      "loss": 0.8941,
      "step": 566
    },
    {
      "epoch": 1.0617977528089888,
      "grad_norm": 1.8202630281448364,
      "learning_rate": 4.0871678724865084e-05,
      "loss": 0.8267,
      "step": 567
    },
    {
      "epoch": 1.0636704119850187,
      "grad_norm": 1.806135892868042,
      "learning_rate": 4.0829530326481566e-05,
      "loss": 0.7858,
      "step": 568
    },
    {
      "epoch": 1.0655430711610487,
      "grad_norm": 2.030073642730713,
      "learning_rate": 4.078730668968253e-05,
      "loss": 0.8494,
      "step": 569
    },
    {
      "epoch": 1.0674157303370786,
      "grad_norm": 1.8419065475463867,
      "learning_rate": 4.074500801515867e-05,
      "loss": 0.8556,
      "step": 570
    },
    {
      "epoch": 1.0692883895131087,
      "grad_norm": 1.983514428138733,
      "learning_rate": 4.070263450395735e-05,
      "loss": 0.9299,
      "step": 571
    },
    {
      "epoch": 1.0711610486891385,
      "grad_norm": 1.8088353872299194,
      "learning_rate": 4.0660186357481625e-05,
      "loss": 0.8944,
      "step": 572
    },
    {
      "epoch": 1.0730337078651686,
      "grad_norm": 1.7331210374832153,
      "learning_rate": 4.0617663777489314e-05,
      "loss": 0.9301,
      "step": 573
    },
    {
      "epoch": 1.0749063670411985,
      "grad_norm": 1.7542271614074707,
      "learning_rate": 4.0575066966091984e-05,
      "loss": 0.8975,
      "step": 574
    },
    {
      "epoch": 1.0767790262172285,
      "grad_norm": 1.82411789894104,
      "learning_rate": 4.053239612575407e-05,
      "loss": 0.8299,
      "step": 575
    },
    {
      "epoch": 1.0786516853932584,
      "grad_norm": 1.7257235050201416,
      "learning_rate": 4.0489651459291844e-05,
      "loss": 0.9547,
      "step": 576
    },
    {
      "epoch": 1.0805243445692885,
      "grad_norm": 1.7096600532531738,
      "learning_rate": 4.0446833169872475e-05,
      "loss": 0.9108,
      "step": 577
    },
    {
      "epoch": 1.0823970037453183,
      "grad_norm": 1.7903965711593628,
      "learning_rate": 4.040394146101307e-05,
      "loss": 0.8498,
      "step": 578
    },
    {
      "epoch": 1.0842696629213484,
      "grad_norm": 1.7145309448242188,
      "learning_rate": 4.036097653657972e-05,
      "loss": 0.8843,
      "step": 579
    },
    {
      "epoch": 1.0861423220973783,
      "grad_norm": 1.8997694253921509,
      "learning_rate": 4.031793860078649e-05,
      "loss": 0.869,
      "step": 580
    },
    {
      "epoch": 1.0880149812734083,
      "grad_norm": 1.9322923421859741,
      "learning_rate": 4.0274827858194466e-05,
      "loss": 0.9678,
      "step": 581
    },
    {
      "epoch": 1.0898876404494382,
      "grad_norm": 2.041665554046631,
      "learning_rate": 4.023164451371081e-05,
      "loss": 1.1283,
      "step": 582
    },
    {
      "epoch": 1.0917602996254683,
      "grad_norm": 1.807553768157959,
      "learning_rate": 4.018838877258775e-05,
      "loss": 0.8673,
      "step": 583
    },
    {
      "epoch": 1.0936329588014981,
      "grad_norm": 1.757662296295166,
      "learning_rate": 4.0145060840421625e-05,
      "loss": 1.001,
      "step": 584
    },
    {
      "epoch": 1.095505617977528,
      "grad_norm": 1.7281726598739624,
      "learning_rate": 4.0101660923151895e-05,
      "loss": 0.9227,
      "step": 585
    },
    {
      "epoch": 1.097378277153558,
      "grad_norm": 1.7753331661224365,
      "learning_rate": 4.0058189227060163e-05,
      "loss": 0.7583,
      "step": 586
    },
    {
      "epoch": 1.099250936329588,
      "grad_norm": 1.6863261461257935,
      "learning_rate": 4.001464595876923e-05,
      "loss": 0.8575,
      "step": 587
    },
    {
      "epoch": 1.101123595505618,
      "grad_norm": 1.6255892515182495,
      "learning_rate": 3.997103132524202e-05,
      "loss": 0.9448,
      "step": 588
    },
    {
      "epoch": 1.1029962546816479,
      "grad_norm": 1.6957449913024902,
      "learning_rate": 3.9927345533780745e-05,
      "loss": 0.8649,
      "step": 589
    },
    {
      "epoch": 1.104868913857678,
      "grad_norm": 1.7411872148513794,
      "learning_rate": 3.988358879202576e-05,
      "loss": 0.843,
      "step": 590
    },
    {
      "epoch": 1.1067415730337078,
      "grad_norm": 1.9100240468978882,
      "learning_rate": 3.9839761307954675e-05,
      "loss": 0.7695,
      "step": 591
    },
    {
      "epoch": 1.1086142322097379,
      "grad_norm": 2.1886041164398193,
      "learning_rate": 3.9795863289881354e-05,
      "loss": 0.9019,
      "step": 592
    },
    {
      "epoch": 1.1104868913857677,
      "grad_norm": 2.0975728034973145,
      "learning_rate": 3.9751894946454895e-05,
      "loss": 0.9733,
      "step": 593
    },
    {
      "epoch": 1.1123595505617978,
      "grad_norm": 1.9563359022140503,
      "learning_rate": 3.970785648665867e-05,
      "loss": 0.9904,
      "step": 594
    },
    {
      "epoch": 1.1142322097378277,
      "grad_norm": 1.8268530368804932,
      "learning_rate": 3.96637481198093e-05,
      "loss": 0.836,
      "step": 595
    },
    {
      "epoch": 1.1161048689138577,
      "grad_norm": 1.8501758575439453,
      "learning_rate": 3.9619570055555685e-05,
      "loss": 0.8601,
      "step": 596
    },
    {
      "epoch": 1.1179775280898876,
      "grad_norm": 2.0343570709228516,
      "learning_rate": 3.9575322503878014e-05,
      "loss": 0.9111,
      "step": 597
    },
    {
      "epoch": 1.1198501872659177,
      "grad_norm": 1.9712011814117432,
      "learning_rate": 3.953100567508672e-05,
      "loss": 0.8724,
      "step": 598
    },
    {
      "epoch": 1.1217228464419475,
      "grad_norm": 1.73610258102417,
      "learning_rate": 3.9486619779821555e-05,
      "loss": 0.8674,
      "step": 599
    },
    {
      "epoch": 1.1235955056179776,
      "grad_norm": 1.8618731498718262,
      "learning_rate": 3.94421650290505e-05,
      "loss": 0.9941,
      "step": 600
    },
    {
      "epoch": 1.1254681647940075,
      "grad_norm": 1.797279715538025,
      "learning_rate": 3.9397641634068836e-05,
      "loss": 0.8445,
      "step": 601
    },
    {
      "epoch": 1.1273408239700375,
      "grad_norm": 1.9361034631729126,
      "learning_rate": 3.935304980649813e-05,
      "loss": 0.9033,
      "step": 602
    },
    {
      "epoch": 1.1292134831460674,
      "grad_norm": 1.9349952936172485,
      "learning_rate": 3.930838975828518e-05,
      "loss": 0.8683,
      "step": 603
    },
    {
      "epoch": 1.1310861423220975,
      "grad_norm": 2.0299923419952393,
      "learning_rate": 3.926366170170104e-05,
      "loss": 0.8806,
      "step": 604
    },
    {
      "epoch": 1.1329588014981273,
      "grad_norm": 2.0115840435028076,
      "learning_rate": 3.921886584934004e-05,
      "loss": 0.8464,
      "step": 605
    },
    {
      "epoch": 1.1348314606741572,
      "grad_norm": 1.803796410560608,
      "learning_rate": 3.917400241411872e-05,
      "loss": 0.9594,
      "step": 606
    },
    {
      "epoch": 1.1367041198501873,
      "grad_norm": 1.959556221961975,
      "learning_rate": 3.912907160927484e-05,
      "loss": 0.8242,
      "step": 607
    },
    {
      "epoch": 1.1385767790262173,
      "grad_norm": 1.9541592597961426,
      "learning_rate": 3.9084073648366404e-05,
      "loss": 0.8637,
      "step": 608
    },
    {
      "epoch": 1.1404494382022472,
      "grad_norm": 1.776226282119751,
      "learning_rate": 3.9039008745270584e-05,
      "loss": 0.7083,
      "step": 609
    },
    {
      "epoch": 1.142322097378277,
      "grad_norm": 2.126882791519165,
      "learning_rate": 3.899387711418273e-05,
      "loss": 0.9305,
      "step": 610
    },
    {
      "epoch": 1.1441947565543071,
      "grad_norm": 2.0424046516418457,
      "learning_rate": 3.894867896961536e-05,
      "loss": 0.938,
      "step": 611
    },
    {
      "epoch": 1.146067415730337,
      "grad_norm": 2.058445453643799,
      "learning_rate": 3.890341452639714e-05,
      "loss": 0.9299,
      "step": 612
    },
    {
      "epoch": 1.147940074906367,
      "grad_norm": 1.9364691972732544,
      "learning_rate": 3.8858083999671855e-05,
      "loss": 0.8314,
      "step": 613
    },
    {
      "epoch": 1.149812734082397,
      "grad_norm": 1.9851540327072144,
      "learning_rate": 3.881268760489737e-05,
      "loss": 0.8593,
      "step": 614
    },
    {
      "epoch": 1.151685393258427,
      "grad_norm": 1.8440266847610474,
      "learning_rate": 3.876722555784464e-05,
      "loss": 0.8639,
      "step": 615
    },
    {
      "epoch": 1.1535580524344569,
      "grad_norm": 1.8923161029815674,
      "learning_rate": 3.8721698074596674e-05,
      "loss": 0.7612,
      "step": 616
    },
    {
      "epoch": 1.155430711610487,
      "grad_norm": 2.060699939727783,
      "learning_rate": 3.867610537154748e-05,
      "loss": 0.877,
      "step": 617
    },
    {
      "epoch": 1.1573033707865168,
      "grad_norm": 1.7698261737823486,
      "learning_rate": 3.863044766540107e-05,
      "loss": 0.8269,
      "step": 618
    },
    {
      "epoch": 1.1591760299625469,
      "grad_norm": 1.7975224256515503,
      "learning_rate": 3.858472517317043e-05,
      "loss": 0.7981,
      "step": 619
    },
    {
      "epoch": 1.1610486891385767,
      "grad_norm": 1.9878660440444946,
      "learning_rate": 3.853893811217645e-05,
      "loss": 0.9843,
      "step": 620
    },
    {
      "epoch": 1.1629213483146068,
      "grad_norm": 1.9159960746765137,
      "learning_rate": 3.849308670004694e-05,
      "loss": 0.8036,
      "step": 621
    },
    {
      "epoch": 1.1647940074906367,
      "grad_norm": 1.78152334690094,
      "learning_rate": 3.844717115471558e-05,
      "loss": 0.9487,
      "step": 622
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 2.0267910957336426,
      "learning_rate": 3.840119169442085e-05,
      "loss": 0.9223,
      "step": 623
    },
    {
      "epoch": 1.1685393258426966,
      "grad_norm": 2.0303313732147217,
      "learning_rate": 3.835514853770505e-05,
      "loss": 0.9106,
      "step": 624
    },
    {
      "epoch": 1.1704119850187267,
      "grad_norm": 1.9710241556167603,
      "learning_rate": 3.8309041903413196e-05,
      "loss": 0.8762,
      "step": 625
    },
    {
      "epoch": 1.1722846441947565,
      "grad_norm": 1.9042834043502808,
      "learning_rate": 3.8262872010692055e-05,
      "loss": 0.9025,
      "step": 626
    },
    {
      "epoch": 1.1741573033707866,
      "grad_norm": 2.027129888534546,
      "learning_rate": 3.821663907898904e-05,
      "loss": 0.9219,
      "step": 627
    },
    {
      "epoch": 1.1760299625468165,
      "grad_norm": 1.838456630706787,
      "learning_rate": 3.817034332805119e-05,
      "loss": 0.8845,
      "step": 628
    },
    {
      "epoch": 1.1779026217228465,
      "grad_norm": 1.8870186805725098,
      "learning_rate": 3.8123984977924155e-05,
      "loss": 0.9938,
      "step": 629
    },
    {
      "epoch": 1.1797752808988764,
      "grad_norm": 2.2087645530700684,
      "learning_rate": 3.807756424895107e-05,
      "loss": 0.996,
      "step": 630
    },
    {
      "epoch": 1.1816479400749063,
      "grad_norm": 1.8590561151504517,
      "learning_rate": 3.803108136177161e-05,
      "loss": 0.7718,
      "step": 631
    },
    {
      "epoch": 1.1835205992509363,
      "grad_norm": 2.1689648628234863,
      "learning_rate": 3.7984536537320866e-05,
      "loss": 0.8713,
      "step": 632
    },
    {
      "epoch": 1.1853932584269664,
      "grad_norm": 1.9680993556976318,
      "learning_rate": 3.793792999682832e-05,
      "loss": 0.9367,
      "step": 633
    },
    {
      "epoch": 1.1872659176029963,
      "grad_norm": 1.8764774799346924,
      "learning_rate": 3.7891261961816795e-05,
      "loss": 0.9105,
      "step": 634
    },
    {
      "epoch": 1.1891385767790261,
      "grad_norm": 1.6814703941345215,
      "learning_rate": 3.784453265410141e-05,
      "loss": 0.941,
      "step": 635
    },
    {
      "epoch": 1.1910112359550562,
      "grad_norm": 1.870432734489441,
      "learning_rate": 3.7797742295788486e-05,
      "loss": 0.8684,
      "step": 636
    },
    {
      "epoch": 1.192883895131086,
      "grad_norm": 1.788495659828186,
      "learning_rate": 3.775089110927456e-05,
      "loss": 0.9145,
      "step": 637
    },
    {
      "epoch": 1.1947565543071161,
      "grad_norm": 2.024714469909668,
      "learning_rate": 3.7703979317245266e-05,
      "loss": 0.8497,
      "step": 638
    },
    {
      "epoch": 1.196629213483146,
      "grad_norm": 1.9500266313552856,
      "learning_rate": 3.765700714267429e-05,
      "loss": 0.8277,
      "step": 639
    },
    {
      "epoch": 1.198501872659176,
      "grad_norm": 2.0312447547912598,
      "learning_rate": 3.7609974808822345e-05,
      "loss": 0.7829,
      "step": 640
    },
    {
      "epoch": 1.200374531835206,
      "grad_norm": 1.8658101558685303,
      "learning_rate": 3.756288253923606e-05,
      "loss": 0.8629,
      "step": 641
    },
    {
      "epoch": 1.202247191011236,
      "grad_norm": 2.1856496334075928,
      "learning_rate": 3.7515730557746956e-05,
      "loss": 1.0577,
      "step": 642
    },
    {
      "epoch": 1.2041198501872659,
      "grad_norm": 1.9711968898773193,
      "learning_rate": 3.746851908847034e-05,
      "loss": 0.8834,
      "step": 643
    },
    {
      "epoch": 1.205992509363296,
      "grad_norm": 2.1932971477508545,
      "learning_rate": 3.742124835580432e-05,
      "loss": 0.9112,
      "step": 644
    },
    {
      "epoch": 1.2078651685393258,
      "grad_norm": 1.7414127588272095,
      "learning_rate": 3.7373918584428624e-05,
      "loss": 0.8531,
      "step": 645
    },
    {
      "epoch": 1.2097378277153559,
      "grad_norm": 2.143062114715576,
      "learning_rate": 3.732652999930364e-05,
      "loss": 0.9215,
      "step": 646
    },
    {
      "epoch": 1.2116104868913857,
      "grad_norm": 1.9152326583862305,
      "learning_rate": 3.727908282566927e-05,
      "loss": 0.8886,
      "step": 647
    },
    {
      "epoch": 1.2134831460674158,
      "grad_norm": 1.8175158500671387,
      "learning_rate": 3.7231577289043916e-05,
      "loss": 0.8143,
      "step": 648
    },
    {
      "epoch": 1.2153558052434457,
      "grad_norm": 1.8586068153381348,
      "learning_rate": 3.7184013615223364e-05,
      "loss": 0.9361,
      "step": 649
    },
    {
      "epoch": 1.2172284644194757,
      "grad_norm": 1.9381693601608276,
      "learning_rate": 3.7136392030279724e-05,
      "loss": 0.914,
      "step": 650
    },
    {
      "epoch": 1.2191011235955056,
      "grad_norm": 1.838897705078125,
      "learning_rate": 3.7088712760560366e-05,
      "loss": 0.9772,
      "step": 651
    },
    {
      "epoch": 1.2209737827715357,
      "grad_norm": 2.0299930572509766,
      "learning_rate": 3.704097603268686e-05,
      "loss": 0.8648,
      "step": 652
    },
    {
      "epoch": 1.2228464419475655,
      "grad_norm": 2.098155975341797,
      "learning_rate": 3.699318207355384e-05,
      "loss": 0.8811,
      "step": 653
    },
    {
      "epoch": 1.2247191011235956,
      "grad_norm": 2.039661407470703,
      "learning_rate": 3.6945331110328e-05,
      "loss": 0.8573,
      "step": 654
    },
    {
      "epoch": 1.2265917602996255,
      "grad_norm": 2.0975513458251953,
      "learning_rate": 3.689742337044692e-05,
      "loss": 0.8001,
      "step": 655
    },
    {
      "epoch": 1.2284644194756553,
      "grad_norm": 2.0027599334716797,
      "learning_rate": 3.684945908161812e-05,
      "loss": 0.9465,
      "step": 656
    },
    {
      "epoch": 1.2303370786516854,
      "grad_norm": 1.7930567264556885,
      "learning_rate": 3.680143847181783e-05,
      "loss": 0.764,
      "step": 657
    },
    {
      "epoch": 1.2322097378277155,
      "grad_norm": 2.086738109588623,
      "learning_rate": 3.675336176929002e-05,
      "loss": 0.8561,
      "step": 658
    },
    {
      "epoch": 1.2340823970037453,
      "grad_norm": 1.7855607271194458,
      "learning_rate": 3.670522920254524e-05,
      "loss": 0.9082,
      "step": 659
    },
    {
      "epoch": 1.2359550561797752,
      "grad_norm": 2.067283868789673,
      "learning_rate": 3.665704100035959e-05,
      "loss": 0.9147,
      "step": 660
    },
    {
      "epoch": 1.2378277153558053,
      "grad_norm": 1.959327220916748,
      "learning_rate": 3.660879739177361e-05,
      "loss": 0.801,
      "step": 661
    },
    {
      "epoch": 1.2397003745318351,
      "grad_norm": 2.1100645065307617,
      "learning_rate": 3.656049860609115e-05,
      "loss": 0.8093,
      "step": 662
    },
    {
      "epoch": 1.2415730337078652,
      "grad_norm": 1.7685705423355103,
      "learning_rate": 3.651214487287837e-05,
      "loss": 0.9406,
      "step": 663
    },
    {
      "epoch": 1.243445692883895,
      "grad_norm": 2.0655624866485596,
      "learning_rate": 3.646373642196255e-05,
      "loss": 0.8549,
      "step": 664
    },
    {
      "epoch": 1.2453183520599251,
      "grad_norm": 1.9937583208084106,
      "learning_rate": 3.641527348343109e-05,
      "loss": 0.8698,
      "step": 665
    },
    {
      "epoch": 1.247191011235955,
      "grad_norm": 2.2197160720825195,
      "learning_rate": 3.636675628763034e-05,
      "loss": 0.9046,
      "step": 666
    },
    {
      "epoch": 1.249063670411985,
      "grad_norm": 2.038623571395874,
      "learning_rate": 3.631818506516455e-05,
      "loss": 0.7569,
      "step": 667
    },
    {
      "epoch": 1.250936329588015,
      "grad_norm": 1.8967522382736206,
      "learning_rate": 3.6269560046894766e-05,
      "loss": 0.7733,
      "step": 668
    },
    {
      "epoch": 1.252808988764045,
      "grad_norm": 1.8420881032943726,
      "learning_rate": 3.6220881463937705e-05,
      "loss": 0.819,
      "step": 669
    },
    {
      "epoch": 1.2546816479400749,
      "grad_norm": 1.830077052116394,
      "learning_rate": 3.617214954766471e-05,
      "loss": 0.9267,
      "step": 670
    },
    {
      "epoch": 1.256554307116105,
      "grad_norm": 2.124821186065674,
      "learning_rate": 3.61233645297006e-05,
      "loss": 0.9597,
      "step": 671
    },
    {
      "epoch": 1.2584269662921348,
      "grad_norm": 1.997345209121704,
      "learning_rate": 3.607452664192259e-05,
      "loss": 0.8594,
      "step": 672
    },
    {
      "epoch": 1.2602996254681649,
      "grad_norm": 2.0383787155151367,
      "learning_rate": 3.602563611645919e-05,
      "loss": 0.8653,
      "step": 673
    },
    {
      "epoch": 1.2621722846441947,
      "grad_norm": 1.994863748550415,
      "learning_rate": 3.59766931856891e-05,
      "loss": 0.9198,
      "step": 674
    },
    {
      "epoch": 1.2640449438202248,
      "grad_norm": 1.6969037055969238,
      "learning_rate": 3.5927698082240116e-05,
      "loss": 0.8571,
      "step": 675
    },
    {
      "epoch": 1.2659176029962547,
      "grad_norm": 1.7476623058319092,
      "learning_rate": 3.5878651038987976e-05,
      "loss": 0.8817,
      "step": 676
    },
    {
      "epoch": 1.2677902621722845,
      "grad_norm": 1.8020098209381104,
      "learning_rate": 3.5829552289055335e-05,
      "loss": 0.8729,
      "step": 677
    },
    {
      "epoch": 1.2696629213483146,
      "grad_norm": 1.9839736223220825,
      "learning_rate": 3.578040206581059e-05,
      "loss": 0.886,
      "step": 678
    },
    {
      "epoch": 1.2715355805243447,
      "grad_norm": 1.881667137145996,
      "learning_rate": 3.573120060286679e-05,
      "loss": 0.8505,
      "step": 679
    },
    {
      "epoch": 1.2734082397003745,
      "grad_norm": 2.0950613021850586,
      "learning_rate": 3.568194813408053e-05,
      "loss": 0.9016,
      "step": 680
    },
    {
      "epoch": 1.2752808988764044,
      "grad_norm": 1.6656041145324707,
      "learning_rate": 3.563264489355085e-05,
      "loss": 0.8596,
      "step": 681
    },
    {
      "epoch": 1.2771535580524345,
      "grad_norm": 1.8961650133132935,
      "learning_rate": 3.558329111561809e-05,
      "loss": 0.8165,
      "step": 682
    },
    {
      "epoch": 1.2790262172284645,
      "grad_norm": 1.9196280241012573,
      "learning_rate": 3.5533887034862826e-05,
      "loss": 0.9331,
      "step": 683
    },
    {
      "epoch": 1.2808988764044944,
      "grad_norm": 2.0215675830841064,
      "learning_rate": 3.548443288610468e-05,
      "loss": 0.7857,
      "step": 684
    },
    {
      "epoch": 1.2827715355805243,
      "grad_norm": 2.001086950302124,
      "learning_rate": 3.54349289044013e-05,
      "loss": 0.9421,
      "step": 685
    },
    {
      "epoch": 1.2846441947565543,
      "grad_norm": 2.0237104892730713,
      "learning_rate": 3.5385375325047166e-05,
      "loss": 0.8377,
      "step": 686
    },
    {
      "epoch": 1.2865168539325842,
      "grad_norm": 1.7052642107009888,
      "learning_rate": 3.5335772383572485e-05,
      "loss": 0.7181,
      "step": 687
    },
    {
      "epoch": 1.2883895131086143,
      "grad_norm": 2.0267467498779297,
      "learning_rate": 3.528612031574212e-05,
      "loss": 0.9093,
      "step": 688
    },
    {
      "epoch": 1.2902621722846441,
      "grad_norm": 2.123560667037964,
      "learning_rate": 3.5236419357554385e-05,
      "loss": 0.8007,
      "step": 689
    },
    {
      "epoch": 1.2921348314606742,
      "grad_norm": 1.8707765340805054,
      "learning_rate": 3.5186669745240026e-05,
      "loss": 0.8112,
      "step": 690
    },
    {
      "epoch": 1.294007490636704,
      "grad_norm": 1.8026789426803589,
      "learning_rate": 3.5136871715261014e-05,
      "loss": 0.8119,
      "step": 691
    },
    {
      "epoch": 1.2958801498127341,
      "grad_norm": 2.0742743015289307,
      "learning_rate": 3.508702550430944e-05,
      "loss": 0.7983,
      "step": 692
    },
    {
      "epoch": 1.297752808988764,
      "grad_norm": 2.3089675903320312,
      "learning_rate": 3.503713134930643e-05,
      "loss": 0.9723,
      "step": 693
    },
    {
      "epoch": 1.299625468164794,
      "grad_norm": 1.8752373456954956,
      "learning_rate": 3.4987189487400965e-05,
      "loss": 0.791,
      "step": 694
    },
    {
      "epoch": 1.301498127340824,
      "grad_norm": 2.243183135986328,
      "learning_rate": 3.49372001559688e-05,
      "loss": 0.8389,
      "step": 695
    },
    {
      "epoch": 1.303370786516854,
      "grad_norm": 2.171312093734741,
      "learning_rate": 3.4887163592611307e-05,
      "loss": 0.9232,
      "step": 696
    },
    {
      "epoch": 1.3052434456928839,
      "grad_norm": 2.218933343887329,
      "learning_rate": 3.483708003515434e-05,
      "loss": 1.0515,
      "step": 697
    },
    {
      "epoch": 1.3071161048689137,
      "grad_norm": 1.8597537279129028,
      "learning_rate": 3.4786949721647146e-05,
      "loss": 0.838,
      "step": 698
    },
    {
      "epoch": 1.3089887640449438,
      "grad_norm": 2.0094287395477295,
      "learning_rate": 3.473677289036117e-05,
      "loss": 0.9057,
      "step": 699
    },
    {
      "epoch": 1.3108614232209739,
      "grad_norm": 1.9377511739730835,
      "learning_rate": 3.468654977978898e-05,
      "loss": 1.0709,
      "step": 700
    },
    {
      "epoch": 1.3127340823970037,
      "grad_norm": 1.9534810781478882,
      "learning_rate": 3.463628062864312e-05,
      "loss": 0.8954,
      "step": 701
    },
    {
      "epoch": 1.3146067415730336,
      "grad_norm": 2.201421022415161,
      "learning_rate": 3.458596567585494e-05,
      "loss": 0.9062,
      "step": 702
    },
    {
      "epoch": 1.3164794007490637,
      "grad_norm": 2.1270740032196045,
      "learning_rate": 3.4535605160573515e-05,
      "loss": 0.7755,
      "step": 703
    },
    {
      "epoch": 1.3183520599250937,
      "grad_norm": 1.9949228763580322,
      "learning_rate": 3.448519932216446e-05,
      "loss": 0.9053,
      "step": 704
    },
    {
      "epoch": 1.3202247191011236,
      "grad_norm": 2.0522568225860596,
      "learning_rate": 3.443474840020882e-05,
      "loss": 0.8843,
      "step": 705
    },
    {
      "epoch": 1.3220973782771535,
      "grad_norm": 2.0730514526367188,
      "learning_rate": 3.438425263450192e-05,
      "loss": 0.9653,
      "step": 706
    },
    {
      "epoch": 1.3239700374531835,
      "grad_norm": 2.1196141242980957,
      "learning_rate": 3.4333712265052234e-05,
      "loss": 0.889,
      "step": 707
    },
    {
      "epoch": 1.3258426966292136,
      "grad_norm": 2.174046039581299,
      "learning_rate": 3.4283127532080264e-05,
      "loss": 0.9227,
      "step": 708
    },
    {
      "epoch": 1.3277153558052435,
      "grad_norm": 2.070115566253662,
      "learning_rate": 3.42324986760173e-05,
      "loss": 0.911,
      "step": 709
    },
    {
      "epoch": 1.3295880149812733,
      "grad_norm": 1.9668123722076416,
      "learning_rate": 3.4181825937504435e-05,
      "loss": 0.8367,
      "step": 710
    },
    {
      "epoch": 1.3314606741573034,
      "grad_norm": 1.8326268196105957,
      "learning_rate": 3.413110955739129e-05,
      "loss": 0.7927,
      "step": 711
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.066711664199829,
      "learning_rate": 3.4080349776734925e-05,
      "loss": 0.9909,
      "step": 712
    },
    {
      "epoch": 1.3352059925093633,
      "grad_norm": 1.9361563920974731,
      "learning_rate": 3.40295468367987e-05,
      "loss": 0.8175,
      "step": 713
    },
    {
      "epoch": 1.3370786516853932,
      "grad_norm": 1.721789002418518,
      "learning_rate": 3.397870097905109e-05,
      "loss": 0.8351,
      "step": 714
    },
    {
      "epoch": 1.3389513108614233,
      "grad_norm": 1.9850127696990967,
      "learning_rate": 3.392781244516455e-05,
      "loss": 0.9037,
      "step": 715
    },
    {
      "epoch": 1.3408239700374531,
      "grad_norm": 2.248720407485962,
      "learning_rate": 3.387688147701444e-05,
      "loss": 0.9291,
      "step": 716
    },
    {
      "epoch": 1.3426966292134832,
      "grad_norm": 1.9062105417251587,
      "learning_rate": 3.382590831667774e-05,
      "loss": 0.8246,
      "step": 717
    },
    {
      "epoch": 1.344569288389513,
      "grad_norm": 1.9842383861541748,
      "learning_rate": 3.3774893206432e-05,
      "loss": 0.8566,
      "step": 718
    },
    {
      "epoch": 1.3464419475655431,
      "grad_norm": 2.048441171646118,
      "learning_rate": 3.372383638875416e-05,
      "loss": 0.877,
      "step": 719
    },
    {
      "epoch": 1.348314606741573,
      "grad_norm": 2.1857004165649414,
      "learning_rate": 3.367273810631941e-05,
      "loss": 0.896,
      "step": 720
    },
    {
      "epoch": 1.350187265917603,
      "grad_norm": 2.265347480773926,
      "learning_rate": 3.3621598602e-05,
      "loss": 0.8366,
      "step": 721
    },
    {
      "epoch": 1.352059925093633,
      "grad_norm": 1.9907594919204712,
      "learning_rate": 3.357041811886411e-05,
      "loss": 0.8671,
      "step": 722
    },
    {
      "epoch": 1.3539325842696628,
      "grad_norm": 2.329467296600342,
      "learning_rate": 3.351919690017473e-05,
      "loss": 0.909,
      "step": 723
    },
    {
      "epoch": 1.3558052434456929,
      "grad_norm": 2.328843593597412,
      "learning_rate": 3.34679351893884e-05,
      "loss": 0.8197,
      "step": 724
    },
    {
      "epoch": 1.357677902621723,
      "grad_norm": 2.265484094619751,
      "learning_rate": 3.341663323015421e-05,
      "loss": 0.943,
      "step": 725
    },
    {
      "epoch": 1.3595505617977528,
      "grad_norm": 1.9432964324951172,
      "learning_rate": 3.3365291266312464e-05,
      "loss": 0.8881,
      "step": 726
    },
    {
      "epoch": 1.3614232209737827,
      "grad_norm": 1.8170361518859863,
      "learning_rate": 3.3313909541893676e-05,
      "loss": 0.7658,
      "step": 727
    },
    {
      "epoch": 1.3632958801498127,
      "grad_norm": 1.9824120998382568,
      "learning_rate": 3.326248830111731e-05,
      "loss": 0.7999,
      "step": 728
    },
    {
      "epoch": 1.3651685393258428,
      "grad_norm": 2.112417459487915,
      "learning_rate": 3.3211027788390645e-05,
      "loss": 0.9249,
      "step": 729
    },
    {
      "epoch": 1.3670411985018727,
      "grad_norm": 1.8910340070724487,
      "learning_rate": 3.315952824830766e-05,
      "loss": 0.785,
      "step": 730
    },
    {
      "epoch": 1.3689138576779025,
      "grad_norm": 1.9307289123535156,
      "learning_rate": 3.3107989925647795e-05,
      "loss": 0.8747,
      "step": 731
    },
    {
      "epoch": 1.3707865168539326,
      "grad_norm": 2.0052859783172607,
      "learning_rate": 3.305641306537485e-05,
      "loss": 0.8737,
      "step": 732
    },
    {
      "epoch": 1.3726591760299627,
      "grad_norm": 1.9063730239868164,
      "learning_rate": 3.300479791263578e-05,
      "loss": 0.8,
      "step": 733
    },
    {
      "epoch": 1.3745318352059925,
      "grad_norm": 2.1904211044311523,
      "learning_rate": 3.2953144712759545e-05,
      "loss": 0.891,
      "step": 734
    },
    {
      "epoch": 1.3764044943820224,
      "grad_norm": 2.1661927700042725,
      "learning_rate": 3.290145371125596e-05,
      "loss": 0.8564,
      "step": 735
    },
    {
      "epoch": 1.3782771535580525,
      "grad_norm": 1.7209943532943726,
      "learning_rate": 3.2849725153814495e-05,
      "loss": 0.8218,
      "step": 736
    },
    {
      "epoch": 1.3801498127340823,
      "grad_norm": 2.1256117820739746,
      "learning_rate": 3.279795928630315e-05,
      "loss": 0.8682,
      "step": 737
    },
    {
      "epoch": 1.3820224719101124,
      "grad_norm": 2.0350182056427,
      "learning_rate": 3.274615635476722e-05,
      "loss": 0.7903,
      "step": 738
    },
    {
      "epoch": 1.3838951310861423,
      "grad_norm": 2.0008578300476074,
      "learning_rate": 3.269431660542821e-05,
      "loss": 0.8528,
      "step": 739
    },
    {
      "epoch": 1.3857677902621723,
      "grad_norm": 2.1600968837738037,
      "learning_rate": 3.26424402846826e-05,
      "loss": 0.892,
      "step": 740
    },
    {
      "epoch": 1.3876404494382022,
      "grad_norm": 1.838702917098999,
      "learning_rate": 3.259052763910068e-05,
      "loss": 0.8038,
      "step": 741
    },
    {
      "epoch": 1.3895131086142323,
      "grad_norm": 1.6921861171722412,
      "learning_rate": 3.253857891542545e-05,
      "loss": 0.8394,
      "step": 742
    },
    {
      "epoch": 1.3913857677902621,
      "grad_norm": 1.982797384262085,
      "learning_rate": 3.248659436057131e-05,
      "loss": 0.8428,
      "step": 743
    },
    {
      "epoch": 1.3932584269662922,
      "grad_norm": 2.2228477001190186,
      "learning_rate": 3.243457422162305e-05,
      "loss": 0.8051,
      "step": 744
    },
    {
      "epoch": 1.395131086142322,
      "grad_norm": 1.7974876165390015,
      "learning_rate": 3.238251874583452e-05,
      "loss": 0.7448,
      "step": 745
    },
    {
      "epoch": 1.3970037453183521,
      "grad_norm": 2.073329448699951,
      "learning_rate": 3.2330428180627575e-05,
      "loss": 0.8384,
      "step": 746
    },
    {
      "epoch": 1.398876404494382,
      "grad_norm": 2.2806830406188965,
      "learning_rate": 3.227830277359084e-05,
      "loss": 0.8455,
      "step": 747
    },
    {
      "epoch": 1.4007490636704119,
      "grad_norm": 1.92719566822052,
      "learning_rate": 3.2226142772478525e-05,
      "loss": 0.8085,
      "step": 748
    },
    {
      "epoch": 1.402621722846442,
      "grad_norm": 1.882110595703125,
      "learning_rate": 3.217394842520929e-05,
      "loss": 0.7188,
      "step": 749
    },
    {
      "epoch": 1.404494382022472,
      "grad_norm": 2.035609006881714,
      "learning_rate": 3.2121719979865054e-05,
      "loss": 0.8829,
      "step": 750
    },
    {
      "epoch": 1.4063670411985019,
      "grad_norm": 2.015223979949951,
      "learning_rate": 3.206945768468976e-05,
      "loss": 0.8299,
      "step": 751
    },
    {
      "epoch": 1.4082397003745317,
      "grad_norm": 2.07606840133667,
      "learning_rate": 3.201716178808829e-05,
      "loss": 0.8646,
      "step": 752
    },
    {
      "epoch": 1.4101123595505618,
      "grad_norm": 1.8067352771759033,
      "learning_rate": 3.196483253862521e-05,
      "loss": 0.8194,
      "step": 753
    },
    {
      "epoch": 1.4119850187265919,
      "grad_norm": 2.05181622505188,
      "learning_rate": 3.1912470185023615e-05,
      "loss": 0.8358,
      "step": 754
    },
    {
      "epoch": 1.4138576779026217,
      "grad_norm": 1.8342537879943848,
      "learning_rate": 3.186007497616394e-05,
      "loss": 0.6954,
      "step": 755
    },
    {
      "epoch": 1.4157303370786516,
      "grad_norm": 2.282930612564087,
      "learning_rate": 3.1807647161082795e-05,
      "loss": 0.866,
      "step": 756
    },
    {
      "epoch": 1.4176029962546817,
      "grad_norm": 2.028805732727051,
      "learning_rate": 3.175518698897178e-05,
      "loss": 0.761,
      "step": 757
    },
    {
      "epoch": 1.4194756554307117,
      "grad_norm": 2.2743337154388428,
      "learning_rate": 3.170269470917625e-05,
      "loss": 0.8038,
      "step": 758
    },
    {
      "epoch": 1.4213483146067416,
      "grad_norm": 2.1382741928100586,
      "learning_rate": 3.16501705711942e-05,
      "loss": 0.9405,
      "step": 759
    },
    {
      "epoch": 1.4232209737827715,
      "grad_norm": 1.8539307117462158,
      "learning_rate": 3.159761482467505e-05,
      "loss": 0.8741,
      "step": 760
    },
    {
      "epoch": 1.4250936329588015,
      "grad_norm": 2.1956048011779785,
      "learning_rate": 3.154502771941843e-05,
      "loss": 0.8594,
      "step": 761
    },
    {
      "epoch": 1.4269662921348314,
      "grad_norm": 2.1292033195495605,
      "learning_rate": 3.149240950537306e-05,
      "loss": 0.8453,
      "step": 762
    },
    {
      "epoch": 1.4288389513108615,
      "grad_norm": 2.0787179470062256,
      "learning_rate": 3.143976043263548e-05,
      "loss": 0.7941,
      "step": 763
    },
    {
      "epoch": 1.4307116104868913,
      "grad_norm": 2.2224669456481934,
      "learning_rate": 3.1387080751448927e-05,
      "loss": 0.8583,
      "step": 764
    },
    {
      "epoch": 1.4325842696629214,
      "grad_norm": 2.0202314853668213,
      "learning_rate": 3.133437071220211e-05,
      "loss": 0.8092,
      "step": 765
    },
    {
      "epoch": 1.4344569288389513,
      "grad_norm": 1.9189914464950562,
      "learning_rate": 3.128163056542805e-05,
      "loss": 0.834,
      "step": 766
    },
    {
      "epoch": 1.4363295880149813,
      "grad_norm": 1.8898454904556274,
      "learning_rate": 3.122886056180284e-05,
      "loss": 0.7688,
      "step": 767
    },
    {
      "epoch": 1.4382022471910112,
      "grad_norm": 1.8227170705795288,
      "learning_rate": 3.117606095214451e-05,
      "loss": 0.877,
      "step": 768
    },
    {
      "epoch": 1.4400749063670413,
      "grad_norm": 1.9851120710372925,
      "learning_rate": 3.112323198741179e-05,
      "loss": 0.9074,
      "step": 769
    },
    {
      "epoch": 1.4419475655430711,
      "grad_norm": 1.685333013534546,
      "learning_rate": 3.107037391870296e-05,
      "loss": 0.8266,
      "step": 770
    },
    {
      "epoch": 1.4438202247191012,
      "grad_norm": 1.9148869514465332,
      "learning_rate": 3.10174869972546e-05,
      "loss": 0.8262,
      "step": 771
    },
    {
      "epoch": 1.445692883895131,
      "grad_norm": 2.3265798091888428,
      "learning_rate": 3.0964571474440466e-05,
      "loss": 0.9214,
      "step": 772
    },
    {
      "epoch": 1.447565543071161,
      "grad_norm": 2.125643014907837,
      "learning_rate": 3.091162760177022e-05,
      "loss": 0.8517,
      "step": 773
    },
    {
      "epoch": 1.449438202247191,
      "grad_norm": 1.8511165380477905,
      "learning_rate": 3.0858655630888286e-05,
      "loss": 0.8229,
      "step": 774
    },
    {
      "epoch": 1.451310861423221,
      "grad_norm": 1.9651626348495483,
      "learning_rate": 3.080565581357266e-05,
      "loss": 0.825,
      "step": 775
    },
    {
      "epoch": 1.453183520599251,
      "grad_norm": 1.8481760025024414,
      "learning_rate": 3.075262840173367e-05,
      "loss": 0.9317,
      "step": 776
    },
    {
      "epoch": 1.4550561797752808,
      "grad_norm": 1.981337308883667,
      "learning_rate": 3.069957364741281e-05,
      "loss": 0.8163,
      "step": 777
    },
    {
      "epoch": 1.4569288389513109,
      "grad_norm": 1.930458903312683,
      "learning_rate": 3.064649180278152e-05,
      "loss": 0.8303,
      "step": 778
    },
    {
      "epoch": 1.458801498127341,
      "grad_norm": 2.069483757019043,
      "learning_rate": 3.059338312014002e-05,
      "loss": 0.8086,
      "step": 779
    },
    {
      "epoch": 1.4606741573033708,
      "grad_norm": 1.821169137954712,
      "learning_rate": 3.054024785191609e-05,
      "loss": 0.6967,
      "step": 780
    },
    {
      "epoch": 1.4625468164794007,
      "grad_norm": 1.9950743913650513,
      "learning_rate": 3.0487086250663876e-05,
      "loss": 0.7651,
      "step": 781
    },
    {
      "epoch": 1.4644194756554307,
      "grad_norm": 2.384225845336914,
      "learning_rate": 3.0433898569062665e-05,
      "loss": 0.9745,
      "step": 782
    },
    {
      "epoch": 1.4662921348314606,
      "grad_norm": 2.2805111408233643,
      "learning_rate": 3.0380685059915713e-05,
      "loss": 0.8832,
      "step": 783
    },
    {
      "epoch": 1.4681647940074907,
      "grad_norm": 2.069483518600464,
      "learning_rate": 3.0327445976149056e-05,
      "loss": 0.9764,
      "step": 784
    },
    {
      "epoch": 1.4700374531835205,
      "grad_norm": 1.8074579238891602,
      "learning_rate": 3.0274181570810262e-05,
      "loss": 0.8268,
      "step": 785
    },
    {
      "epoch": 1.4719101123595506,
      "grad_norm": 1.9492288827896118,
      "learning_rate": 3.0220892097067267e-05,
      "loss": 0.8347,
      "step": 786
    },
    {
      "epoch": 1.4737827715355805,
      "grad_norm": 1.9764297008514404,
      "learning_rate": 3.016757780820716e-05,
      "loss": 0.9218,
      "step": 787
    },
    {
      "epoch": 1.4756554307116105,
      "grad_norm": 2.17279314994812,
      "learning_rate": 3.0114238957634956e-05,
      "loss": 0.7748,
      "step": 788
    },
    {
      "epoch": 1.4775280898876404,
      "grad_norm": 1.9863344430923462,
      "learning_rate": 3.006087579887244e-05,
      "loss": 0.7933,
      "step": 789
    },
    {
      "epoch": 1.4794007490636705,
      "grad_norm": 2.10351300239563,
      "learning_rate": 3.000748858555692e-05,
      "loss": 0.8666,
      "step": 790
    },
    {
      "epoch": 1.4812734082397003,
      "grad_norm": 2.208491563796997,
      "learning_rate": 2.9954077571440043e-05,
      "loss": 0.9408,
      "step": 791
    },
    {
      "epoch": 1.4831460674157304,
      "grad_norm": 2.045579433441162,
      "learning_rate": 2.990064301038658e-05,
      "loss": 0.8997,
      "step": 792
    },
    {
      "epoch": 1.4850187265917603,
      "grad_norm": 2.2169342041015625,
      "learning_rate": 2.9847185156373216e-05,
      "loss": 0.8974,
      "step": 793
    },
    {
      "epoch": 1.4868913857677903,
      "grad_norm": 2.047705888748169,
      "learning_rate": 2.979370426348735e-05,
      "loss": 0.7848,
      "step": 794
    },
    {
      "epoch": 1.4887640449438202,
      "grad_norm": 2.1522233486175537,
      "learning_rate": 2.9740200585925894e-05,
      "loss": 0.8197,
      "step": 795
    },
    {
      "epoch": 1.4906367041198503,
      "grad_norm": 2.231931209564209,
      "learning_rate": 2.968667437799405e-05,
      "loss": 0.9419,
      "step": 796
    },
    {
      "epoch": 1.4925093632958801,
      "grad_norm": 2.1116912364959717,
      "learning_rate": 2.9633125894104107e-05,
      "loss": 0.8198,
      "step": 797
    },
    {
      "epoch": 1.49438202247191,
      "grad_norm": 1.97824227809906,
      "learning_rate": 2.957955538877424e-05,
      "loss": 0.8404,
      "step": 798
    },
    {
      "epoch": 1.49625468164794,
      "grad_norm": 1.8709267377853394,
      "learning_rate": 2.9525963116627282e-05,
      "loss": 0.6706,
      "step": 799
    },
    {
      "epoch": 1.4981273408239701,
      "grad_norm": 2.0602028369903564,
      "learning_rate": 2.9472349332389525e-05,
      "loss": 0.8381,
      "step": 800
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.9107080698013306,
      "learning_rate": 2.9418714290889527e-05,
      "loss": 0.8126,
      "step": 801
    },
    {
      "epoch": 1.5018726591760299,
      "grad_norm": 2.1514804363250732,
      "learning_rate": 2.9365058247056864e-05,
      "loss": 0.8905,
      "step": 802
    },
    {
      "epoch": 1.50374531835206,
      "grad_norm": 2.138956069946289,
      "learning_rate": 2.931138145592093e-05,
      "loss": 0.8795,
      "step": 803
    },
    {
      "epoch": 1.50561797752809,
      "grad_norm": 2.0594165325164795,
      "learning_rate": 2.9257684172609762e-05,
      "loss": 0.7342,
      "step": 804
    },
    {
      "epoch": 1.5074906367041199,
      "grad_norm": 2.002101421356201,
      "learning_rate": 2.9203966652348764e-05,
      "loss": 0.782,
      "step": 805
    },
    {
      "epoch": 1.5093632958801497,
      "grad_norm": 2.439159393310547,
      "learning_rate": 2.9150229150459557e-05,
      "loss": 0.9131,
      "step": 806
    },
    {
      "epoch": 1.5112359550561798,
      "grad_norm": 2.1754424571990967,
      "learning_rate": 2.9096471922358715e-05,
      "loss": 0.7987,
      "step": 807
    },
    {
      "epoch": 1.5131086142322099,
      "grad_norm": 2.0490329265594482,
      "learning_rate": 2.9042695223556578e-05,
      "loss": 0.8112,
      "step": 808
    },
    {
      "epoch": 1.5149812734082397,
      "grad_norm": 2.169816017150879,
      "learning_rate": 2.8988899309656015e-05,
      "loss": 0.8144,
      "step": 809
    },
    {
      "epoch": 1.5168539325842696,
      "grad_norm": 2.2394819259643555,
      "learning_rate": 2.893508443635127e-05,
      "loss": 0.8204,
      "step": 810
    },
    {
      "epoch": 1.5187265917602997,
      "grad_norm": 2.068720579147339,
      "learning_rate": 2.8881250859426646e-05,
      "loss": 0.8869,
      "step": 811
    },
    {
      "epoch": 1.5205992509363297,
      "grad_norm": 1.8675874471664429,
      "learning_rate": 2.8827398834755387e-05,
      "loss": 0.7843,
      "step": 812
    },
    {
      "epoch": 1.5224719101123596,
      "grad_norm": 2.013995885848999,
      "learning_rate": 2.877352861829839e-05,
      "loss": 0.8563,
      "step": 813
    },
    {
      "epoch": 1.5243445692883895,
      "grad_norm": 1.9649431705474854,
      "learning_rate": 2.871964046610305e-05,
      "loss": 0.8744,
      "step": 814
    },
    {
      "epoch": 1.5262172284644193,
      "grad_norm": 2.0239551067352295,
      "learning_rate": 2.866573463430199e-05,
      "loss": 0.865,
      "step": 815
    },
    {
      "epoch": 1.5280898876404494,
      "grad_norm": 2.039821147918701,
      "learning_rate": 2.861181137911186e-05,
      "loss": 1.0623,
      "step": 816
    },
    {
      "epoch": 1.5299625468164795,
      "grad_norm": 1.7383253574371338,
      "learning_rate": 2.8557870956832132e-05,
      "loss": 0.8826,
      "step": 817
    },
    {
      "epoch": 1.5318352059925093,
      "grad_norm": 1.740822196006775,
      "learning_rate": 2.850391362384388e-05,
      "loss": 0.7792,
      "step": 818
    },
    {
      "epoch": 1.5337078651685392,
      "grad_norm": 1.7616338729858398,
      "learning_rate": 2.8449939636608557e-05,
      "loss": 0.7961,
      "step": 819
    },
    {
      "epoch": 1.5355805243445693,
      "grad_norm": 1.929697871208191,
      "learning_rate": 2.8395949251666758e-05,
      "loss": 0.7805,
      "step": 820
    },
    {
      "epoch": 1.5374531835205993,
      "grad_norm": 1.973687767982483,
      "learning_rate": 2.8341942725637038e-05,
      "loss": 0.8083,
      "step": 821
    },
    {
      "epoch": 1.5393258426966292,
      "grad_norm": 2.022160530090332,
      "learning_rate": 2.8287920315214643e-05,
      "loss": 0.7441,
      "step": 822
    },
    {
      "epoch": 1.541198501872659,
      "grad_norm": 1.9795082807540894,
      "learning_rate": 2.8233882277170348e-05,
      "loss": 0.8385,
      "step": 823
    },
    {
      "epoch": 1.5430711610486891,
      "grad_norm": 1.9533348083496094,
      "learning_rate": 2.817982886834918e-05,
      "loss": 0.8493,
      "step": 824
    },
    {
      "epoch": 1.5449438202247192,
      "grad_norm": 2.1738698482513428,
      "learning_rate": 2.8125760345669255e-05,
      "loss": 0.9569,
      "step": 825
    },
    {
      "epoch": 1.546816479400749,
      "grad_norm": 2.504946231842041,
      "learning_rate": 2.8071676966120496e-05,
      "loss": 0.8658,
      "step": 826
    },
    {
      "epoch": 1.548689138576779,
      "grad_norm": 2.2606699466705322,
      "learning_rate": 2.8017578986763464e-05,
      "loss": 0.8174,
      "step": 827
    },
    {
      "epoch": 1.550561797752809,
      "grad_norm": 1.7741096019744873,
      "learning_rate": 2.7963466664728087e-05,
      "loss": 0.7638,
      "step": 828
    },
    {
      "epoch": 1.552434456928839,
      "grad_norm": 2.0022614002227783,
      "learning_rate": 2.79093402572125e-05,
      "loss": 0.7535,
      "step": 829
    },
    {
      "epoch": 1.554307116104869,
      "grad_norm": 2.3463430404663086,
      "learning_rate": 2.7855200021481752e-05,
      "loss": 0.8353,
      "step": 830
    },
    {
      "epoch": 1.5561797752808988,
      "grad_norm": 1.936607837677002,
      "learning_rate": 2.7801046214866644e-05,
      "loss": 0.7697,
      "step": 831
    },
    {
      "epoch": 1.5580524344569289,
      "grad_norm": 2.274137496948242,
      "learning_rate": 2.774687909476246e-05,
      "loss": 0.8985,
      "step": 832
    },
    {
      "epoch": 1.559925093632959,
      "grad_norm": 1.879332423210144,
      "learning_rate": 2.7692698918627778e-05,
      "loss": 0.8523,
      "step": 833
    },
    {
      "epoch": 1.5617977528089888,
      "grad_norm": 1.8186306953430176,
      "learning_rate": 2.7638505943983234e-05,
      "loss": 0.7749,
      "step": 834
    },
    {
      "epoch": 1.5636704119850187,
      "grad_norm": 2.327240467071533,
      "learning_rate": 2.758430042841027e-05,
      "loss": 0.7926,
      "step": 835
    },
    {
      "epoch": 1.5655430711610487,
      "grad_norm": 1.9896254539489746,
      "learning_rate": 2.753008262954998e-05,
      "loss": 0.7289,
      "step": 836
    },
    {
      "epoch": 1.5674157303370788,
      "grad_norm": 2.300457715988159,
      "learning_rate": 2.747585280510179e-05,
      "loss": 0.9338,
      "step": 837
    },
    {
      "epoch": 1.5692883895131087,
      "grad_norm": 1.9869569540023804,
      "learning_rate": 2.7421611212822322e-05,
      "loss": 0.716,
      "step": 838
    },
    {
      "epoch": 1.5711610486891385,
      "grad_norm": 1.9542715549468994,
      "learning_rate": 2.7367358110524115e-05,
      "loss": 0.6837,
      "step": 839
    },
    {
      "epoch": 1.5730337078651684,
      "grad_norm": 1.9569438695907593,
      "learning_rate": 2.7313093756074425e-05,
      "loss": 0.7521,
      "step": 840
    },
    {
      "epoch": 1.5749063670411985,
      "grad_norm": 1.9465059041976929,
      "learning_rate": 2.725881840739398e-05,
      "loss": 0.7631,
      "step": 841
    },
    {
      "epoch": 1.5767790262172285,
      "grad_norm": 2.3844785690307617,
      "learning_rate": 2.7204532322455768e-05,
      "loss": 0.8693,
      "step": 842
    },
    {
      "epoch": 1.5786516853932584,
      "grad_norm": 2.0626027584075928,
      "learning_rate": 2.7150235759283806e-05,
      "loss": 0.8521,
      "step": 843
    },
    {
      "epoch": 1.5805243445692883,
      "grad_norm": 2.068049669265747,
      "learning_rate": 2.7095928975951913e-05,
      "loss": 0.9318,
      "step": 844
    },
    {
      "epoch": 1.5823970037453183,
      "grad_norm": 2.221632719039917,
      "learning_rate": 2.70416122305825e-05,
      "loss": 0.7827,
      "step": 845
    },
    {
      "epoch": 1.5842696629213484,
      "grad_norm": 2.1992974281311035,
      "learning_rate": 2.6987285781345295e-05,
      "loss": 0.7931,
      "step": 846
    },
    {
      "epoch": 1.5861423220973783,
      "grad_norm": 2.10249662399292,
      "learning_rate": 2.6932949886456183e-05,
      "loss": 0.7681,
      "step": 847
    },
    {
      "epoch": 1.5880149812734081,
      "grad_norm": 2.040968179702759,
      "learning_rate": 2.687860480417592e-05,
      "loss": 0.7746,
      "step": 848
    },
    {
      "epoch": 1.5898876404494382,
      "grad_norm": 2.115309715270996,
      "learning_rate": 2.6824250792808957e-05,
      "loss": 0.7523,
      "step": 849
    },
    {
      "epoch": 1.5917602996254683,
      "grad_norm": 2.1083409786224365,
      "learning_rate": 2.676988811070215e-05,
      "loss": 0.8331,
      "step": 850
    },
    {
      "epoch": 1.5936329588014981,
      "grad_norm": 2.111668825149536,
      "learning_rate": 2.6715517016243614e-05,
      "loss": 0.8147,
      "step": 851
    },
    {
      "epoch": 1.595505617977528,
      "grad_norm": 1.8096346855163574,
      "learning_rate": 2.6661137767861387e-05,
      "loss": 0.7512,
      "step": 852
    },
    {
      "epoch": 1.597378277153558,
      "grad_norm": 2.107985734939575,
      "learning_rate": 2.660675062402232e-05,
      "loss": 0.6565,
      "step": 853
    },
    {
      "epoch": 1.5992509363295881,
      "grad_norm": 2.0349738597869873,
      "learning_rate": 2.6552355843230752e-05,
      "loss": 0.8178,
      "step": 854
    },
    {
      "epoch": 1.601123595505618,
      "grad_norm": 2.395418405532837,
      "learning_rate": 2.649795368402735e-05,
      "loss": 0.9321,
      "step": 855
    },
    {
      "epoch": 1.6029962546816479,
      "grad_norm": 2.2907466888427734,
      "learning_rate": 2.6443544404987837e-05,
      "loss": 0.8339,
      "step": 856
    },
    {
      "epoch": 1.604868913857678,
      "grad_norm": 2.2324681282043457,
      "learning_rate": 2.6389128264721768e-05,
      "loss": 0.81,
      "step": 857
    },
    {
      "epoch": 1.606741573033708,
      "grad_norm": 2.011767625808716,
      "learning_rate": 2.633470552187132e-05,
      "loss": 0.7933,
      "step": 858
    },
    {
      "epoch": 1.6086142322097379,
      "grad_norm": 2.2673604488372803,
      "learning_rate": 2.6280276435110062e-05,
      "loss": 0.7507,
      "step": 859
    },
    {
      "epoch": 1.6104868913857677,
      "grad_norm": 2.236374855041504,
      "learning_rate": 2.62258412631417e-05,
      "loss": 0.9236,
      "step": 860
    },
    {
      "epoch": 1.6123595505617978,
      "grad_norm": 2.2057223320007324,
      "learning_rate": 2.6171400264698868e-05,
      "loss": 0.903,
      "step": 861
    },
    {
      "epoch": 1.6142322097378277,
      "grad_norm": 2.1376936435699463,
      "learning_rate": 2.6116953698541897e-05,
      "loss": 0.9012,
      "step": 862
    },
    {
      "epoch": 1.6161048689138577,
      "grad_norm": 2.151864767074585,
      "learning_rate": 2.6062501823457563e-05,
      "loss": 0.854,
      "step": 863
    },
    {
      "epoch": 1.6179775280898876,
      "grad_norm": 2.125399589538574,
      "learning_rate": 2.6008044898257915e-05,
      "loss": 0.8422,
      "step": 864
    },
    {
      "epoch": 1.6198501872659175,
      "grad_norm": 1.923624038696289,
      "learning_rate": 2.5953583181778968e-05,
      "loss": 0.7877,
      "step": 865
    },
    {
      "epoch": 1.6217228464419475,
      "grad_norm": 2.123814344406128,
      "learning_rate": 2.5899116932879534e-05,
      "loss": 0.8506,
      "step": 866
    },
    {
      "epoch": 1.6235955056179776,
      "grad_norm": 2.090711832046509,
      "learning_rate": 2.5844646410439944e-05,
      "loss": 0.7012,
      "step": 867
    },
    {
      "epoch": 1.6254681647940075,
      "grad_norm": 2.192798137664795,
      "learning_rate": 2.5790171873360862e-05,
      "loss": 0.8075,
      "step": 868
    },
    {
      "epoch": 1.6273408239700373,
      "grad_norm": 2.3388078212738037,
      "learning_rate": 2.573569358056202e-05,
      "loss": 0.7946,
      "step": 869
    },
    {
      "epoch": 1.6292134831460674,
      "grad_norm": 2.2489678859710693,
      "learning_rate": 2.5681211790981024e-05,
      "loss": 0.8884,
      "step": 870
    },
    {
      "epoch": 1.6310861423220975,
      "grad_norm": 2.0106968879699707,
      "learning_rate": 2.5626726763572075e-05,
      "loss": 0.8152,
      "step": 871
    },
    {
      "epoch": 1.6329588014981273,
      "grad_norm": 2.1075329780578613,
      "learning_rate": 2.5572238757304768e-05,
      "loss": 0.8226,
      "step": 872
    },
    {
      "epoch": 1.6348314606741572,
      "grad_norm": 2.3428235054016113,
      "learning_rate": 2.5517748031162856e-05,
      "loss": 0.8997,
      "step": 873
    },
    {
      "epoch": 1.6367041198501873,
      "grad_norm": 2.2089715003967285,
      "learning_rate": 2.546325484414305e-05,
      "loss": 0.8878,
      "step": 874
    },
    {
      "epoch": 1.6385767790262173,
      "grad_norm": 2.1943042278289795,
      "learning_rate": 2.540875945525371e-05,
      "loss": 0.8594,
      "step": 875
    },
    {
      "epoch": 1.6404494382022472,
      "grad_norm": 1.9753762483596802,
      "learning_rate": 2.5354262123513695e-05,
      "loss": 0.7866,
      "step": 876
    },
    {
      "epoch": 1.642322097378277,
      "grad_norm": 2.016860008239746,
      "learning_rate": 2.529976310795108e-05,
      "loss": 0.8024,
      "step": 877
    },
    {
      "epoch": 1.6441947565543071,
      "grad_norm": 2.0258703231811523,
      "learning_rate": 2.524526266760195e-05,
      "loss": 0.803,
      "step": 878
    },
    {
      "epoch": 1.6460674157303372,
      "grad_norm": 2.16494083404541,
      "learning_rate": 2.519076106150917e-05,
      "loss": 0.901,
      "step": 879
    },
    {
      "epoch": 1.647940074906367,
      "grad_norm": 1.9267734289169312,
      "learning_rate": 2.5136258548721137e-05,
      "loss": 0.7943,
      "step": 880
    },
    {
      "epoch": 1.649812734082397,
      "grad_norm": 2.299868106842041,
      "learning_rate": 2.5081755388290567e-05,
      "loss": 0.8745,
      "step": 881
    },
    {
      "epoch": 1.651685393258427,
      "grad_norm": 2.4311273097991943,
      "learning_rate": 2.502725183927323e-05,
      "loss": 0.7826,
      "step": 882
    },
    {
      "epoch": 1.653558052434457,
      "grad_norm": 2.0969784259796143,
      "learning_rate": 2.4972748160726775e-05,
      "loss": 0.8119,
      "step": 883
    },
    {
      "epoch": 1.655430711610487,
      "grad_norm": 2.1566667556762695,
      "learning_rate": 2.4918244611709436e-05,
      "loss": 0.748,
      "step": 884
    },
    {
      "epoch": 1.6573033707865168,
      "grad_norm": 2.2286458015441895,
      "learning_rate": 2.486374145127886e-05,
      "loss": 0.7987,
      "step": 885
    },
    {
      "epoch": 1.6591760299625467,
      "grad_norm": 2.038698196411133,
      "learning_rate": 2.4809238938490832e-05,
      "loss": 0.7289,
      "step": 886
    },
    {
      "epoch": 1.6610486891385767,
      "grad_norm": 2.1242589950561523,
      "learning_rate": 2.475473733239805e-05,
      "loss": 0.7768,
      "step": 887
    },
    {
      "epoch": 1.6629213483146068,
      "grad_norm": 2.0104684829711914,
      "learning_rate": 2.470023689204893e-05,
      "loss": 0.6394,
      "step": 888
    },
    {
      "epoch": 1.6647940074906367,
      "grad_norm": 2.1858770847320557,
      "learning_rate": 2.464573787648632e-05,
      "loss": 0.771,
      "step": 889
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2.0235722064971924,
      "learning_rate": 2.4591240544746294e-05,
      "loss": 0.7529,
      "step": 890
    },
    {
      "epoch": 1.6685393258426966,
      "grad_norm": 2.4070801734924316,
      "learning_rate": 2.4536745155856954e-05,
      "loss": 0.9296,
      "step": 891
    },
    {
      "epoch": 1.6704119850187267,
      "grad_norm": 2.2075958251953125,
      "learning_rate": 2.4482251968837146e-05,
      "loss": 0.7513,
      "step": 892
    },
    {
      "epoch": 1.6722846441947565,
      "grad_norm": 2.18265700340271,
      "learning_rate": 2.4427761242695238e-05,
      "loss": 0.8016,
      "step": 893
    },
    {
      "epoch": 1.6741573033707864,
      "grad_norm": 2.0772907733917236,
      "learning_rate": 2.437327323642793e-05,
      "loss": 0.8148,
      "step": 894
    },
    {
      "epoch": 1.6760299625468165,
      "grad_norm": 2.095461130142212,
      "learning_rate": 2.4318788209018975e-05,
      "loss": 0.776,
      "step": 895
    },
    {
      "epoch": 1.6779026217228465,
      "grad_norm": 1.8665443658828735,
      "learning_rate": 2.4264306419437977e-05,
      "loss": 0.684,
      "step": 896
    },
    {
      "epoch": 1.6797752808988764,
      "grad_norm": 2.024674654006958,
      "learning_rate": 2.420982812663914e-05,
      "loss": 0.7105,
      "step": 897
    },
    {
      "epoch": 1.6816479400749063,
      "grad_norm": 1.9110828638076782,
      "learning_rate": 2.4155353589560072e-05,
      "loss": 0.7598,
      "step": 898
    },
    {
      "epoch": 1.6835205992509363,
      "grad_norm": 1.9556998014450073,
      "learning_rate": 2.4100883067120475e-05,
      "loss": 0.7715,
      "step": 899
    },
    {
      "epoch": 1.6853932584269664,
      "grad_norm": 2.131227731704712,
      "learning_rate": 2.4046416818221038e-05,
      "loss": 0.8085,
      "step": 900
    },
    {
      "epoch": 1.6872659176029963,
      "grad_norm": 2.155935525894165,
      "learning_rate": 2.3991955101742088e-05,
      "loss": 0.7409,
      "step": 901
    },
    {
      "epoch": 1.6891385767790261,
      "grad_norm": 2.048574686050415,
      "learning_rate": 2.393749817654244e-05,
      "loss": 0.8579,
      "step": 902
    },
    {
      "epoch": 1.6910112359550562,
      "grad_norm": 1.9662325382232666,
      "learning_rate": 2.3883046301458112e-05,
      "loss": 0.732,
      "step": 903
    },
    {
      "epoch": 1.6928838951310863,
      "grad_norm": 2.4095983505249023,
      "learning_rate": 2.3828599735301138e-05,
      "loss": 0.8993,
      "step": 904
    },
    {
      "epoch": 1.6947565543071161,
      "grad_norm": 2.321106433868408,
      "learning_rate": 2.3774158736858303e-05,
      "loss": 0.8288,
      "step": 905
    },
    {
      "epoch": 1.696629213483146,
      "grad_norm": 2.1649022102355957,
      "learning_rate": 2.3719723564889937e-05,
      "loss": 0.8192,
      "step": 906
    },
    {
      "epoch": 1.698501872659176,
      "grad_norm": 1.951302170753479,
      "learning_rate": 2.366529447812868e-05,
      "loss": 0.8189,
      "step": 907
    },
    {
      "epoch": 1.7003745318352061,
      "grad_norm": 1.970429539680481,
      "learning_rate": 2.3610871735278244e-05,
      "loss": 0.74,
      "step": 908
    },
    {
      "epoch": 1.702247191011236,
      "grad_norm": 1.9944065809249878,
      "learning_rate": 2.3556455595012172e-05,
      "loss": 0.8115,
      "step": 909
    },
    {
      "epoch": 1.7041198501872659,
      "grad_norm": 2.107818365097046,
      "learning_rate": 2.3502046315972656e-05,
      "loss": 0.7445,
      "step": 910
    },
    {
      "epoch": 1.7059925093632957,
      "grad_norm": 1.986425518989563,
      "learning_rate": 2.3447644156769254e-05,
      "loss": 0.7441,
      "step": 911
    },
    {
      "epoch": 1.7078651685393258,
      "grad_norm": 2.2982020378112793,
      "learning_rate": 2.3393249375977687e-05,
      "loss": 0.8018,
      "step": 912
    },
    {
      "epoch": 1.7097378277153559,
      "grad_norm": 2.2821106910705566,
      "learning_rate": 2.333886223213862e-05,
      "loss": 0.8098,
      "step": 913
    },
    {
      "epoch": 1.7116104868913857,
      "grad_norm": 2.0549094676971436,
      "learning_rate": 2.3284482983756392e-05,
      "loss": 0.7308,
      "step": 914
    },
    {
      "epoch": 1.7134831460674156,
      "grad_norm": 1.8695205450057983,
      "learning_rate": 2.3230111889297845e-05,
      "loss": 0.7675,
      "step": 915
    },
    {
      "epoch": 1.7153558052434457,
      "grad_norm": 2.1578423976898193,
      "learning_rate": 2.317574920719105e-05,
      "loss": 0.8174,
      "step": 916
    },
    {
      "epoch": 1.7172284644194757,
      "grad_norm": 2.4784488677978516,
      "learning_rate": 2.3121395195824078e-05,
      "loss": 0.8123,
      "step": 917
    },
    {
      "epoch": 1.7191011235955056,
      "grad_norm": 2.1350693702697754,
      "learning_rate": 2.306705011354383e-05,
      "loss": 0.7993,
      "step": 918
    },
    {
      "epoch": 1.7209737827715355,
      "grad_norm": 2.1910626888275146,
      "learning_rate": 2.3012714218654717e-05,
      "loss": 0.8144,
      "step": 919
    },
    {
      "epoch": 1.7228464419475655,
      "grad_norm": 2.5566303730010986,
      "learning_rate": 2.295838776941751e-05,
      "loss": 0.7331,
      "step": 920
    },
    {
      "epoch": 1.7247191011235956,
      "grad_norm": 2.067856550216675,
      "learning_rate": 2.290407102404809e-05,
      "loss": 0.6312,
      "step": 921
    },
    {
      "epoch": 1.7265917602996255,
      "grad_norm": 1.9344468116760254,
      "learning_rate": 2.2849764240716204e-05,
      "loss": 0.8123,
      "step": 922
    },
    {
      "epoch": 1.7284644194756553,
      "grad_norm": 2.2607431411743164,
      "learning_rate": 2.2795467677544235e-05,
      "loss": 0.7844,
      "step": 923
    },
    {
      "epoch": 1.7303370786516854,
      "grad_norm": 2.337343692779541,
      "learning_rate": 2.2741181592606025e-05,
      "loss": 0.7406,
      "step": 924
    },
    {
      "epoch": 1.7322097378277155,
      "grad_norm": 2.412174701690674,
      "learning_rate": 2.2686906243925578e-05,
      "loss": 0.7911,
      "step": 925
    },
    {
      "epoch": 1.7340823970037453,
      "grad_norm": 2.039496660232544,
      "learning_rate": 2.2632641889475887e-05,
      "loss": 0.7603,
      "step": 926
    },
    {
      "epoch": 1.7359550561797752,
      "grad_norm": 2.3114452362060547,
      "learning_rate": 2.257838878717769e-05,
      "loss": 0.8696,
      "step": 927
    },
    {
      "epoch": 1.7378277153558053,
      "grad_norm": 2.187440872192383,
      "learning_rate": 2.2524147194898224e-05,
      "loss": 0.7821,
      "step": 928
    },
    {
      "epoch": 1.7397003745318353,
      "grad_norm": 2.295379638671875,
      "learning_rate": 2.2469917370450032e-05,
      "loss": 0.728,
      "step": 929
    },
    {
      "epoch": 1.7415730337078652,
      "grad_norm": 2.0311923027038574,
      "learning_rate": 2.2415699571589735e-05,
      "loss": 0.7956,
      "step": 930
    },
    {
      "epoch": 1.743445692883895,
      "grad_norm": 2.373204469680786,
      "learning_rate": 2.2361494056016775e-05,
      "loss": 0.7725,
      "step": 931
    },
    {
      "epoch": 1.7453183520599251,
      "grad_norm": 2.6641108989715576,
      "learning_rate": 2.2307301081372224e-05,
      "loss": 0.7921,
      "step": 932
    },
    {
      "epoch": 1.7471910112359552,
      "grad_norm": 2.043346881866455,
      "learning_rate": 2.225312090523754e-05,
      "loss": 0.7312,
      "step": 933
    },
    {
      "epoch": 1.749063670411985,
      "grad_norm": 2.1061863899230957,
      "learning_rate": 2.2198953785133362e-05,
      "loss": 0.7507,
      "step": 934
    },
    {
      "epoch": 1.750936329588015,
      "grad_norm": 1.9608986377716064,
      "learning_rate": 2.2144799978518254e-05,
      "loss": 0.7773,
      "step": 935
    },
    {
      "epoch": 1.7528089887640448,
      "grad_norm": 1.994983434677124,
      "learning_rate": 2.20906597427875e-05,
      "loss": 0.8185,
      "step": 936
    },
    {
      "epoch": 1.7546816479400749,
      "grad_norm": 2.3102517127990723,
      "learning_rate": 2.2036533335271916e-05,
      "loss": 0.8116,
      "step": 937
    },
    {
      "epoch": 1.756554307116105,
      "grad_norm": 2.184103012084961,
      "learning_rate": 2.1982421013236545e-05,
      "loss": 0.9001,
      "step": 938
    },
    {
      "epoch": 1.7584269662921348,
      "grad_norm": 2.599863052368164,
      "learning_rate": 2.1928323033879506e-05,
      "loss": 1.0037,
      "step": 939
    },
    {
      "epoch": 1.7602996254681647,
      "grad_norm": 1.9696508646011353,
      "learning_rate": 2.187423965433075e-05,
      "loss": 0.7016,
      "step": 940
    },
    {
      "epoch": 1.7621722846441947,
      "grad_norm": 2.0636136531829834,
      "learning_rate": 2.1820171131650823e-05,
      "loss": 0.8586,
      "step": 941
    },
    {
      "epoch": 1.7640449438202248,
      "grad_norm": 2.490070104598999,
      "learning_rate": 2.1766117722829658e-05,
      "loss": 0.8899,
      "step": 942
    },
    {
      "epoch": 1.7659176029962547,
      "grad_norm": 2.322782516479492,
      "learning_rate": 2.1712079684785363e-05,
      "loss": 0.7396,
      "step": 943
    },
    {
      "epoch": 1.7677902621722845,
      "grad_norm": 2.0914852619171143,
      "learning_rate": 2.1658057274362965e-05,
      "loss": 0.7646,
      "step": 944
    },
    {
      "epoch": 1.7696629213483146,
      "grad_norm": 1.8365741968154907,
      "learning_rate": 2.160405074833324e-05,
      "loss": 0.6788,
      "step": 945
    },
    {
      "epoch": 1.7715355805243447,
      "grad_norm": 2.087909698486328,
      "learning_rate": 2.155006036339144e-05,
      "loss": 0.7921,
      "step": 946
    },
    {
      "epoch": 1.7734082397003745,
      "grad_norm": 2.052351713180542,
      "learning_rate": 2.1496086376156128e-05,
      "loss": 0.7919,
      "step": 947
    },
    {
      "epoch": 1.7752808988764044,
      "grad_norm": 2.265352725982666,
      "learning_rate": 2.1442129043167874e-05,
      "loss": 0.7533,
      "step": 948
    },
    {
      "epoch": 1.7771535580524345,
      "grad_norm": 2.134551763534546,
      "learning_rate": 2.1388188620888154e-05,
      "loss": 0.7801,
      "step": 949
    },
    {
      "epoch": 1.7790262172284645,
      "grad_norm": 1.9844648838043213,
      "learning_rate": 2.133426536569802e-05,
      "loss": 0.8079,
      "step": 950
    },
    {
      "epoch": 1.7808988764044944,
      "grad_norm": 2.194890022277832,
      "learning_rate": 2.1280359533896954e-05,
      "loss": 0.7938,
      "step": 951
    },
    {
      "epoch": 1.7827715355805243,
      "grad_norm": 3.160144090652466,
      "learning_rate": 2.1226471381701616e-05,
      "loss": 0.8757,
      "step": 952
    },
    {
      "epoch": 1.7846441947565543,
      "grad_norm": 2.228642225265503,
      "learning_rate": 2.1172601165244623e-05,
      "loss": 0.7847,
      "step": 953
    },
    {
      "epoch": 1.7865168539325844,
      "grad_norm": 2.3390843868255615,
      "learning_rate": 2.111874914057336e-05,
      "loss": 0.812,
      "step": 954
    },
    {
      "epoch": 1.7883895131086143,
      "grad_norm": 1.970752477645874,
      "learning_rate": 2.1064915563648734e-05,
      "loss": 0.875,
      "step": 955
    },
    {
      "epoch": 1.7902621722846441,
      "grad_norm": 2.0295329093933105,
      "learning_rate": 2.101110069034398e-05,
      "loss": 0.848,
      "step": 956
    },
    {
      "epoch": 1.7921348314606742,
      "grad_norm": 2.143707275390625,
      "learning_rate": 2.0957304776443438e-05,
      "loss": 0.833,
      "step": 957
    },
    {
      "epoch": 1.7940074906367043,
      "grad_norm": 2.3146674633026123,
      "learning_rate": 2.0903528077641297e-05,
      "loss": 0.8396,
      "step": 958
    },
    {
      "epoch": 1.7958801498127341,
      "grad_norm": 1.8896136283874512,
      "learning_rate": 2.0849770849540445e-05,
      "loss": 0.7542,
      "step": 959
    },
    {
      "epoch": 1.797752808988764,
      "grad_norm": 2.2351839542388916,
      "learning_rate": 2.0796033347651238e-05,
      "loss": 0.8645,
      "step": 960
    },
    {
      "epoch": 1.7996254681647939,
      "grad_norm": 2.1038708686828613,
      "learning_rate": 2.0742315827390244e-05,
      "loss": 0.7924,
      "step": 961
    },
    {
      "epoch": 1.801498127340824,
      "grad_norm": 2.0076451301574707,
      "learning_rate": 2.0688618544079074e-05,
      "loss": 0.7977,
      "step": 962
    },
    {
      "epoch": 1.803370786516854,
      "grad_norm": 2.3624866008758545,
      "learning_rate": 2.0634941752943142e-05,
      "loss": 0.7118,
      "step": 963
    },
    {
      "epoch": 1.8052434456928839,
      "grad_norm": 2.335219144821167,
      "learning_rate": 2.0581285709110475e-05,
      "loss": 0.7292,
      "step": 964
    },
    {
      "epoch": 1.8071161048689137,
      "grad_norm": 2.3958075046539307,
      "learning_rate": 2.0527650667610478e-05,
      "loss": 0.8226,
      "step": 965
    },
    {
      "epoch": 1.8089887640449438,
      "grad_norm": 2.2965612411499023,
      "learning_rate": 2.047403688337272e-05,
      "loss": 0.8249,
      "step": 966
    },
    {
      "epoch": 1.8108614232209739,
      "grad_norm": 2.1609792709350586,
      "learning_rate": 2.042044461122577e-05,
      "loss": 0.7684,
      "step": 967
    },
    {
      "epoch": 1.8127340823970037,
      "grad_norm": 1.9220958948135376,
      "learning_rate": 2.03668741058959e-05,
      "loss": 0.7267,
      "step": 968
    },
    {
      "epoch": 1.8146067415730336,
      "grad_norm": 1.9152714014053345,
      "learning_rate": 2.0313325622005955e-05,
      "loss": 0.7989,
      "step": 969
    },
    {
      "epoch": 1.8164794007490637,
      "grad_norm": 2.099062919616699,
      "learning_rate": 2.0259799414074112e-05,
      "loss": 0.9019,
      "step": 970
    },
    {
      "epoch": 1.8183520599250937,
      "grad_norm": 1.9831821918487549,
      "learning_rate": 2.020629573651266e-05,
      "loss": 0.6922,
      "step": 971
    },
    {
      "epoch": 1.8202247191011236,
      "grad_norm": 2.177271604537964,
      "learning_rate": 2.015281484362679e-05,
      "loss": 0.7319,
      "step": 972
    },
    {
      "epoch": 1.8220973782771535,
      "grad_norm": 2.0315942764282227,
      "learning_rate": 2.0099356989613426e-05,
      "loss": 0.7477,
      "step": 973
    },
    {
      "epoch": 1.8239700374531835,
      "grad_norm": 2.2397162914276123,
      "learning_rate": 2.0045922428559956e-05,
      "loss": 0.7797,
      "step": 974
    },
    {
      "epoch": 1.8258426966292136,
      "grad_norm": 2.290930986404419,
      "learning_rate": 1.9992511414443083e-05,
      "loss": 0.7175,
      "step": 975
    },
    {
      "epoch": 1.8277153558052435,
      "grad_norm": 2.259906768798828,
      "learning_rate": 1.993912420112756e-05,
      "loss": 0.8049,
      "step": 976
    },
    {
      "epoch": 1.8295880149812733,
      "grad_norm": 2.179974317550659,
      "learning_rate": 1.9885761042365056e-05,
      "loss": 0.7848,
      "step": 977
    },
    {
      "epoch": 1.8314606741573034,
      "grad_norm": 2.1355557441711426,
      "learning_rate": 1.983242219179285e-05,
      "loss": 0.7695,
      "step": 978
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 2.229724168777466,
      "learning_rate": 1.9779107902932742e-05,
      "loss": 0.7641,
      "step": 979
    },
    {
      "epoch": 1.8352059925093633,
      "grad_norm": 2.0777928829193115,
      "learning_rate": 1.972581842918974e-05,
      "loss": 0.751,
      "step": 980
    },
    {
      "epoch": 1.8370786516853932,
      "grad_norm": 2.432767391204834,
      "learning_rate": 1.967255402385095e-05,
      "loss": 0.765,
      "step": 981
    },
    {
      "epoch": 1.8389513108614233,
      "grad_norm": 2.220407724380493,
      "learning_rate": 1.961931494008429e-05,
      "loss": 0.7345,
      "step": 982
    },
    {
      "epoch": 1.8408239700374533,
      "grad_norm": 2.320863723754883,
      "learning_rate": 1.9566101430937345e-05,
      "loss": 0.8324,
      "step": 983
    },
    {
      "epoch": 1.8426966292134832,
      "grad_norm": 2.255598306655884,
      "learning_rate": 1.9512913749336133e-05,
      "loss": 0.7409,
      "step": 984
    },
    {
      "epoch": 1.844569288389513,
      "grad_norm": 2.024686098098755,
      "learning_rate": 1.9459752148083905e-05,
      "loss": 0.7443,
      "step": 985
    },
    {
      "epoch": 1.846441947565543,
      "grad_norm": 2.350085735321045,
      "learning_rate": 1.940661687985998e-05,
      "loss": 0.7961,
      "step": 986
    },
    {
      "epoch": 1.848314606741573,
      "grad_norm": 2.1695733070373535,
      "learning_rate": 1.935350819721849e-05,
      "loss": 0.882,
      "step": 987
    },
    {
      "epoch": 1.850187265917603,
      "grad_norm": 2.0919249057769775,
      "learning_rate": 1.9300426352587205e-05,
      "loss": 0.7544,
      "step": 988
    },
    {
      "epoch": 1.852059925093633,
      "grad_norm": 2.35370135307312,
      "learning_rate": 1.9247371598266335e-05,
      "loss": 0.7877,
      "step": 989
    },
    {
      "epoch": 1.8539325842696628,
      "grad_norm": 2.2515347003936768,
      "learning_rate": 1.9194344186427344e-05,
      "loss": 0.7638,
      "step": 990
    },
    {
      "epoch": 1.8558052434456929,
      "grad_norm": 2.039456367492676,
      "learning_rate": 1.9141344369111713e-05,
      "loss": 0.7029,
      "step": 991
    },
    {
      "epoch": 1.857677902621723,
      "grad_norm": 2.167478322982788,
      "learning_rate": 1.908837239822979e-05,
      "loss": 0.823,
      "step": 992
    },
    {
      "epoch": 1.8595505617977528,
      "grad_norm": 2.3313028812408447,
      "learning_rate": 1.9035428525559536e-05,
      "loss": 0.7589,
      "step": 993
    },
    {
      "epoch": 1.8614232209737827,
      "grad_norm": 2.0848116874694824,
      "learning_rate": 1.89825130027454e-05,
      "loss": 0.7314,
      "step": 994
    },
    {
      "epoch": 1.8632958801498127,
      "grad_norm": 1.895466685295105,
      "learning_rate": 1.8929626081297047e-05,
      "loss": 0.8651,
      "step": 995
    },
    {
      "epoch": 1.8651685393258428,
      "grad_norm": 2.1828746795654297,
      "learning_rate": 1.887676801258821e-05,
      "loss": 0.8061,
      "step": 996
    },
    {
      "epoch": 1.8670411985018727,
      "grad_norm": 1.9397919178009033,
      "learning_rate": 1.88239390478555e-05,
      "loss": 0.6977,
      "step": 997
    },
    {
      "epoch": 1.8689138576779025,
      "grad_norm": 2.1129589080810547,
      "learning_rate": 1.8771139438197168e-05,
      "loss": 0.7904,
      "step": 998
    },
    {
      "epoch": 1.8707865168539326,
      "grad_norm": 2.2732760906219482,
      "learning_rate": 1.871836943457196e-05,
      "loss": 0.8377,
      "step": 999
    },
    {
      "epoch": 1.8726591760299627,
      "grad_norm": 2.3531720638275146,
      "learning_rate": 1.866562928779789e-05,
      "loss": 0.7915,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1602,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.626238984989286e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
