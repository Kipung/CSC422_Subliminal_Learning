{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1602,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0018726591760299626,
      "grad_norm": 0.6862156391143799,
      "learning_rate": 0.0,
      "loss": 2.4203,
      "step": 1
    },
    {
      "epoch": 0.003745318352059925,
      "grad_norm": 0.8148097991943359,
      "learning_rate": 3.1055900621118013e-07,
      "loss": 2.4452,
      "step": 2
    },
    {
      "epoch": 0.0056179775280898875,
      "grad_norm": 0.8379510641098022,
      "learning_rate": 6.211180124223603e-07,
      "loss": 2.5417,
      "step": 3
    },
    {
      "epoch": 0.00749063670411985,
      "grad_norm": 0.8270372152328491,
      "learning_rate": 9.316770186335405e-07,
      "loss": 2.6114,
      "step": 4
    },
    {
      "epoch": 0.009363295880149813,
      "grad_norm": 0.7986171841621399,
      "learning_rate": 1.2422360248447205e-06,
      "loss": 2.4141,
      "step": 5
    },
    {
      "epoch": 0.011235955056179775,
      "grad_norm": 0.7237752676010132,
      "learning_rate": 1.5527950310559006e-06,
      "loss": 2.3327,
      "step": 6
    },
    {
      "epoch": 0.013108614232209739,
      "grad_norm": 0.758604109287262,
      "learning_rate": 1.863354037267081e-06,
      "loss": 2.3151,
      "step": 7
    },
    {
      "epoch": 0.0149812734082397,
      "grad_norm": 0.7311609983444214,
      "learning_rate": 2.173913043478261e-06,
      "loss": 2.3612,
      "step": 8
    },
    {
      "epoch": 0.016853932584269662,
      "grad_norm": 0.7172086834907532,
      "learning_rate": 2.484472049689441e-06,
      "loss": 2.3983,
      "step": 9
    },
    {
      "epoch": 0.018726591760299626,
      "grad_norm": 0.7926287651062012,
      "learning_rate": 2.7950310559006214e-06,
      "loss": 2.3883,
      "step": 10
    },
    {
      "epoch": 0.020599250936329586,
      "grad_norm": 0.768844485282898,
      "learning_rate": 3.1055900621118013e-06,
      "loss": 2.3866,
      "step": 11
    },
    {
      "epoch": 0.02247191011235955,
      "grad_norm": 0.6583000421524048,
      "learning_rate": 3.4161490683229816e-06,
      "loss": 2.4266,
      "step": 12
    },
    {
      "epoch": 0.024344569288389514,
      "grad_norm": 0.7227286100387573,
      "learning_rate": 3.726708074534162e-06,
      "loss": 2.4133,
      "step": 13
    },
    {
      "epoch": 0.026217228464419477,
      "grad_norm": 0.7205581068992615,
      "learning_rate": 4.037267080745342e-06,
      "loss": 2.228,
      "step": 14
    },
    {
      "epoch": 0.028089887640449437,
      "grad_norm": 0.7400261759757996,
      "learning_rate": 4.347826086956522e-06,
      "loss": 2.2967,
      "step": 15
    },
    {
      "epoch": 0.0299625468164794,
      "grad_norm": 0.7203162312507629,
      "learning_rate": 4.658385093167702e-06,
      "loss": 2.3766,
      "step": 16
    },
    {
      "epoch": 0.031835205992509365,
      "grad_norm": 0.8467589020729065,
      "learning_rate": 4.968944099378882e-06,
      "loss": 2.5279,
      "step": 17
    },
    {
      "epoch": 0.033707865168539325,
      "grad_norm": 0.8626510500907898,
      "learning_rate": 5.279503105590062e-06,
      "loss": 2.5995,
      "step": 18
    },
    {
      "epoch": 0.035580524344569285,
      "grad_norm": 0.798574686050415,
      "learning_rate": 5.590062111801243e-06,
      "loss": 2.3704,
      "step": 19
    },
    {
      "epoch": 0.03745318352059925,
      "grad_norm": 0.7173448801040649,
      "learning_rate": 5.900621118012423e-06,
      "loss": 2.4181,
      "step": 20
    },
    {
      "epoch": 0.03932584269662921,
      "grad_norm": 0.7280279397964478,
      "learning_rate": 6.2111801242236025e-06,
      "loss": 2.3427,
      "step": 21
    },
    {
      "epoch": 0.04119850187265917,
      "grad_norm": 0.8061512112617493,
      "learning_rate": 6.521739130434783e-06,
      "loss": 2.3761,
      "step": 22
    },
    {
      "epoch": 0.04307116104868914,
      "grad_norm": 0.7702102661132812,
      "learning_rate": 6.832298136645963e-06,
      "loss": 2.3777,
      "step": 23
    },
    {
      "epoch": 0.0449438202247191,
      "grad_norm": 0.909122109413147,
      "learning_rate": 7.142857142857143e-06,
      "loss": 2.3046,
      "step": 24
    },
    {
      "epoch": 0.04681647940074907,
      "grad_norm": 0.798661470413208,
      "learning_rate": 7.453416149068324e-06,
      "loss": 2.5036,
      "step": 25
    },
    {
      "epoch": 0.04868913857677903,
      "grad_norm": 0.6909631490707397,
      "learning_rate": 7.763975155279503e-06,
      "loss": 2.1318,
      "step": 26
    },
    {
      "epoch": 0.05056179775280899,
      "grad_norm": 0.7938671708106995,
      "learning_rate": 8.074534161490684e-06,
      "loss": 2.3157,
      "step": 27
    },
    {
      "epoch": 0.052434456928838954,
      "grad_norm": 0.774236798286438,
      "learning_rate": 8.385093167701864e-06,
      "loss": 2.2112,
      "step": 28
    },
    {
      "epoch": 0.054307116104868915,
      "grad_norm": 0.7078742980957031,
      "learning_rate": 8.695652173913044e-06,
      "loss": 2.2753,
      "step": 29
    },
    {
      "epoch": 0.056179775280898875,
      "grad_norm": 0.7753035426139832,
      "learning_rate": 9.006211180124225e-06,
      "loss": 2.2969,
      "step": 30
    },
    {
      "epoch": 0.05805243445692884,
      "grad_norm": 0.7423351407051086,
      "learning_rate": 9.316770186335403e-06,
      "loss": 2.4018,
      "step": 31
    },
    {
      "epoch": 0.0599250936329588,
      "grad_norm": 0.7763627171516418,
      "learning_rate": 9.627329192546584e-06,
      "loss": 2.3503,
      "step": 32
    },
    {
      "epoch": 0.06179775280898876,
      "grad_norm": 0.6655669212341309,
      "learning_rate": 9.937888198757764e-06,
      "loss": 2.3522,
      "step": 33
    },
    {
      "epoch": 0.06367041198501873,
      "grad_norm": 0.8971410393714905,
      "learning_rate": 1.0248447204968944e-05,
      "loss": 2.3586,
      "step": 34
    },
    {
      "epoch": 0.06554307116104868,
      "grad_norm": 0.6079484820365906,
      "learning_rate": 1.0559006211180125e-05,
      "loss": 2.2063,
      "step": 35
    },
    {
      "epoch": 0.06741573033707865,
      "grad_norm": 0.5551945567131042,
      "learning_rate": 1.0869565217391305e-05,
      "loss": 2.166,
      "step": 36
    },
    {
      "epoch": 0.06928838951310862,
      "grad_norm": 0.6834009289741516,
      "learning_rate": 1.1180124223602485e-05,
      "loss": 2.2067,
      "step": 37
    },
    {
      "epoch": 0.07116104868913857,
      "grad_norm": 0.7723991274833679,
      "learning_rate": 1.1490683229813664e-05,
      "loss": 2.3026,
      "step": 38
    },
    {
      "epoch": 0.07303370786516854,
      "grad_norm": 0.7157897353172302,
      "learning_rate": 1.1801242236024846e-05,
      "loss": 2.2445,
      "step": 39
    },
    {
      "epoch": 0.0749063670411985,
      "grad_norm": 0.5413849949836731,
      "learning_rate": 1.2111801242236026e-05,
      "loss": 2.2555,
      "step": 40
    },
    {
      "epoch": 0.07677902621722846,
      "grad_norm": 0.5947456359863281,
      "learning_rate": 1.2422360248447205e-05,
      "loss": 2.2317,
      "step": 41
    },
    {
      "epoch": 0.07865168539325842,
      "grad_norm": 0.4907327890396118,
      "learning_rate": 1.2732919254658385e-05,
      "loss": 2.0319,
      "step": 42
    },
    {
      "epoch": 0.08052434456928839,
      "grad_norm": 0.4443601369857788,
      "learning_rate": 1.3043478260869566e-05,
      "loss": 2.1947,
      "step": 43
    },
    {
      "epoch": 0.08239700374531835,
      "grad_norm": 0.5595204830169678,
      "learning_rate": 1.3354037267080746e-05,
      "loss": 2.232,
      "step": 44
    },
    {
      "epoch": 0.08426966292134831,
      "grad_norm": 0.4817068874835968,
      "learning_rate": 1.3664596273291926e-05,
      "loss": 2.2159,
      "step": 45
    },
    {
      "epoch": 0.08614232209737828,
      "grad_norm": 0.5093395113945007,
      "learning_rate": 1.3975155279503105e-05,
      "loss": 2.2358,
      "step": 46
    },
    {
      "epoch": 0.08801498127340825,
      "grad_norm": 0.4826807379722595,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 2.0646,
      "step": 47
    },
    {
      "epoch": 0.0898876404494382,
      "grad_norm": 0.5266623497009277,
      "learning_rate": 1.4596273291925466e-05,
      "loss": 2.1837,
      "step": 48
    },
    {
      "epoch": 0.09176029962546817,
      "grad_norm": 0.4682087302207947,
      "learning_rate": 1.4906832298136648e-05,
      "loss": 2.1012,
      "step": 49
    },
    {
      "epoch": 0.09363295880149813,
      "grad_norm": 0.5103937983512878,
      "learning_rate": 1.5217391304347828e-05,
      "loss": 2.2283,
      "step": 50
    },
    {
      "epoch": 0.09550561797752809,
      "grad_norm": 0.4861562252044678,
      "learning_rate": 1.5527950310559007e-05,
      "loss": 2.0432,
      "step": 51
    },
    {
      "epoch": 0.09737827715355805,
      "grad_norm": 0.43148231506347656,
      "learning_rate": 1.5838509316770185e-05,
      "loss": 2.1518,
      "step": 52
    },
    {
      "epoch": 0.09925093632958802,
      "grad_norm": 0.4887496531009674,
      "learning_rate": 1.6149068322981367e-05,
      "loss": 2.2233,
      "step": 53
    },
    {
      "epoch": 0.10112359550561797,
      "grad_norm": 0.4370217025279999,
      "learning_rate": 1.645962732919255e-05,
      "loss": 2.0894,
      "step": 54
    },
    {
      "epoch": 0.10299625468164794,
      "grad_norm": 0.43351879715919495,
      "learning_rate": 1.6770186335403728e-05,
      "loss": 2.1677,
      "step": 55
    },
    {
      "epoch": 0.10486891385767791,
      "grad_norm": 0.42292311787605286,
      "learning_rate": 1.7080745341614907e-05,
      "loss": 2.025,
      "step": 56
    },
    {
      "epoch": 0.10674157303370786,
      "grad_norm": 0.39215144515037537,
      "learning_rate": 1.739130434782609e-05,
      "loss": 1.9801,
      "step": 57
    },
    {
      "epoch": 0.10861423220973783,
      "grad_norm": 0.40920212864875793,
      "learning_rate": 1.7701863354037267e-05,
      "loss": 1.9533,
      "step": 58
    },
    {
      "epoch": 0.1104868913857678,
      "grad_norm": 0.4604615569114685,
      "learning_rate": 1.801242236024845e-05,
      "loss": 2.0494,
      "step": 59
    },
    {
      "epoch": 0.11235955056179775,
      "grad_norm": 0.41583961248397827,
      "learning_rate": 1.8322981366459628e-05,
      "loss": 1.968,
      "step": 60
    },
    {
      "epoch": 0.11423220973782772,
      "grad_norm": 0.4730437695980072,
      "learning_rate": 1.8633540372670807e-05,
      "loss": 1.9796,
      "step": 61
    },
    {
      "epoch": 0.11610486891385768,
      "grad_norm": 0.5084205269813538,
      "learning_rate": 1.894409937888199e-05,
      "loss": 2.0582,
      "step": 62
    },
    {
      "epoch": 0.11797752808988764,
      "grad_norm": 0.43450015783309937,
      "learning_rate": 1.9254658385093167e-05,
      "loss": 2.0278,
      "step": 63
    },
    {
      "epoch": 0.1198501872659176,
      "grad_norm": 0.38430267572402954,
      "learning_rate": 1.956521739130435e-05,
      "loss": 1.9992,
      "step": 64
    },
    {
      "epoch": 0.12172284644194757,
      "grad_norm": 0.4317219853401184,
      "learning_rate": 1.9875776397515528e-05,
      "loss": 1.8488,
      "step": 65
    },
    {
      "epoch": 0.12359550561797752,
      "grad_norm": 0.43305137753486633,
      "learning_rate": 2.0186335403726707e-05,
      "loss": 2.1378,
      "step": 66
    },
    {
      "epoch": 0.1254681647940075,
      "grad_norm": 0.45419126749038696,
      "learning_rate": 2.049689440993789e-05,
      "loss": 1.8591,
      "step": 67
    },
    {
      "epoch": 0.12734082397003746,
      "grad_norm": 0.4167960286140442,
      "learning_rate": 2.080745341614907e-05,
      "loss": 1.9388,
      "step": 68
    },
    {
      "epoch": 0.12921348314606743,
      "grad_norm": 0.5256338119506836,
      "learning_rate": 2.111801242236025e-05,
      "loss": 1.9189,
      "step": 69
    },
    {
      "epoch": 0.13108614232209737,
      "grad_norm": 0.45891547203063965,
      "learning_rate": 2.1428571428571428e-05,
      "loss": 1.8709,
      "step": 70
    },
    {
      "epoch": 0.13295880149812733,
      "grad_norm": 0.4492231607437134,
      "learning_rate": 2.173913043478261e-05,
      "loss": 1.7995,
      "step": 71
    },
    {
      "epoch": 0.1348314606741573,
      "grad_norm": 0.41171613335609436,
      "learning_rate": 2.204968944099379e-05,
      "loss": 1.8441,
      "step": 72
    },
    {
      "epoch": 0.13670411985018727,
      "grad_norm": 0.45569080114364624,
      "learning_rate": 2.236024844720497e-05,
      "loss": 1.7861,
      "step": 73
    },
    {
      "epoch": 0.13857677902621723,
      "grad_norm": 0.395170658826828,
      "learning_rate": 2.2670807453416153e-05,
      "loss": 1.8542,
      "step": 74
    },
    {
      "epoch": 0.1404494382022472,
      "grad_norm": 0.379631906747818,
      "learning_rate": 2.2981366459627328e-05,
      "loss": 1.9839,
      "step": 75
    },
    {
      "epoch": 0.14232209737827714,
      "grad_norm": 0.34769588708877563,
      "learning_rate": 2.329192546583851e-05,
      "loss": 1.8862,
      "step": 76
    },
    {
      "epoch": 0.1441947565543071,
      "grad_norm": 0.3263604938983917,
      "learning_rate": 2.3602484472049692e-05,
      "loss": 1.7279,
      "step": 77
    },
    {
      "epoch": 0.14606741573033707,
      "grad_norm": 0.3614303469657898,
      "learning_rate": 2.391304347826087e-05,
      "loss": 1.7654,
      "step": 78
    },
    {
      "epoch": 0.14794007490636704,
      "grad_norm": 0.35340362787246704,
      "learning_rate": 2.4223602484472053e-05,
      "loss": 1.909,
      "step": 79
    },
    {
      "epoch": 0.149812734082397,
      "grad_norm": 0.33974406123161316,
      "learning_rate": 2.453416149068323e-05,
      "loss": 1.7562,
      "step": 80
    },
    {
      "epoch": 0.15168539325842698,
      "grad_norm": 0.4251735806465149,
      "learning_rate": 2.484472049689441e-05,
      "loss": 1.7551,
      "step": 81
    },
    {
      "epoch": 0.15355805243445692,
      "grad_norm": 0.39633187651634216,
      "learning_rate": 2.515527950310559e-05,
      "loss": 1.7986,
      "step": 82
    },
    {
      "epoch": 0.15543071161048688,
      "grad_norm": 0.3537903130054474,
      "learning_rate": 2.546583850931677e-05,
      "loss": 1.728,
      "step": 83
    },
    {
      "epoch": 0.15730337078651685,
      "grad_norm": 0.3790740966796875,
      "learning_rate": 2.577639751552795e-05,
      "loss": 1.8341,
      "step": 84
    },
    {
      "epoch": 0.15917602996254682,
      "grad_norm": 0.33082810044288635,
      "learning_rate": 2.608695652173913e-05,
      "loss": 1.849,
      "step": 85
    },
    {
      "epoch": 0.16104868913857678,
      "grad_norm": 0.3899897634983063,
      "learning_rate": 2.639751552795031e-05,
      "loss": 1.8481,
      "step": 86
    },
    {
      "epoch": 0.16292134831460675,
      "grad_norm": 0.3592529594898224,
      "learning_rate": 2.6708074534161492e-05,
      "loss": 1.8026,
      "step": 87
    },
    {
      "epoch": 0.1647940074906367,
      "grad_norm": 0.3973512351512909,
      "learning_rate": 2.7018633540372674e-05,
      "loss": 1.7285,
      "step": 88
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.40704816579818726,
      "learning_rate": 2.7329192546583853e-05,
      "loss": 1.8636,
      "step": 89
    },
    {
      "epoch": 0.16853932584269662,
      "grad_norm": 0.35816729068756104,
      "learning_rate": 2.7639751552795035e-05,
      "loss": 1.7675,
      "step": 90
    },
    {
      "epoch": 0.1704119850187266,
      "grad_norm": 0.4020141661167145,
      "learning_rate": 2.795031055900621e-05,
      "loss": 1.7127,
      "step": 91
    },
    {
      "epoch": 0.17228464419475656,
      "grad_norm": 0.35825005173683167,
      "learning_rate": 2.826086956521739e-05,
      "loss": 1.7945,
      "step": 92
    },
    {
      "epoch": 0.17415730337078653,
      "grad_norm": 0.39006906747817993,
      "learning_rate": 2.857142857142857e-05,
      "loss": 1.8575,
      "step": 93
    },
    {
      "epoch": 0.1760299625468165,
      "grad_norm": 0.36737024784088135,
      "learning_rate": 2.8881987577639753e-05,
      "loss": 1.8733,
      "step": 94
    },
    {
      "epoch": 0.17790262172284643,
      "grad_norm": 0.3792107403278351,
      "learning_rate": 2.919254658385093e-05,
      "loss": 1.7267,
      "step": 95
    },
    {
      "epoch": 0.1797752808988764,
      "grad_norm": 0.3475061357021332,
      "learning_rate": 2.9503105590062114e-05,
      "loss": 1.7808,
      "step": 96
    },
    {
      "epoch": 0.18164794007490637,
      "grad_norm": 0.3768819272518158,
      "learning_rate": 2.9813664596273296e-05,
      "loss": 1.7764,
      "step": 97
    },
    {
      "epoch": 0.18352059925093633,
      "grad_norm": 0.3919396698474884,
      "learning_rate": 3.0124223602484474e-05,
      "loss": 1.7765,
      "step": 98
    },
    {
      "epoch": 0.1853932584269663,
      "grad_norm": 0.3803817629814148,
      "learning_rate": 3.0434782608695656e-05,
      "loss": 1.7571,
      "step": 99
    },
    {
      "epoch": 0.18726591760299627,
      "grad_norm": 0.3334600627422333,
      "learning_rate": 3.074534161490684e-05,
      "loss": 1.7877,
      "step": 100
    },
    {
      "epoch": 0.1891385767790262,
      "grad_norm": 0.3750142455101013,
      "learning_rate": 3.1055900621118014e-05,
      "loss": 1.6794,
      "step": 101
    },
    {
      "epoch": 0.19101123595505617,
      "grad_norm": 0.3895450830459595,
      "learning_rate": 3.136645962732919e-05,
      "loss": 1.6766,
      "step": 102
    },
    {
      "epoch": 0.19288389513108614,
      "grad_norm": 0.41611000895500183,
      "learning_rate": 3.167701863354037e-05,
      "loss": 1.8552,
      "step": 103
    },
    {
      "epoch": 0.1947565543071161,
      "grad_norm": 0.3540753722190857,
      "learning_rate": 3.198757763975155e-05,
      "loss": 1.7061,
      "step": 104
    },
    {
      "epoch": 0.19662921348314608,
      "grad_norm": 0.4042249917984009,
      "learning_rate": 3.2298136645962735e-05,
      "loss": 1.8451,
      "step": 105
    },
    {
      "epoch": 0.19850187265917604,
      "grad_norm": 0.3393270671367645,
      "learning_rate": 3.260869565217392e-05,
      "loss": 1.7757,
      "step": 106
    },
    {
      "epoch": 0.20037453183520598,
      "grad_norm": 0.36984241008758545,
      "learning_rate": 3.29192546583851e-05,
      "loss": 1.7598,
      "step": 107
    },
    {
      "epoch": 0.20224719101123595,
      "grad_norm": 0.3981074094772339,
      "learning_rate": 3.3229813664596274e-05,
      "loss": 1.893,
      "step": 108
    },
    {
      "epoch": 0.20411985018726592,
      "grad_norm": 0.3713698387145996,
      "learning_rate": 3.3540372670807456e-05,
      "loss": 1.6592,
      "step": 109
    },
    {
      "epoch": 0.20599250936329588,
      "grad_norm": 0.36082765460014343,
      "learning_rate": 3.385093167701863e-05,
      "loss": 1.6094,
      "step": 110
    },
    {
      "epoch": 0.20786516853932585,
      "grad_norm": 0.4205794632434845,
      "learning_rate": 3.4161490683229814e-05,
      "loss": 1.8439,
      "step": 111
    },
    {
      "epoch": 0.20973782771535582,
      "grad_norm": 0.3949315547943115,
      "learning_rate": 3.4472049689440996e-05,
      "loss": 1.7599,
      "step": 112
    },
    {
      "epoch": 0.21161048689138576,
      "grad_norm": 0.39037972688674927,
      "learning_rate": 3.478260869565218e-05,
      "loss": 1.8136,
      "step": 113
    },
    {
      "epoch": 0.21348314606741572,
      "grad_norm": 0.39995935559272766,
      "learning_rate": 3.509316770186335e-05,
      "loss": 1.8023,
      "step": 114
    },
    {
      "epoch": 0.2153558052434457,
      "grad_norm": 0.4110732674598694,
      "learning_rate": 3.5403726708074535e-05,
      "loss": 1.6947,
      "step": 115
    },
    {
      "epoch": 0.21722846441947566,
      "grad_norm": 0.3799525797367096,
      "learning_rate": 3.571428571428572e-05,
      "loss": 1.7012,
      "step": 116
    },
    {
      "epoch": 0.21910112359550563,
      "grad_norm": 0.38151878118515015,
      "learning_rate": 3.60248447204969e-05,
      "loss": 1.7019,
      "step": 117
    },
    {
      "epoch": 0.2209737827715356,
      "grad_norm": 0.4282132685184479,
      "learning_rate": 3.633540372670808e-05,
      "loss": 1.6298,
      "step": 118
    },
    {
      "epoch": 0.22284644194756553,
      "grad_norm": 0.4020110070705414,
      "learning_rate": 3.6645962732919256e-05,
      "loss": 1.7606,
      "step": 119
    },
    {
      "epoch": 0.2247191011235955,
      "grad_norm": 0.42285481095314026,
      "learning_rate": 3.695652173913043e-05,
      "loss": 1.7406,
      "step": 120
    },
    {
      "epoch": 0.22659176029962547,
      "grad_norm": 0.430306077003479,
      "learning_rate": 3.7267080745341614e-05,
      "loss": 1.7316,
      "step": 121
    },
    {
      "epoch": 0.22846441947565543,
      "grad_norm": 0.47591641545295715,
      "learning_rate": 3.7577639751552796e-05,
      "loss": 1.7344,
      "step": 122
    },
    {
      "epoch": 0.2303370786516854,
      "grad_norm": 0.428207665681839,
      "learning_rate": 3.788819875776398e-05,
      "loss": 1.7873,
      "step": 123
    },
    {
      "epoch": 0.23220973782771537,
      "grad_norm": 0.3839445114135742,
      "learning_rate": 3.819875776397516e-05,
      "loss": 1.7101,
      "step": 124
    },
    {
      "epoch": 0.2340823970037453,
      "grad_norm": 0.4558573067188263,
      "learning_rate": 3.8509316770186335e-05,
      "loss": 1.7485,
      "step": 125
    },
    {
      "epoch": 0.23595505617977527,
      "grad_norm": 0.4245564043521881,
      "learning_rate": 3.881987577639752e-05,
      "loss": 1.8253,
      "step": 126
    },
    {
      "epoch": 0.23782771535580524,
      "grad_norm": 0.4732948839664459,
      "learning_rate": 3.91304347826087e-05,
      "loss": 1.6041,
      "step": 127
    },
    {
      "epoch": 0.2397003745318352,
      "grad_norm": 0.41951197385787964,
      "learning_rate": 3.944099378881988e-05,
      "loss": 1.6994,
      "step": 128
    },
    {
      "epoch": 0.24157303370786518,
      "grad_norm": 0.38071030378341675,
      "learning_rate": 3.9751552795031056e-05,
      "loss": 1.6963,
      "step": 129
    },
    {
      "epoch": 0.24344569288389514,
      "grad_norm": 0.4742838442325592,
      "learning_rate": 4.006211180124224e-05,
      "loss": 1.7073,
      "step": 130
    },
    {
      "epoch": 0.24531835205992508,
      "grad_norm": 0.4745315611362457,
      "learning_rate": 4.0372670807453414e-05,
      "loss": 1.7477,
      "step": 131
    },
    {
      "epoch": 0.24719101123595505,
      "grad_norm": 0.5403038263320923,
      "learning_rate": 4.0683229813664596e-05,
      "loss": 1.7518,
      "step": 132
    },
    {
      "epoch": 0.24906367041198502,
      "grad_norm": 0.49794575572013855,
      "learning_rate": 4.099378881987578e-05,
      "loss": 1.6872,
      "step": 133
    },
    {
      "epoch": 0.250936329588015,
      "grad_norm": 0.4618912637233734,
      "learning_rate": 4.130434782608696e-05,
      "loss": 1.7014,
      "step": 134
    },
    {
      "epoch": 0.25280898876404495,
      "grad_norm": 0.49396389722824097,
      "learning_rate": 4.161490683229814e-05,
      "loss": 1.6686,
      "step": 135
    },
    {
      "epoch": 0.2546816479400749,
      "grad_norm": 0.492561012506485,
      "learning_rate": 4.192546583850932e-05,
      "loss": 1.6132,
      "step": 136
    },
    {
      "epoch": 0.2565543071161049,
      "grad_norm": 0.5913630127906799,
      "learning_rate": 4.22360248447205e-05,
      "loss": 1.7343,
      "step": 137
    },
    {
      "epoch": 0.25842696629213485,
      "grad_norm": 0.50649493932724,
      "learning_rate": 4.254658385093168e-05,
      "loss": 1.7821,
      "step": 138
    },
    {
      "epoch": 0.2602996254681648,
      "grad_norm": 0.5220556855201721,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 1.6846,
      "step": 139
    },
    {
      "epoch": 0.26217228464419473,
      "grad_norm": 0.4311257302761078,
      "learning_rate": 4.316770186335404e-05,
      "loss": 1.7951,
      "step": 140
    },
    {
      "epoch": 0.2640449438202247,
      "grad_norm": 0.4498140215873718,
      "learning_rate": 4.347826086956522e-05,
      "loss": 1.7492,
      "step": 141
    },
    {
      "epoch": 0.26591760299625467,
      "grad_norm": 0.42480704188346863,
      "learning_rate": 4.3788819875776396e-05,
      "loss": 1.5236,
      "step": 142
    },
    {
      "epoch": 0.26779026217228463,
      "grad_norm": 0.42777466773986816,
      "learning_rate": 4.409937888198758e-05,
      "loss": 1.7228,
      "step": 143
    },
    {
      "epoch": 0.2696629213483146,
      "grad_norm": 0.4389907717704773,
      "learning_rate": 4.440993788819876e-05,
      "loss": 1.6845,
      "step": 144
    },
    {
      "epoch": 0.27153558052434457,
      "grad_norm": 0.5106849670410156,
      "learning_rate": 4.472049689440994e-05,
      "loss": 1.7163,
      "step": 145
    },
    {
      "epoch": 0.27340823970037453,
      "grad_norm": 0.45500481128692627,
      "learning_rate": 4.5031055900621124e-05,
      "loss": 1.6811,
      "step": 146
    },
    {
      "epoch": 0.2752808988764045,
      "grad_norm": 0.39502817392349243,
      "learning_rate": 4.5341614906832306e-05,
      "loss": 1.7262,
      "step": 147
    },
    {
      "epoch": 0.27715355805243447,
      "grad_norm": 0.4347379803657532,
      "learning_rate": 4.565217391304348e-05,
      "loss": 1.7845,
      "step": 148
    },
    {
      "epoch": 0.27902621722846443,
      "grad_norm": 0.45603761076927185,
      "learning_rate": 4.5962732919254656e-05,
      "loss": 1.6192,
      "step": 149
    },
    {
      "epoch": 0.2808988764044944,
      "grad_norm": 0.43670642375946045,
      "learning_rate": 4.627329192546584e-05,
      "loss": 1.5388,
      "step": 150
    },
    {
      "epoch": 0.28277153558052437,
      "grad_norm": 0.4410499036312103,
      "learning_rate": 4.658385093167702e-05,
      "loss": 1.6261,
      "step": 151
    },
    {
      "epoch": 0.2846441947565543,
      "grad_norm": 0.44227147102355957,
      "learning_rate": 4.68944099378882e-05,
      "loss": 1.6832,
      "step": 152
    },
    {
      "epoch": 0.28651685393258425,
      "grad_norm": 0.4501180946826935,
      "learning_rate": 4.7204968944099384e-05,
      "loss": 1.4926,
      "step": 153
    },
    {
      "epoch": 0.2883895131086142,
      "grad_norm": 0.46027323603630066,
      "learning_rate": 4.751552795031056e-05,
      "loss": 1.7175,
      "step": 154
    },
    {
      "epoch": 0.2902621722846442,
      "grad_norm": 0.4783925712108612,
      "learning_rate": 4.782608695652174e-05,
      "loss": 1.5639,
      "step": 155
    },
    {
      "epoch": 0.29213483146067415,
      "grad_norm": 0.4869202971458435,
      "learning_rate": 4.8136645962732924e-05,
      "loss": 1.6313,
      "step": 156
    },
    {
      "epoch": 0.2940074906367041,
      "grad_norm": 0.5587906837463379,
      "learning_rate": 4.8447204968944106e-05,
      "loss": 1.536,
      "step": 157
    },
    {
      "epoch": 0.2958801498127341,
      "grad_norm": 0.5088545083999634,
      "learning_rate": 4.875776397515528e-05,
      "loss": 1.6529,
      "step": 158
    },
    {
      "epoch": 0.29775280898876405,
      "grad_norm": 0.4899033010005951,
      "learning_rate": 4.906832298136646e-05,
      "loss": 1.6757,
      "step": 159
    },
    {
      "epoch": 0.299625468164794,
      "grad_norm": 0.6127312183380127,
      "learning_rate": 4.937888198757764e-05,
      "loss": 1.599,
      "step": 160
    },
    {
      "epoch": 0.301498127340824,
      "grad_norm": 0.48272666335105896,
      "learning_rate": 4.968944099378882e-05,
      "loss": 1.6436,
      "step": 161
    },
    {
      "epoch": 0.30337078651685395,
      "grad_norm": 0.5945610404014587,
      "learning_rate": 5e-05,
      "loss": 1.6305,
      "step": 162
    },
    {
      "epoch": 0.3052434456928839,
      "grad_norm": 0.5272984504699707,
      "learning_rate": 4.9999940586980495e-05,
      "loss": 1.7071,
      "step": 163
    },
    {
      "epoch": 0.30711610486891383,
      "grad_norm": 0.4856862723827362,
      "learning_rate": 4.999976234820439e-05,
      "loss": 1.635,
      "step": 164
    },
    {
      "epoch": 0.3089887640449438,
      "grad_norm": 0.5502104759216309,
      "learning_rate": 4.999946528451884e-05,
      "loss": 1.5664,
      "step": 165
    },
    {
      "epoch": 0.31086142322097376,
      "grad_norm": 0.5018648505210876,
      "learning_rate": 4.999904939733581e-05,
      "loss": 1.5629,
      "step": 166
    },
    {
      "epoch": 0.31273408239700373,
      "grad_norm": 0.4030786156654358,
      "learning_rate": 4.999851468863204e-05,
      "loss": 1.6362,
      "step": 167
    },
    {
      "epoch": 0.3146067415730337,
      "grad_norm": 0.4449768662452698,
      "learning_rate": 4.9997861160949e-05,
      "loss": 1.5881,
      "step": 168
    },
    {
      "epoch": 0.31647940074906367,
      "grad_norm": 0.45413538813591003,
      "learning_rate": 4.999708881739296e-05,
      "loss": 1.6601,
      "step": 169
    },
    {
      "epoch": 0.31835205992509363,
      "grad_norm": 0.4422096610069275,
      "learning_rate": 4.9996197661634876e-05,
      "loss": 1.5468,
      "step": 170
    },
    {
      "epoch": 0.3202247191011236,
      "grad_norm": 0.49282556772232056,
      "learning_rate": 4.999518769791046e-05,
      "loss": 1.6043,
      "step": 171
    },
    {
      "epoch": 0.32209737827715357,
      "grad_norm": 0.4960557222366333,
      "learning_rate": 4.999405893102012e-05,
      "loss": 1.6568,
      "step": 172
    },
    {
      "epoch": 0.32397003745318353,
      "grad_norm": 0.46024054288864136,
      "learning_rate": 4.9992811366328926e-05,
      "loss": 1.739,
      "step": 173
    },
    {
      "epoch": 0.3258426966292135,
      "grad_norm": 0.4613288640975952,
      "learning_rate": 4.9991445009766595e-05,
      "loss": 1.5553,
      "step": 174
    },
    {
      "epoch": 0.32771535580524347,
      "grad_norm": 0.4948190152645111,
      "learning_rate": 4.9989959867827486e-05,
      "loss": 1.5604,
      "step": 175
    },
    {
      "epoch": 0.3295880149812734,
      "grad_norm": 0.4752849042415619,
      "learning_rate": 4.998835594757054e-05,
      "loss": 1.6094,
      "step": 176
    },
    {
      "epoch": 0.33146067415730335,
      "grad_norm": 0.44036462903022766,
      "learning_rate": 4.998663325661926e-05,
      "loss": 1.6716,
      "step": 177
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.456157922744751,
      "learning_rate": 4.998479180316166e-05,
      "loss": 1.6842,
      "step": 178
    },
    {
      "epoch": 0.3352059925093633,
      "grad_norm": 0.42126184701919556,
      "learning_rate": 4.998283159595024e-05,
      "loss": 1.5805,
      "step": 179
    },
    {
      "epoch": 0.33707865168539325,
      "grad_norm": 0.4051833748817444,
      "learning_rate": 4.9980752644301964e-05,
      "loss": 1.5423,
      "step": 180
    },
    {
      "epoch": 0.3389513108614232,
      "grad_norm": 0.4825299382209778,
      "learning_rate": 4.997855495809816e-05,
      "loss": 1.6822,
      "step": 181
    },
    {
      "epoch": 0.3408239700374532,
      "grad_norm": 0.4582969546318054,
      "learning_rate": 4.997623854778453e-05,
      "loss": 1.6823,
      "step": 182
    },
    {
      "epoch": 0.34269662921348315,
      "grad_norm": 0.4724233150482178,
      "learning_rate": 4.997380342437107e-05,
      "loss": 1.6624,
      "step": 183
    },
    {
      "epoch": 0.3445692883895131,
      "grad_norm": 0.46797752380371094,
      "learning_rate": 4.997124959943201e-05,
      "loss": 1.6514,
      "step": 184
    },
    {
      "epoch": 0.3464419475655431,
      "grad_norm": 0.4656717777252197,
      "learning_rate": 4.99685770851058e-05,
      "loss": 1.5649,
      "step": 185
    },
    {
      "epoch": 0.34831460674157305,
      "grad_norm": 0.47677087783813477,
      "learning_rate": 4.996578589409501e-05,
      "loss": 1.4873,
      "step": 186
    },
    {
      "epoch": 0.350187265917603,
      "grad_norm": 0.43565914034843445,
      "learning_rate": 4.996287603966627e-05,
      "loss": 1.584,
      "step": 187
    },
    {
      "epoch": 0.352059925093633,
      "grad_norm": 0.4308316111564636,
      "learning_rate": 4.995984753565027e-05,
      "loss": 1.6794,
      "step": 188
    },
    {
      "epoch": 0.3539325842696629,
      "grad_norm": 0.5149612426757812,
      "learning_rate": 4.995670039644159e-05,
      "loss": 1.662,
      "step": 189
    },
    {
      "epoch": 0.35580524344569286,
      "grad_norm": 0.4292680025100708,
      "learning_rate": 4.995343463699872e-05,
      "loss": 1.6711,
      "step": 190
    },
    {
      "epoch": 0.35767790262172283,
      "grad_norm": 0.4777805805206299,
      "learning_rate": 4.9950050272843954e-05,
      "loss": 1.5707,
      "step": 191
    },
    {
      "epoch": 0.3595505617977528,
      "grad_norm": 0.4696762263774872,
      "learning_rate": 4.994654732006331e-05,
      "loss": 1.6115,
      "step": 192
    },
    {
      "epoch": 0.36142322097378277,
      "grad_norm": 0.472688764333725,
      "learning_rate": 4.9942925795306476e-05,
      "loss": 1.514,
      "step": 193
    },
    {
      "epoch": 0.36329588014981273,
      "grad_norm": 0.466297447681427,
      "learning_rate": 4.993918571578671e-05,
      "loss": 1.4418,
      "step": 194
    },
    {
      "epoch": 0.3651685393258427,
      "grad_norm": 0.4818367063999176,
      "learning_rate": 4.993532709928075e-05,
      "loss": 1.588,
      "step": 195
    },
    {
      "epoch": 0.36704119850187267,
      "grad_norm": 0.47317877411842346,
      "learning_rate": 4.993134996412877e-05,
      "loss": 1.7489,
      "step": 196
    },
    {
      "epoch": 0.36891385767790263,
      "grad_norm": 0.4497988820075989,
      "learning_rate": 4.9927254329234266e-05,
      "loss": 1.5385,
      "step": 197
    },
    {
      "epoch": 0.3707865168539326,
      "grad_norm": 0.47293680906295776,
      "learning_rate": 4.9923040214063954e-05,
      "loss": 1.5489,
      "step": 198
    },
    {
      "epoch": 0.37265917602996257,
      "grad_norm": 0.5177451968193054,
      "learning_rate": 4.99187076386477e-05,
      "loss": 1.6589,
      "step": 199
    },
    {
      "epoch": 0.37453183520599254,
      "grad_norm": 0.43428167700767517,
      "learning_rate": 4.991425662357841e-05,
      "loss": 1.5959,
      "step": 200
    },
    {
      "epoch": 0.37640449438202245,
      "grad_norm": 0.4598582684993744,
      "learning_rate": 4.990968719001195e-05,
      "loss": 1.5641,
      "step": 201
    },
    {
      "epoch": 0.3782771535580524,
      "grad_norm": 0.4689788818359375,
      "learning_rate": 4.990499935966702e-05,
      "loss": 1.7046,
      "step": 202
    },
    {
      "epoch": 0.3801498127340824,
      "grad_norm": 0.42370954155921936,
      "learning_rate": 4.9900193154825095e-05,
      "loss": 1.6555,
      "step": 203
    },
    {
      "epoch": 0.38202247191011235,
      "grad_norm": 0.46950820088386536,
      "learning_rate": 4.989526859833024e-05,
      "loss": 1.5568,
      "step": 204
    },
    {
      "epoch": 0.3838951310861423,
      "grad_norm": 0.48908302187919617,
      "learning_rate": 4.989022571358908e-05,
      "loss": 1.5932,
      "step": 205
    },
    {
      "epoch": 0.3857677902621723,
      "grad_norm": 0.5161827206611633,
      "learning_rate": 4.9885064524570665e-05,
      "loss": 1.6172,
      "step": 206
    },
    {
      "epoch": 0.38764044943820225,
      "grad_norm": 0.4839257299900055,
      "learning_rate": 4.987978505580634e-05,
      "loss": 1.7533,
      "step": 207
    },
    {
      "epoch": 0.3895131086142322,
      "grad_norm": 0.45044028759002686,
      "learning_rate": 4.987438733238963e-05,
      "loss": 1.6407,
      "step": 208
    },
    {
      "epoch": 0.3913857677902622,
      "grad_norm": 0.4101540446281433,
      "learning_rate": 4.986887137997615e-05,
      "loss": 1.5913,
      "step": 209
    },
    {
      "epoch": 0.39325842696629215,
      "grad_norm": 0.46702685952186584,
      "learning_rate": 4.986323722478345e-05,
      "loss": 1.615,
      "step": 210
    },
    {
      "epoch": 0.3951310861423221,
      "grad_norm": 0.4679385721683502,
      "learning_rate": 4.98574848935909e-05,
      "loss": 1.6068,
      "step": 211
    },
    {
      "epoch": 0.3970037453183521,
      "grad_norm": 0.43589574098587036,
      "learning_rate": 4.9851614413739566e-05,
      "loss": 1.4171,
      "step": 212
    },
    {
      "epoch": 0.398876404494382,
      "grad_norm": 0.4490697383880615,
      "learning_rate": 4.984562581313209e-05,
      "loss": 1.618,
      "step": 213
    },
    {
      "epoch": 0.40074906367041196,
      "grad_norm": 0.47769695520401,
      "learning_rate": 4.9839519120232534e-05,
      "loss": 1.552,
      "step": 214
    },
    {
      "epoch": 0.40262172284644193,
      "grad_norm": 0.437332421541214,
      "learning_rate": 4.9833294364066266e-05,
      "loss": 1.4827,
      "step": 215
    },
    {
      "epoch": 0.4044943820224719,
      "grad_norm": 0.4764851927757263,
      "learning_rate": 4.982695157421982e-05,
      "loss": 1.6663,
      "step": 216
    },
    {
      "epoch": 0.40636704119850187,
      "grad_norm": 0.4588969349861145,
      "learning_rate": 4.982049078084071e-05,
      "loss": 1.4294,
      "step": 217
    },
    {
      "epoch": 0.40823970037453183,
      "grad_norm": 0.5041879415512085,
      "learning_rate": 4.981391201463739e-05,
      "loss": 1.6674,
      "step": 218
    },
    {
      "epoch": 0.4101123595505618,
      "grad_norm": 0.4578835070133209,
      "learning_rate": 4.980721530687899e-05,
      "loss": 1.737,
      "step": 219
    },
    {
      "epoch": 0.41198501872659177,
      "grad_norm": 0.4716559648513794,
      "learning_rate": 4.980040068939524e-05,
      "loss": 1.4698,
      "step": 220
    },
    {
      "epoch": 0.41385767790262173,
      "grad_norm": 0.4869203567504883,
      "learning_rate": 4.979346819457631e-05,
      "loss": 1.6813,
      "step": 221
    },
    {
      "epoch": 0.4157303370786517,
      "grad_norm": 0.4664956033229828,
      "learning_rate": 4.978641785537264e-05,
      "loss": 1.5939,
      "step": 222
    },
    {
      "epoch": 0.41760299625468167,
      "grad_norm": 0.5354037284851074,
      "learning_rate": 4.9779249705294764e-05,
      "loss": 1.5672,
      "step": 223
    },
    {
      "epoch": 0.41947565543071164,
      "grad_norm": 0.4865328371524811,
      "learning_rate": 4.977196377841321e-05,
      "loss": 1.7284,
      "step": 224
    },
    {
      "epoch": 0.42134831460674155,
      "grad_norm": 0.5116218328475952,
      "learning_rate": 4.9764560109358295e-05,
      "loss": 1.6358,
      "step": 225
    },
    {
      "epoch": 0.4232209737827715,
      "grad_norm": 0.5371300578117371,
      "learning_rate": 4.975703873331996e-05,
      "loss": 1.5181,
      "step": 226
    },
    {
      "epoch": 0.4250936329588015,
      "grad_norm": 0.5277207493782043,
      "learning_rate": 4.974939968604761e-05,
      "loss": 1.5824,
      "step": 227
    },
    {
      "epoch": 0.42696629213483145,
      "grad_norm": 0.44764843583106995,
      "learning_rate": 4.974164300384998e-05,
      "loss": 1.5254,
      "step": 228
    },
    {
      "epoch": 0.4288389513108614,
      "grad_norm": 0.4828822910785675,
      "learning_rate": 4.973376872359488e-05,
      "loss": 1.5898,
      "step": 229
    },
    {
      "epoch": 0.4307116104868914,
      "grad_norm": 0.45093193650245667,
      "learning_rate": 4.972577688270909e-05,
      "loss": 1.6639,
      "step": 230
    },
    {
      "epoch": 0.43258426966292135,
      "grad_norm": 0.4590798020362854,
      "learning_rate": 4.9717667519178176e-05,
      "loss": 1.523,
      "step": 231
    },
    {
      "epoch": 0.4344569288389513,
      "grad_norm": 0.4599727392196655,
      "learning_rate": 4.970944067154627e-05,
      "loss": 1.6248,
      "step": 232
    },
    {
      "epoch": 0.4363295880149813,
      "grad_norm": 0.5347080230712891,
      "learning_rate": 4.970109637891593e-05,
      "loss": 1.5517,
      "step": 233
    },
    {
      "epoch": 0.43820224719101125,
      "grad_norm": 0.46360185742378235,
      "learning_rate": 4.969263468094791e-05,
      "loss": 1.634,
      "step": 234
    },
    {
      "epoch": 0.4400749063670412,
      "grad_norm": 0.5259782671928406,
      "learning_rate": 4.968405561786103e-05,
      "loss": 1.6532,
      "step": 235
    },
    {
      "epoch": 0.4419475655430712,
      "grad_norm": 0.5161953568458557,
      "learning_rate": 4.967535923043192e-05,
      "loss": 1.5077,
      "step": 236
    },
    {
      "epoch": 0.4438202247191011,
      "grad_norm": 0.5532400608062744,
      "learning_rate": 4.966654555999488e-05,
      "loss": 1.5847,
      "step": 237
    },
    {
      "epoch": 0.44569288389513106,
      "grad_norm": 0.45722532272338867,
      "learning_rate": 4.965761464844165e-05,
      "loss": 1.5818,
      "step": 238
    },
    {
      "epoch": 0.44756554307116103,
      "grad_norm": 0.46450093388557434,
      "learning_rate": 4.964856653822122e-05,
      "loss": 1.5668,
      "step": 239
    },
    {
      "epoch": 0.449438202247191,
      "grad_norm": 0.458299845457077,
      "learning_rate": 4.963940127233963e-05,
      "loss": 1.6532,
      "step": 240
    },
    {
      "epoch": 0.45131086142322097,
      "grad_norm": 0.5316346287727356,
      "learning_rate": 4.9630118894359775e-05,
      "loss": 1.6082,
      "step": 241
    },
    {
      "epoch": 0.45318352059925093,
      "grad_norm": 0.4656638503074646,
      "learning_rate": 4.962071944840119e-05,
      "loss": 1.6371,
      "step": 242
    },
    {
      "epoch": 0.4550561797752809,
      "grad_norm": 0.49957895278930664,
      "learning_rate": 4.961120297913983e-05,
      "loss": 1.5427,
      "step": 243
    },
    {
      "epoch": 0.45692883895131087,
      "grad_norm": 0.5062387585639954,
      "learning_rate": 4.960156953180786e-05,
      "loss": 1.6613,
      "step": 244
    },
    {
      "epoch": 0.45880149812734083,
      "grad_norm": 0.48347923159599304,
      "learning_rate": 4.959181915219345e-05,
      "loss": 1.6585,
      "step": 245
    },
    {
      "epoch": 0.4606741573033708,
      "grad_norm": 0.48942047357559204,
      "learning_rate": 4.958195188664058e-05,
      "loss": 1.477,
      "step": 246
    },
    {
      "epoch": 0.46254681647940077,
      "grad_norm": 0.5495572686195374,
      "learning_rate": 4.9571967782048765e-05,
      "loss": 1.6585,
      "step": 247
    },
    {
      "epoch": 0.46441947565543074,
      "grad_norm": 0.4699864983558655,
      "learning_rate": 4.956186688587287e-05,
      "loss": 1.597,
      "step": 248
    },
    {
      "epoch": 0.46629213483146065,
      "grad_norm": 0.5192463397979736,
      "learning_rate": 4.9551649246122854e-05,
      "loss": 1.5463,
      "step": 249
    },
    {
      "epoch": 0.4681647940074906,
      "grad_norm": 0.5062743425369263,
      "learning_rate": 4.954131491136362e-05,
      "loss": 1.4779,
      "step": 250
    },
    {
      "epoch": 0.4700374531835206,
      "grad_norm": 0.5463809967041016,
      "learning_rate": 4.953086393071466e-05,
      "loss": 1.6564,
      "step": 251
    },
    {
      "epoch": 0.47191011235955055,
      "grad_norm": 0.46898511052131653,
      "learning_rate": 4.9520296353849935e-05,
      "loss": 1.4605,
      "step": 252
    },
    {
      "epoch": 0.4737827715355805,
      "grad_norm": 0.49623310565948486,
      "learning_rate": 4.950961223099757e-05,
      "loss": 1.4773,
      "step": 253
    },
    {
      "epoch": 0.4756554307116105,
      "grad_norm": 0.5095434784889221,
      "learning_rate": 4.9498811612939656e-05,
      "loss": 1.639,
      "step": 254
    },
    {
      "epoch": 0.47752808988764045,
      "grad_norm": 0.47205817699432373,
      "learning_rate": 4.948789455101196e-05,
      "loss": 1.6078,
      "step": 255
    },
    {
      "epoch": 0.4794007490636704,
      "grad_norm": 0.5237607955932617,
      "learning_rate": 4.947686109710375e-05,
      "loss": 1.6636,
      "step": 256
    },
    {
      "epoch": 0.4812734082397004,
      "grad_norm": 0.5230041146278381,
      "learning_rate": 4.946571130365748e-05,
      "loss": 1.5452,
      "step": 257
    },
    {
      "epoch": 0.48314606741573035,
      "grad_norm": 0.47485053539276123,
      "learning_rate": 4.9454445223668586e-05,
      "loss": 1.5984,
      "step": 258
    },
    {
      "epoch": 0.4850187265917603,
      "grad_norm": 0.46689414978027344,
      "learning_rate": 4.944306291068522e-05,
      "loss": 1.5536,
      "step": 259
    },
    {
      "epoch": 0.4868913857677903,
      "grad_norm": 0.5557060241699219,
      "learning_rate": 4.943156441880797e-05,
      "loss": 1.7775,
      "step": 260
    },
    {
      "epoch": 0.4887640449438202,
      "grad_norm": 0.5886977314949036,
      "learning_rate": 4.9419949802689666e-05,
      "loss": 1.5,
      "step": 261
    },
    {
      "epoch": 0.49063670411985016,
      "grad_norm": 0.4618476927280426,
      "learning_rate": 4.940821911753505e-05,
      "loss": 1.5962,
      "step": 262
    },
    {
      "epoch": 0.49250936329588013,
      "grad_norm": 0.5321581959724426,
      "learning_rate": 4.939637241910056e-05,
      "loss": 1.5452,
      "step": 263
    },
    {
      "epoch": 0.4943820224719101,
      "grad_norm": 0.4441875219345093,
      "learning_rate": 4.9384409763694045e-05,
      "loss": 1.5536,
      "step": 264
    },
    {
      "epoch": 0.49625468164794007,
      "grad_norm": 0.49354279041290283,
      "learning_rate": 4.937233120817451e-05,
      "loss": 1.5245,
      "step": 265
    },
    {
      "epoch": 0.49812734082397003,
      "grad_norm": 0.44919273257255554,
      "learning_rate": 4.936013680995181e-05,
      "loss": 1.5356,
      "step": 266
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.46806272864341736,
      "learning_rate": 4.9347826626986445e-05,
      "loss": 1.5569,
      "step": 267
    },
    {
      "epoch": 0.50187265917603,
      "grad_norm": 0.5086281299591064,
      "learning_rate": 4.9335400717789233e-05,
      "loss": 1.5238,
      "step": 268
    },
    {
      "epoch": 0.5037453183520599,
      "grad_norm": 0.4898712635040283,
      "learning_rate": 4.932285914142102e-05,
      "loss": 1.5825,
      "step": 269
    },
    {
      "epoch": 0.5056179775280899,
      "grad_norm": 0.4957011640071869,
      "learning_rate": 4.9310201957492444e-05,
      "loss": 1.537,
      "step": 270
    },
    {
      "epoch": 0.5074906367041199,
      "grad_norm": 0.5321497321128845,
      "learning_rate": 4.929742922616363e-05,
      "loss": 1.5635,
      "step": 271
    },
    {
      "epoch": 0.5093632958801498,
      "grad_norm": 0.493096262216568,
      "learning_rate": 4.92845410081439e-05,
      "loss": 1.533,
      "step": 272
    },
    {
      "epoch": 0.5112359550561798,
      "grad_norm": 0.560766875743866,
      "learning_rate": 4.927153736469149e-05,
      "loss": 1.6402,
      "step": 273
    },
    {
      "epoch": 0.5131086142322098,
      "grad_norm": 0.4761003255844116,
      "learning_rate": 4.9258418357613257e-05,
      "loss": 1.5899,
      "step": 274
    },
    {
      "epoch": 0.5149812734082397,
      "grad_norm": 0.5738330483436584,
      "learning_rate": 4.924518404926438e-05,
      "loss": 1.4679,
      "step": 275
    },
    {
      "epoch": 0.5168539325842697,
      "grad_norm": 0.513062059879303,
      "learning_rate": 4.923183450254809e-05,
      "loss": 1.62,
      "step": 276
    },
    {
      "epoch": 0.5187265917602997,
      "grad_norm": 0.4815886914730072,
      "learning_rate": 4.921836978091533e-05,
      "loss": 1.5702,
      "step": 277
    },
    {
      "epoch": 0.5205992509363296,
      "grad_norm": 0.48029497265815735,
      "learning_rate": 4.9204789948364485e-05,
      "loss": 1.4812,
      "step": 278
    },
    {
      "epoch": 0.5224719101123596,
      "grad_norm": 0.5691071152687073,
      "learning_rate": 4.919109506944106e-05,
      "loss": 1.6383,
      "step": 279
    },
    {
      "epoch": 0.5243445692883895,
      "grad_norm": 0.4737044870853424,
      "learning_rate": 4.917728520923738e-05,
      "loss": 1.4664,
      "step": 280
    },
    {
      "epoch": 0.5262172284644194,
      "grad_norm": 0.5554286241531372,
      "learning_rate": 4.916336043339228e-05,
      "loss": 1.6395,
      "step": 281
    },
    {
      "epoch": 0.5280898876404494,
      "grad_norm": 0.49738261103630066,
      "learning_rate": 4.9149320808090826e-05,
      "loss": 1.5225,
      "step": 282
    },
    {
      "epoch": 0.5299625468164794,
      "grad_norm": 0.4999229609966278,
      "learning_rate": 4.913516640006392e-05,
      "loss": 1.5191,
      "step": 283
    },
    {
      "epoch": 0.5318352059925093,
      "grad_norm": 0.5017945170402527,
      "learning_rate": 4.912089727658804e-05,
      "loss": 1.6843,
      "step": 284
    },
    {
      "epoch": 0.5337078651685393,
      "grad_norm": 0.5088334679603577,
      "learning_rate": 4.910651350548494e-05,
      "loss": 1.6308,
      "step": 285
    },
    {
      "epoch": 0.5355805243445693,
      "grad_norm": 0.49007877707481384,
      "learning_rate": 4.909201515512128e-05,
      "loss": 1.5256,
      "step": 286
    },
    {
      "epoch": 0.5374531835205992,
      "grad_norm": 0.4955326318740845,
      "learning_rate": 4.907740229440832e-05,
      "loss": 1.4309,
      "step": 287
    },
    {
      "epoch": 0.5393258426966292,
      "grad_norm": 0.5405351519584656,
      "learning_rate": 4.9062674992801594e-05,
      "loss": 1.5989,
      "step": 288
    },
    {
      "epoch": 0.5411985018726592,
      "grad_norm": 0.4819856584072113,
      "learning_rate": 4.904783332030058e-05,
      "loss": 1.5971,
      "step": 289
    },
    {
      "epoch": 0.5430711610486891,
      "grad_norm": 0.5026049613952637,
      "learning_rate": 4.903287734744836e-05,
      "loss": 1.6114,
      "step": 290
    },
    {
      "epoch": 0.5449438202247191,
      "grad_norm": 0.5226225852966309,
      "learning_rate": 4.90178071453313e-05,
      "loss": 1.6324,
      "step": 291
    },
    {
      "epoch": 0.5468164794007491,
      "grad_norm": 0.48058897256851196,
      "learning_rate": 4.900262278557869e-05,
      "loss": 1.615,
      "step": 292
    },
    {
      "epoch": 0.548689138576779,
      "grad_norm": 0.5486940741539001,
      "learning_rate": 4.898732434036244e-05,
      "loss": 1.6697,
      "step": 293
    },
    {
      "epoch": 0.550561797752809,
      "grad_norm": 0.4935324192047119,
      "learning_rate": 4.897191188239667e-05,
      "loss": 1.6598,
      "step": 294
    },
    {
      "epoch": 0.552434456928839,
      "grad_norm": 0.5295780897140503,
      "learning_rate": 4.895638548493745e-05,
      "loss": 1.5042,
      "step": 295
    },
    {
      "epoch": 0.5543071161048689,
      "grad_norm": 0.5292391777038574,
      "learning_rate": 4.894074522178239e-05,
      "loss": 1.492,
      "step": 296
    },
    {
      "epoch": 0.5561797752808989,
      "grad_norm": 0.5110599994659424,
      "learning_rate": 4.892499116727032e-05,
      "loss": 1.4915,
      "step": 297
    },
    {
      "epoch": 0.5580524344569289,
      "grad_norm": 0.4925561547279358,
      "learning_rate": 4.8909123396280894e-05,
      "loss": 1.527,
      "step": 298
    },
    {
      "epoch": 0.5599250936329588,
      "grad_norm": 0.4807674288749695,
      "learning_rate": 4.88931419842343e-05,
      "loss": 1.4707,
      "step": 299
    },
    {
      "epoch": 0.5617977528089888,
      "grad_norm": 0.5118343234062195,
      "learning_rate": 4.8877047007090847e-05,
      "loss": 1.5532,
      "step": 300
    },
    {
      "epoch": 0.5636704119850188,
      "grad_norm": 0.5719465017318726,
      "learning_rate": 4.8860838541350644e-05,
      "loss": 1.6888,
      "step": 301
    },
    {
      "epoch": 0.5655430711610487,
      "grad_norm": 0.5056440830230713,
      "learning_rate": 4.88445166640532e-05,
      "loss": 1.5043,
      "step": 302
    },
    {
      "epoch": 0.5674157303370787,
      "grad_norm": 0.4933072626590729,
      "learning_rate": 4.882808145277705e-05,
      "loss": 1.4929,
      "step": 303
    },
    {
      "epoch": 0.5692883895131086,
      "grad_norm": 0.5022584199905396,
      "learning_rate": 4.881153298563947e-05,
      "loss": 1.5706,
      "step": 304
    },
    {
      "epoch": 0.5711610486891385,
      "grad_norm": 0.5825725793838501,
      "learning_rate": 4.8794871341296e-05,
      "loss": 1.5716,
      "step": 305
    },
    {
      "epoch": 0.5730337078651685,
      "grad_norm": 0.515328049659729,
      "learning_rate": 4.877809659894012e-05,
      "loss": 1.5658,
      "step": 306
    },
    {
      "epoch": 0.5749063670411985,
      "grad_norm": 0.5501182675361633,
      "learning_rate": 4.876120883830288e-05,
      "loss": 1.5321,
      "step": 307
    },
    {
      "epoch": 0.5767790262172284,
      "grad_norm": 0.5486377477645874,
      "learning_rate": 4.8744208139652526e-05,
      "loss": 1.619,
      "step": 308
    },
    {
      "epoch": 0.5786516853932584,
      "grad_norm": 0.5320622324943542,
      "learning_rate": 4.872709458379407e-05,
      "loss": 1.541,
      "step": 309
    },
    {
      "epoch": 0.5805243445692884,
      "grad_norm": 0.5296087265014648,
      "learning_rate": 4.8709868252068947e-05,
      "loss": 1.5674,
      "step": 310
    },
    {
      "epoch": 0.5823970037453183,
      "grad_norm": 0.5324786901473999,
      "learning_rate": 4.8692529226354635e-05,
      "loss": 1.5989,
      "step": 311
    },
    {
      "epoch": 0.5842696629213483,
      "grad_norm": 0.4859946370124817,
      "learning_rate": 4.8675077589064247e-05,
      "loss": 1.5316,
      "step": 312
    },
    {
      "epoch": 0.5861423220973783,
      "grad_norm": 0.5415195822715759,
      "learning_rate": 4.865751342314614e-05,
      "loss": 1.4011,
      "step": 313
    },
    {
      "epoch": 0.5880149812734082,
      "grad_norm": 0.4781811237335205,
      "learning_rate": 4.863983681208352e-05,
      "loss": 1.4864,
      "step": 314
    },
    {
      "epoch": 0.5898876404494382,
      "grad_norm": 0.5694224834442139,
      "learning_rate": 4.862204783989406e-05,
      "loss": 1.5626,
      "step": 315
    },
    {
      "epoch": 0.5917602996254682,
      "grad_norm": 0.524959146976471,
      "learning_rate": 4.8604146591129485e-05,
      "loss": 1.4944,
      "step": 316
    },
    {
      "epoch": 0.5936329588014981,
      "grad_norm": 0.5282776355743408,
      "learning_rate": 4.8586133150875165e-05,
      "loss": 1.5501,
      "step": 317
    },
    {
      "epoch": 0.5955056179775281,
      "grad_norm": 0.5411516427993774,
      "learning_rate": 4.856800760474973e-05,
      "loss": 1.4506,
      "step": 318
    },
    {
      "epoch": 0.5973782771535581,
      "grad_norm": 0.5545268058776855,
      "learning_rate": 4.8549770038904676e-05,
      "loss": 1.544,
      "step": 319
    },
    {
      "epoch": 0.599250936329588,
      "grad_norm": 0.5431240200996399,
      "learning_rate": 4.853142054002388e-05,
      "loss": 1.5939,
      "step": 320
    },
    {
      "epoch": 0.601123595505618,
      "grad_norm": 0.5375198125839233,
      "learning_rate": 4.851295919532329e-05,
      "loss": 1.5568,
      "step": 321
    },
    {
      "epoch": 0.602996254681648,
      "grad_norm": 0.5294409990310669,
      "learning_rate": 4.849438609255045e-05,
      "loss": 1.4905,
      "step": 322
    },
    {
      "epoch": 0.6048689138576779,
      "grad_norm": 0.5949909687042236,
      "learning_rate": 4.847570131998408e-05,
      "loss": 1.5742,
      "step": 323
    },
    {
      "epoch": 0.6067415730337079,
      "grad_norm": 0.5665595531463623,
      "learning_rate": 4.845690496643368e-05,
      "loss": 1.6187,
      "step": 324
    },
    {
      "epoch": 0.6086142322097379,
      "grad_norm": 0.5187398791313171,
      "learning_rate": 4.8437997121239097e-05,
      "loss": 1.3526,
      "step": 325
    },
    {
      "epoch": 0.6104868913857678,
      "grad_norm": 0.4914095103740692,
      "learning_rate": 4.8418977874270114e-05,
      "loss": 1.4501,
      "step": 326
    },
    {
      "epoch": 0.6123595505617978,
      "grad_norm": 0.5716820359230042,
      "learning_rate": 4.8399847315926e-05,
      "loss": 1.5798,
      "step": 327
    },
    {
      "epoch": 0.6142322097378277,
      "grad_norm": 0.5626665949821472,
      "learning_rate": 4.838060553713509e-05,
      "loss": 1.4428,
      "step": 328
    },
    {
      "epoch": 0.6161048689138576,
      "grad_norm": 0.5564137697219849,
      "learning_rate": 4.836125262935436e-05,
      "loss": 1.4684,
      "step": 329
    },
    {
      "epoch": 0.6179775280898876,
      "grad_norm": 0.5066503286361694,
      "learning_rate": 4.834178868456899e-05,
      "loss": 1.5046,
      "step": 330
    },
    {
      "epoch": 0.6198501872659176,
      "grad_norm": 0.5263340473175049,
      "learning_rate": 4.832221379529191e-05,
      "loss": 1.5071,
      "step": 331
    },
    {
      "epoch": 0.6217228464419475,
      "grad_norm": 0.5045592188835144,
      "learning_rate": 4.830252805456339e-05,
      "loss": 1.4528,
      "step": 332
    },
    {
      "epoch": 0.6235955056179775,
      "grad_norm": 0.557365894317627,
      "learning_rate": 4.8282731555950565e-05,
      "loss": 1.5981,
      "step": 333
    },
    {
      "epoch": 0.6254681647940075,
      "grad_norm": 0.5433781743049622,
      "learning_rate": 4.8262824393547025e-05,
      "loss": 1.5432,
      "step": 334
    },
    {
      "epoch": 0.6273408239700374,
      "grad_norm": 0.63008713722229,
      "learning_rate": 4.824280666197233e-05,
      "loss": 1.6066,
      "step": 335
    },
    {
      "epoch": 0.6292134831460674,
      "grad_norm": 0.5495493412017822,
      "learning_rate": 4.82226784563716e-05,
      "loss": 1.5706,
      "step": 336
    },
    {
      "epoch": 0.6310861423220974,
      "grad_norm": 0.5432385206222534,
      "learning_rate": 4.8202439872415026e-05,
      "loss": 1.5847,
      "step": 337
    },
    {
      "epoch": 0.6329588014981273,
      "grad_norm": 0.5611550807952881,
      "learning_rate": 4.818209100629745e-05,
      "loss": 1.6123,
      "step": 338
    },
    {
      "epoch": 0.6348314606741573,
      "grad_norm": 0.5466058850288391,
      "learning_rate": 4.8161631954737863e-05,
      "loss": 1.5324,
      "step": 339
    },
    {
      "epoch": 0.6367041198501873,
      "grad_norm": 0.5323249101638794,
      "learning_rate": 4.814106281497899e-05,
      "loss": 1.5126,
      "step": 340
    },
    {
      "epoch": 0.6385767790262172,
      "grad_norm": 0.5731992125511169,
      "learning_rate": 4.8120383684786816e-05,
      "loss": 1.532,
      "step": 341
    },
    {
      "epoch": 0.6404494382022472,
      "grad_norm": 0.608988881111145,
      "learning_rate": 4.809959466245011e-05,
      "loss": 1.4548,
      "step": 342
    },
    {
      "epoch": 0.6423220973782772,
      "grad_norm": 0.5370555520057678,
      "learning_rate": 4.807869584677994e-05,
      "loss": 1.4218,
      "step": 343
    },
    {
      "epoch": 0.6441947565543071,
      "grad_norm": 0.5719988346099854,
      "learning_rate": 4.805768733710926e-05,
      "loss": 1.6177,
      "step": 344
    },
    {
      "epoch": 0.6460674157303371,
      "grad_norm": 0.5557801723480225,
      "learning_rate": 4.8036569233292385e-05,
      "loss": 1.6428,
      "step": 345
    },
    {
      "epoch": 0.6479400749063671,
      "grad_norm": 0.4843001067638397,
      "learning_rate": 4.8015341635704535e-05,
      "loss": 1.4056,
      "step": 346
    },
    {
      "epoch": 0.649812734082397,
      "grad_norm": 0.5681276917457581,
      "learning_rate": 4.7994004645241374e-05,
      "loss": 1.5999,
      "step": 347
    },
    {
      "epoch": 0.651685393258427,
      "grad_norm": 0.5715843439102173,
      "learning_rate": 4.79725583633185e-05,
      "loss": 1.56,
      "step": 348
    },
    {
      "epoch": 0.653558052434457,
      "grad_norm": 0.5446491241455078,
      "learning_rate": 4.795100289187099e-05,
      "loss": 1.4255,
      "step": 349
    },
    {
      "epoch": 0.6554307116104869,
      "grad_norm": 0.5114531517028809,
      "learning_rate": 4.792933833335287e-05,
      "loss": 1.5299,
      "step": 350
    },
    {
      "epoch": 0.6573033707865169,
      "grad_norm": 0.5313305854797363,
      "learning_rate": 4.790756479073672e-05,
      "loss": 1.5178,
      "step": 351
    },
    {
      "epoch": 0.6591760299625468,
      "grad_norm": 0.5508351922035217,
      "learning_rate": 4.788568236751307e-05,
      "loss": 1.4731,
      "step": 352
    },
    {
      "epoch": 0.6610486891385767,
      "grad_norm": 0.594277024269104,
      "learning_rate": 4.786369116769e-05,
      "loss": 1.5541,
      "step": 353
    },
    {
      "epoch": 0.6629213483146067,
      "grad_norm": 0.5083456635475159,
      "learning_rate": 4.784159129579259e-05,
      "loss": 1.547,
      "step": 354
    },
    {
      "epoch": 0.6647940074906367,
      "grad_norm": 0.5402792096138,
      "learning_rate": 4.781938285686245e-05,
      "loss": 1.4357,
      "step": 355
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.6513243317604065,
      "learning_rate": 4.779706595645721e-05,
      "loss": 1.393,
      "step": 356
    },
    {
      "epoch": 0.6685393258426966,
      "grad_norm": 0.521493673324585,
      "learning_rate": 4.777464070065004e-05,
      "loss": 1.5471,
      "step": 357
    },
    {
      "epoch": 0.6704119850187266,
      "grad_norm": 0.5384711027145386,
      "learning_rate": 4.775210719602909e-05,
      "loss": 1.6588,
      "step": 358
    },
    {
      "epoch": 0.6722846441947565,
      "grad_norm": 0.5594982504844666,
      "learning_rate": 4.772946554969706e-05,
      "loss": 1.6014,
      "step": 359
    },
    {
      "epoch": 0.6741573033707865,
      "grad_norm": 0.5199458599090576,
      "learning_rate": 4.7706715869270635e-05,
      "loss": 1.5611,
      "step": 360
    },
    {
      "epoch": 0.6760299625468165,
      "grad_norm": 0.5427179336547852,
      "learning_rate": 4.768385826287999e-05,
      "loss": 1.5715,
      "step": 361
    },
    {
      "epoch": 0.6779026217228464,
      "grad_norm": 0.6277655363082886,
      "learning_rate": 4.766089283916828e-05,
      "loss": 1.417,
      "step": 362
    },
    {
      "epoch": 0.6797752808988764,
      "grad_norm": 0.5860987901687622,
      "learning_rate": 4.763781970729111e-05,
      "loss": 1.6172,
      "step": 363
    },
    {
      "epoch": 0.6816479400749064,
      "grad_norm": 0.5309717059135437,
      "learning_rate": 4.761463897691604e-05,
      "loss": 1.4927,
      "step": 364
    },
    {
      "epoch": 0.6835205992509363,
      "grad_norm": 0.5887954235076904,
      "learning_rate": 4.759135075822204e-05,
      "loss": 1.5814,
      "step": 365
    },
    {
      "epoch": 0.6853932584269663,
      "grad_norm": 0.5831718444824219,
      "learning_rate": 4.756795516189899e-05,
      "loss": 1.6638,
      "step": 366
    },
    {
      "epoch": 0.6872659176029963,
      "grad_norm": 0.5498172640800476,
      "learning_rate": 4.7544452299147135e-05,
      "loss": 1.5646,
      "step": 367
    },
    {
      "epoch": 0.6891385767790262,
      "grad_norm": 0.5617245435714722,
      "learning_rate": 4.752084228167654e-05,
      "loss": 1.6406,
      "step": 368
    },
    {
      "epoch": 0.6910112359550562,
      "grad_norm": 0.6075766682624817,
      "learning_rate": 4.7497125221706616e-05,
      "loss": 1.4774,
      "step": 369
    },
    {
      "epoch": 0.6928838951310862,
      "grad_norm": 0.5314571261405945,
      "learning_rate": 4.747330123196553e-05,
      "loss": 1.5417,
      "step": 370
    },
    {
      "epoch": 0.6947565543071161,
      "grad_norm": 0.4999915361404419,
      "learning_rate": 4.74493704256897e-05,
      "loss": 1.4209,
      "step": 371
    },
    {
      "epoch": 0.6966292134831461,
      "grad_norm": 0.6234889626502991,
      "learning_rate": 4.7425332916623225e-05,
      "loss": 1.379,
      "step": 372
    },
    {
      "epoch": 0.6985018726591761,
      "grad_norm": 0.5777297616004944,
      "learning_rate": 4.7401188819017404e-05,
      "loss": 1.4909,
      "step": 373
    },
    {
      "epoch": 0.700374531835206,
      "grad_norm": 0.5890584588050842,
      "learning_rate": 4.737693824763013e-05,
      "loss": 1.694,
      "step": 374
    },
    {
      "epoch": 0.702247191011236,
      "grad_norm": 0.6469169855117798,
      "learning_rate": 4.735258131772538e-05,
      "loss": 1.4692,
      "step": 375
    },
    {
      "epoch": 0.704119850187266,
      "grad_norm": 0.5620260834693909,
      "learning_rate": 4.732811814507264e-05,
      "loss": 1.3838,
      "step": 376
    },
    {
      "epoch": 0.7059925093632958,
      "grad_norm": 0.5589960217475891,
      "learning_rate": 4.73035488459464e-05,
      "loss": 1.5055,
      "step": 377
    },
    {
      "epoch": 0.7078651685393258,
      "grad_norm": 0.5930935740470886,
      "learning_rate": 4.727887353712556e-05,
      "loss": 1.4822,
      "step": 378
    },
    {
      "epoch": 0.7097378277153558,
      "grad_norm": 0.6301910877227783,
      "learning_rate": 4.725409233589288e-05,
      "loss": 1.4867,
      "step": 379
    },
    {
      "epoch": 0.7116104868913857,
      "grad_norm": 0.5961433053016663,
      "learning_rate": 4.722920536003444e-05,
      "loss": 1.5965,
      "step": 380
    },
    {
      "epoch": 0.7134831460674157,
      "grad_norm": 0.6016220450401306,
      "learning_rate": 4.7204212727839085e-05,
      "loss": 1.4334,
      "step": 381
    },
    {
      "epoch": 0.7153558052434457,
      "grad_norm": 0.5298953056335449,
      "learning_rate": 4.717911455809782e-05,
      "loss": 1.4963,
      "step": 382
    },
    {
      "epoch": 0.7172284644194756,
      "grad_norm": 0.634963870048523,
      "learning_rate": 4.7153910970103294e-05,
      "loss": 1.3842,
      "step": 383
    },
    {
      "epoch": 0.7191011235955056,
      "grad_norm": 0.6252214312553406,
      "learning_rate": 4.7128602083649206e-05,
      "loss": 1.5458,
      "step": 384
    },
    {
      "epoch": 0.7209737827715356,
      "grad_norm": 0.5553382039070129,
      "learning_rate": 4.710318801902974e-05,
      "loss": 1.5907,
      "step": 385
    },
    {
      "epoch": 0.7228464419475655,
      "grad_norm": 0.5749030709266663,
      "learning_rate": 4.707766889703902e-05,
      "loss": 1.2863,
      "step": 386
    },
    {
      "epoch": 0.7247191011235955,
      "grad_norm": 0.6939529776573181,
      "learning_rate": 4.705204483897048e-05,
      "loss": 1.4606,
      "step": 387
    },
    {
      "epoch": 0.7265917602996255,
      "grad_norm": 0.6326252222061157,
      "learning_rate": 4.702631596661632e-05,
      "loss": 1.5537,
      "step": 388
    },
    {
      "epoch": 0.7284644194756554,
      "grad_norm": 0.6710822582244873,
      "learning_rate": 4.700048240226697e-05,
      "loss": 1.6065,
      "step": 389
    },
    {
      "epoch": 0.7303370786516854,
      "grad_norm": 0.5391440987586975,
      "learning_rate": 4.697454426871041e-05,
      "loss": 1.5287,
      "step": 390
    },
    {
      "epoch": 0.7322097378277154,
      "grad_norm": 0.5395274758338928,
      "learning_rate": 4.6948501689231674e-05,
      "loss": 1.4536,
      "step": 391
    },
    {
      "epoch": 0.7340823970037453,
      "grad_norm": 0.5978331565856934,
      "learning_rate": 4.6922354787612235e-05,
      "loss": 1.4521,
      "step": 392
    },
    {
      "epoch": 0.7359550561797753,
      "grad_norm": 0.5512676239013672,
      "learning_rate": 4.6896103688129385e-05,
      "loss": 1.3876,
      "step": 393
    },
    {
      "epoch": 0.7378277153558053,
      "grad_norm": 0.5813617706298828,
      "learning_rate": 4.68697485155557e-05,
      "loss": 1.648,
      "step": 394
    },
    {
      "epoch": 0.7397003745318352,
      "grad_norm": 0.5922368764877319,
      "learning_rate": 4.6843289395158416e-05,
      "loss": 1.5353,
      "step": 395
    },
    {
      "epoch": 0.7415730337078652,
      "grad_norm": 0.6054628491401672,
      "learning_rate": 4.6816726452698825e-05,
      "loss": 1.7389,
      "step": 396
    },
    {
      "epoch": 0.7434456928838952,
      "grad_norm": 0.5469337105751038,
      "learning_rate": 4.679005981443169e-05,
      "loss": 1.4832,
      "step": 397
    },
    {
      "epoch": 0.7453183520599251,
      "grad_norm": 0.5802353024482727,
      "learning_rate": 4.676328960710467e-05,
      "loss": 1.4773,
      "step": 398
    },
    {
      "epoch": 0.7471910112359551,
      "grad_norm": 0.6091358065605164,
      "learning_rate": 4.673641595795766e-05,
      "loss": 1.4866,
      "step": 399
    },
    {
      "epoch": 0.7490636704119851,
      "grad_norm": 0.5976897478103638,
      "learning_rate": 4.670943899472222e-05,
      "loss": 1.4852,
      "step": 400
    },
    {
      "epoch": 0.7509363295880149,
      "grad_norm": 0.5847672820091248,
      "learning_rate": 4.6682358845621e-05,
      "loss": 1.4742,
      "step": 401
    },
    {
      "epoch": 0.7528089887640449,
      "grad_norm": 0.6346416473388672,
      "learning_rate": 4.6655175639367064e-05,
      "loss": 1.6046,
      "step": 402
    },
    {
      "epoch": 0.7546816479400749,
      "grad_norm": 0.5431764721870422,
      "learning_rate": 4.6627889505163326e-05,
      "loss": 1.5502,
      "step": 403
    },
    {
      "epoch": 0.7565543071161048,
      "grad_norm": 0.6066280007362366,
      "learning_rate": 4.660050057270191e-05,
      "loss": 1.4949,
      "step": 404
    },
    {
      "epoch": 0.7584269662921348,
      "grad_norm": 0.5906315445899963,
      "learning_rate": 4.657300897216355e-05,
      "loss": 1.4764,
      "step": 405
    },
    {
      "epoch": 0.7602996254681648,
      "grad_norm": 0.666949987411499,
      "learning_rate": 4.6545414834216974e-05,
      "loss": 1.4668,
      "step": 406
    },
    {
      "epoch": 0.7621722846441947,
      "grad_norm": 0.61646568775177,
      "learning_rate": 4.6517718290018246e-05,
      "loss": 1.5146,
      "step": 407
    },
    {
      "epoch": 0.7640449438202247,
      "grad_norm": 0.6014331579208374,
      "learning_rate": 4.648991947121022e-05,
      "loss": 1.5316,
      "step": 408
    },
    {
      "epoch": 0.7659176029962547,
      "grad_norm": 0.5537456274032593,
      "learning_rate": 4.646201850992181e-05,
      "loss": 1.552,
      "step": 409
    },
    {
      "epoch": 0.7677902621722846,
      "grad_norm": 0.5870592594146729,
      "learning_rate": 4.643401553876747e-05,
      "loss": 1.4775,
      "step": 410
    },
    {
      "epoch": 0.7696629213483146,
      "grad_norm": 0.5864881873130798,
      "learning_rate": 4.6405910690846465e-05,
      "loss": 1.5432,
      "step": 411
    },
    {
      "epoch": 0.7715355805243446,
      "grad_norm": 0.6766665577888489,
      "learning_rate": 4.6377704099742316e-05,
      "loss": 1.4974,
      "step": 412
    },
    {
      "epoch": 0.7734082397003745,
      "grad_norm": 0.5693112015724182,
      "learning_rate": 4.634939589952212e-05,
      "loss": 1.4171,
      "step": 413
    },
    {
      "epoch": 0.7752808988764045,
      "grad_norm": 0.5992965698242188,
      "learning_rate": 4.632098622473593e-05,
      "loss": 1.4977,
      "step": 414
    },
    {
      "epoch": 0.7771535580524345,
      "grad_norm": 0.6297586560249329,
      "learning_rate": 4.6292475210416106e-05,
      "loss": 1.472,
      "step": 415
    },
    {
      "epoch": 0.7790262172284644,
      "grad_norm": 0.5348846316337585,
      "learning_rate": 4.62638629920767e-05,
      "loss": 1.6152,
      "step": 416
    },
    {
      "epoch": 0.7808988764044944,
      "grad_norm": 0.6020694375038147,
      "learning_rate": 4.623514970571275e-05,
      "loss": 1.5627,
      "step": 417
    },
    {
      "epoch": 0.7827715355805244,
      "grad_norm": 0.6573021411895752,
      "learning_rate": 4.620633548779972e-05,
      "loss": 1.4135,
      "step": 418
    },
    {
      "epoch": 0.7846441947565543,
      "grad_norm": 0.6045812964439392,
      "learning_rate": 4.6177420475292776e-05,
      "loss": 1.4498,
      "step": 419
    },
    {
      "epoch": 0.7865168539325843,
      "grad_norm": 0.638503909111023,
      "learning_rate": 4.614840480562618e-05,
      "loss": 1.5019,
      "step": 420
    },
    {
      "epoch": 0.7883895131086143,
      "grad_norm": 0.6208992600440979,
      "learning_rate": 4.611928861671261e-05,
      "loss": 1.483,
      "step": 421
    },
    {
      "epoch": 0.7902621722846442,
      "grad_norm": 0.5876051783561707,
      "learning_rate": 4.609007204694252e-05,
      "loss": 1.5103,
      "step": 422
    },
    {
      "epoch": 0.7921348314606742,
      "grad_norm": 0.54677414894104,
      "learning_rate": 4.606075523518348e-05,
      "loss": 1.2845,
      "step": 423
    },
    {
      "epoch": 0.7940074906367042,
      "grad_norm": 0.6255847811698914,
      "learning_rate": 4.6031338320779534e-05,
      "loss": 1.5758,
      "step": 424
    },
    {
      "epoch": 0.795880149812734,
      "grad_norm": 0.6170554161071777,
      "learning_rate": 4.600182144355048e-05,
      "loss": 1.6105,
      "step": 425
    },
    {
      "epoch": 0.797752808988764,
      "grad_norm": 0.5956344604492188,
      "learning_rate": 4.597220474379125e-05,
      "loss": 1.5007,
      "step": 426
    },
    {
      "epoch": 0.799625468164794,
      "grad_norm": 0.7042168974876404,
      "learning_rate": 4.594248836227128e-05,
      "loss": 1.522,
      "step": 427
    },
    {
      "epoch": 0.8014981273408239,
      "grad_norm": 0.6028149127960205,
      "learning_rate": 4.591267244023375e-05,
      "loss": 1.4738,
      "step": 428
    },
    {
      "epoch": 0.8033707865168539,
      "grad_norm": 0.5583547353744507,
      "learning_rate": 4.588275711939497e-05,
      "loss": 1.5399,
      "step": 429
    },
    {
      "epoch": 0.8052434456928839,
      "grad_norm": 0.5945014357566833,
      "learning_rate": 4.585274254194372e-05,
      "loss": 1.4734,
      "step": 430
    },
    {
      "epoch": 0.8071161048689138,
      "grad_norm": 0.6551245450973511,
      "learning_rate": 4.582262885054052e-05,
      "loss": 1.5009,
      "step": 431
    },
    {
      "epoch": 0.8089887640449438,
      "grad_norm": 0.6589844226837158,
      "learning_rate": 4.5792416188317e-05,
      "loss": 1.4321,
      "step": 432
    },
    {
      "epoch": 0.8108614232209738,
      "grad_norm": 0.6131711006164551,
      "learning_rate": 4.57621046988752e-05,
      "loss": 1.4466,
      "step": 433
    },
    {
      "epoch": 0.8127340823970037,
      "grad_norm": 0.6128038167953491,
      "learning_rate": 4.573169452628689e-05,
      "loss": 1.4363,
      "step": 434
    },
    {
      "epoch": 0.8146067415730337,
      "grad_norm": 0.5474967956542969,
      "learning_rate": 4.5701185815092894e-05,
      "loss": 1.4952,
      "step": 435
    },
    {
      "epoch": 0.8164794007490637,
      "grad_norm": 0.6201152205467224,
      "learning_rate": 4.567057871030237e-05,
      "loss": 1.4983,
      "step": 436
    },
    {
      "epoch": 0.8183520599250936,
      "grad_norm": 0.6330716013908386,
      "learning_rate": 4.563987335739216e-05,
      "loss": 1.5516,
      "step": 437
    },
    {
      "epoch": 0.8202247191011236,
      "grad_norm": 0.6635071039199829,
      "learning_rate": 4.560906990230609e-05,
      "loss": 1.5818,
      "step": 438
    },
    {
      "epoch": 0.8220973782771536,
      "grad_norm": 0.6100471615791321,
      "learning_rate": 4.557816849145425e-05,
      "loss": 1.4668,
      "step": 439
    },
    {
      "epoch": 0.8239700374531835,
      "grad_norm": 0.6032434701919556,
      "learning_rate": 4.5547169271712345e-05,
      "loss": 1.4282,
      "step": 440
    },
    {
      "epoch": 0.8258426966292135,
      "grad_norm": 0.6211725473403931,
      "learning_rate": 4.551607239042095e-05,
      "loss": 1.3816,
      "step": 441
    },
    {
      "epoch": 0.8277153558052435,
      "grad_norm": 0.5859488248825073,
      "learning_rate": 4.5484877995384824e-05,
      "loss": 1.469,
      "step": 442
    },
    {
      "epoch": 0.8295880149812734,
      "grad_norm": 0.5409771800041199,
      "learning_rate": 4.545358623487224e-05,
      "loss": 1.5087,
      "step": 443
    },
    {
      "epoch": 0.8314606741573034,
      "grad_norm": 0.6589586734771729,
      "learning_rate": 4.542219725761422e-05,
      "loss": 1.4467,
      "step": 444
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.5983123779296875,
      "learning_rate": 4.539071121280389e-05,
      "loss": 1.5711,
      "step": 445
    },
    {
      "epoch": 0.8352059925093633,
      "grad_norm": 0.5949805974960327,
      "learning_rate": 4.535912825009573e-05,
      "loss": 1.5844,
      "step": 446
    },
    {
      "epoch": 0.8370786516853933,
      "grad_norm": 0.6248378753662109,
      "learning_rate": 4.5327448519604854e-05,
      "loss": 1.6583,
      "step": 447
    },
    {
      "epoch": 0.8389513108614233,
      "grad_norm": 0.6629559993743896,
      "learning_rate": 4.5295672171906364e-05,
      "loss": 1.4547,
      "step": 448
    },
    {
      "epoch": 0.8408239700374532,
      "grad_norm": 0.642305850982666,
      "learning_rate": 4.526379935803455e-05,
      "loss": 1.6212,
      "step": 449
    },
    {
      "epoch": 0.8426966292134831,
      "grad_norm": 0.6411550641059875,
      "learning_rate": 4.5231830229482216e-05,
      "loss": 1.4561,
      "step": 450
    },
    {
      "epoch": 0.8445692883895131,
      "grad_norm": 0.6621387600898743,
      "learning_rate": 4.519976493819996e-05,
      "loss": 1.5752,
      "step": 451
    },
    {
      "epoch": 0.846441947565543,
      "grad_norm": 0.5978270769119263,
      "learning_rate": 4.516760363659546e-05,
      "loss": 1.4158,
      "step": 452
    },
    {
      "epoch": 0.848314606741573,
      "grad_norm": 0.622811496257782,
      "learning_rate": 4.5135346477532694e-05,
      "loss": 1.4832,
      "step": 453
    },
    {
      "epoch": 0.850187265917603,
      "grad_norm": 0.6677396893501282,
      "learning_rate": 4.510299361433129e-05,
      "loss": 1.6562,
      "step": 454
    },
    {
      "epoch": 0.8520599250936329,
      "grad_norm": 0.5924921035766602,
      "learning_rate": 4.507054520076576e-05,
      "loss": 1.482,
      "step": 455
    },
    {
      "epoch": 0.8539325842696629,
      "grad_norm": 0.6880545020103455,
      "learning_rate": 4.503800139106475e-05,
      "loss": 1.4818,
      "step": 456
    },
    {
      "epoch": 0.8558052434456929,
      "grad_norm": 0.6191269755363464,
      "learning_rate": 4.500536233991036e-05,
      "loss": 1.4919,
      "step": 457
    },
    {
      "epoch": 0.8576779026217228,
      "grad_norm": 0.5711967945098877,
      "learning_rate": 4.497262820243733e-05,
      "loss": 1.4673,
      "step": 458
    },
    {
      "epoch": 0.8595505617977528,
      "grad_norm": 0.6937093138694763,
      "learning_rate": 4.49397991342324e-05,
      "loss": 1.5748,
      "step": 459
    },
    {
      "epoch": 0.8614232209737828,
      "grad_norm": 0.6372652649879456,
      "learning_rate": 4.490687529133347e-05,
      "loss": 1.4218,
      "step": 460
    },
    {
      "epoch": 0.8632958801498127,
      "grad_norm": 0.618438720703125,
      "learning_rate": 4.4873856830228956e-05,
      "loss": 1.5222,
      "step": 461
    },
    {
      "epoch": 0.8651685393258427,
      "grad_norm": 0.6350244283676147,
      "learning_rate": 4.4840743907856964e-05,
      "loss": 1.5547,
      "step": 462
    },
    {
      "epoch": 0.8670411985018727,
      "grad_norm": 0.6547641754150391,
      "learning_rate": 4.48075366816046e-05,
      "loss": 1.4594,
      "step": 463
    },
    {
      "epoch": 0.8689138576779026,
      "grad_norm": 0.6580266356468201,
      "learning_rate": 4.477423530930718e-05,
      "loss": 1.499,
      "step": 464
    },
    {
      "epoch": 0.8707865168539326,
      "grad_norm": 0.7121537923812866,
      "learning_rate": 4.474083994924751e-05,
      "loss": 1.7005,
      "step": 465
    },
    {
      "epoch": 0.8726591760299626,
      "grad_norm": 0.6517041325569153,
      "learning_rate": 4.470735076015513e-05,
      "loss": 1.4437,
      "step": 466
    },
    {
      "epoch": 0.8745318352059925,
      "grad_norm": 0.593393087387085,
      "learning_rate": 4.467376790120555e-05,
      "loss": 1.5981,
      "step": 467
    },
    {
      "epoch": 0.8764044943820225,
      "grad_norm": 0.6342930793762207,
      "learning_rate": 4.464009153201949e-05,
      "loss": 1.5633,
      "step": 468
    },
    {
      "epoch": 0.8782771535580525,
      "grad_norm": 0.7305744290351868,
      "learning_rate": 4.460632181266213e-05,
      "loss": 1.6198,
      "step": 469
    },
    {
      "epoch": 0.8801498127340824,
      "grad_norm": 0.6323678493499756,
      "learning_rate": 4.4572458903642354e-05,
      "loss": 1.4569,
      "step": 470
    },
    {
      "epoch": 0.8820224719101124,
      "grad_norm": 0.7257384061813354,
      "learning_rate": 4.4538502965911974e-05,
      "loss": 1.4666,
      "step": 471
    },
    {
      "epoch": 0.8838951310861424,
      "grad_norm": 0.6717608571052551,
      "learning_rate": 4.450445416086498e-05,
      "loss": 1.6413,
      "step": 472
    },
    {
      "epoch": 0.8857677902621723,
      "grad_norm": 0.6249281764030457,
      "learning_rate": 4.447031265033675e-05,
      "loss": 1.5166,
      "step": 473
    },
    {
      "epoch": 0.8876404494382022,
      "grad_norm": 0.6789474487304688,
      "learning_rate": 4.4436078596603305e-05,
      "loss": 1.4981,
      "step": 474
    },
    {
      "epoch": 0.8895131086142322,
      "grad_norm": 0.5573594570159912,
      "learning_rate": 4.440175216238052e-05,
      "loss": 1.3724,
      "step": 475
    },
    {
      "epoch": 0.8913857677902621,
      "grad_norm": 0.6518256664276123,
      "learning_rate": 4.436733351082336e-05,
      "loss": 1.5848,
      "step": 476
    },
    {
      "epoch": 0.8932584269662921,
      "grad_norm": 0.6893317103385925,
      "learning_rate": 4.433282280552512e-05,
      "loss": 1.5203,
      "step": 477
    },
    {
      "epoch": 0.8951310861423221,
      "grad_norm": 0.6540635824203491,
      "learning_rate": 4.429822021051662e-05,
      "loss": 1.448,
      "step": 478
    },
    {
      "epoch": 0.897003745318352,
      "grad_norm": 0.670512855052948,
      "learning_rate": 4.426352589026541e-05,
      "loss": 1.5623,
      "step": 479
    },
    {
      "epoch": 0.898876404494382,
      "grad_norm": 0.6146624684333801,
      "learning_rate": 4.422874000967505e-05,
      "loss": 1.431,
      "step": 480
    },
    {
      "epoch": 0.900749063670412,
      "grad_norm": 0.7091189026832581,
      "learning_rate": 4.419386273408428e-05,
      "loss": 1.4195,
      "step": 481
    },
    {
      "epoch": 0.9026217228464419,
      "grad_norm": 0.6718813180923462,
      "learning_rate": 4.415889422926623e-05,
      "loss": 1.5011,
      "step": 482
    },
    {
      "epoch": 0.9044943820224719,
      "grad_norm": 0.6728365421295166,
      "learning_rate": 4.4123834661427665e-05,
      "loss": 1.5721,
      "step": 483
    },
    {
      "epoch": 0.9063670411985019,
      "grad_norm": 0.7053104639053345,
      "learning_rate": 4.408868419720816e-05,
      "loss": 1.5383,
      "step": 484
    },
    {
      "epoch": 0.9082397003745318,
      "grad_norm": 0.6602563858032227,
      "learning_rate": 4.405344300367934e-05,
      "loss": 1.4445,
      "step": 485
    },
    {
      "epoch": 0.9101123595505618,
      "grad_norm": 0.5956897735595703,
      "learning_rate": 4.4018111248344065e-05,
      "loss": 1.4027,
      "step": 486
    },
    {
      "epoch": 0.9119850187265918,
      "grad_norm": 0.629850447177887,
      "learning_rate": 4.398268909913562e-05,
      "loss": 1.4262,
      "step": 487
    },
    {
      "epoch": 0.9138576779026217,
      "grad_norm": 0.7116520404815674,
      "learning_rate": 4.394717672441697e-05,
      "loss": 1.5055,
      "step": 488
    },
    {
      "epoch": 0.9157303370786517,
      "grad_norm": 0.6138755679130554,
      "learning_rate": 4.39115742929799e-05,
      "loss": 1.4503,
      "step": 489
    },
    {
      "epoch": 0.9176029962546817,
      "grad_norm": 0.6816520094871521,
      "learning_rate": 4.3875881974044256e-05,
      "loss": 1.4864,
      "step": 490
    },
    {
      "epoch": 0.9194756554307116,
      "grad_norm": 0.6560823321342468,
      "learning_rate": 4.384009993725709e-05,
      "loss": 1.4949,
      "step": 491
    },
    {
      "epoch": 0.9213483146067416,
      "grad_norm": 0.6912496089935303,
      "learning_rate": 4.3804228352691935e-05,
      "loss": 1.5359,
      "step": 492
    },
    {
      "epoch": 0.9232209737827716,
      "grad_norm": 0.6866959929466248,
      "learning_rate": 4.3768267390847906e-05,
      "loss": 1.602,
      "step": 493
    },
    {
      "epoch": 0.9250936329588015,
      "grad_norm": 0.7588630318641663,
      "learning_rate": 4.373221722264896e-05,
      "loss": 1.471,
      "step": 494
    },
    {
      "epoch": 0.9269662921348315,
      "grad_norm": 0.6435731649398804,
      "learning_rate": 4.369607801944304e-05,
      "loss": 1.4751,
      "step": 495
    },
    {
      "epoch": 0.9288389513108615,
      "grad_norm": 0.6294752955436707,
      "learning_rate": 4.365984995300129e-05,
      "loss": 1.5511,
      "step": 496
    },
    {
      "epoch": 0.9307116104868914,
      "grad_norm": 0.6278359889984131,
      "learning_rate": 4.36235331955172e-05,
      "loss": 1.5117,
      "step": 497
    },
    {
      "epoch": 0.9325842696629213,
      "grad_norm": 0.6702326536178589,
      "learning_rate": 4.358712791960583e-05,
      "loss": 1.5322,
      "step": 498
    },
    {
      "epoch": 0.9344569288389513,
      "grad_norm": 0.6201515197753906,
      "learning_rate": 4.355063429830298e-05,
      "loss": 1.6384,
      "step": 499
    },
    {
      "epoch": 0.9363295880149812,
      "grad_norm": 0.6531240344047546,
      "learning_rate": 4.351405250506434e-05,
      "loss": 1.5449,
      "step": 500
    },
    {
      "epoch": 0.9382022471910112,
      "grad_norm": 0.7214229702949524,
      "learning_rate": 4.347738271376469e-05,
      "loss": 1.4095,
      "step": 501
    },
    {
      "epoch": 0.9400749063670412,
      "grad_norm": 0.7339584231376648,
      "learning_rate": 4.34406250986971e-05,
      "loss": 1.5277,
      "step": 502
    },
    {
      "epoch": 0.9419475655430711,
      "grad_norm": 0.6827532052993774,
      "learning_rate": 4.3403779834572004e-05,
      "loss": 1.3951,
      "step": 503
    },
    {
      "epoch": 0.9438202247191011,
      "grad_norm": 0.6706835031509399,
      "learning_rate": 4.336684709651649e-05,
      "loss": 1.3231,
      "step": 504
    },
    {
      "epoch": 0.9456928838951311,
      "grad_norm": 0.7201130986213684,
      "learning_rate": 4.33298270600734e-05,
      "loss": 1.5487,
      "step": 505
    },
    {
      "epoch": 0.947565543071161,
      "grad_norm": 0.6623441576957703,
      "learning_rate": 4.3292719901200506e-05,
      "loss": 1.4508,
      "step": 506
    },
    {
      "epoch": 0.949438202247191,
      "grad_norm": 0.6618684530258179,
      "learning_rate": 4.325552579626967e-05,
      "loss": 1.4297,
      "step": 507
    },
    {
      "epoch": 0.951310861423221,
      "grad_norm": 0.6746948957443237,
      "learning_rate": 4.321824492206602e-05,
      "loss": 1.5312,
      "step": 508
    },
    {
      "epoch": 0.9531835205992509,
      "grad_norm": 0.6603359580039978,
      "learning_rate": 4.318087745578711e-05,
      "loss": 1.5126,
      "step": 509
    },
    {
      "epoch": 0.9550561797752809,
      "grad_norm": 0.6962143182754517,
      "learning_rate": 4.314342357504205e-05,
      "loss": 1.4686,
      "step": 510
    },
    {
      "epoch": 0.9569288389513109,
      "grad_norm": 0.7206941246986389,
      "learning_rate": 4.3105883457850694e-05,
      "loss": 1.583,
      "step": 511
    },
    {
      "epoch": 0.9588014981273408,
      "grad_norm": 0.7146534323692322,
      "learning_rate": 4.3068257282642777e-05,
      "loss": 1.4448,
      "step": 512
    },
    {
      "epoch": 0.9606741573033708,
      "grad_norm": 0.7417291402816772,
      "learning_rate": 4.303054522825708e-05,
      "loss": 1.5184,
      "step": 513
    },
    {
      "epoch": 0.9625468164794008,
      "grad_norm": 0.7434136867523193,
      "learning_rate": 4.2992747473940556e-05,
      "loss": 1.4752,
      "step": 514
    },
    {
      "epoch": 0.9644194756554307,
      "grad_norm": 0.6772596836090088,
      "learning_rate": 4.295486419934751e-05,
      "loss": 1.5069,
      "step": 515
    },
    {
      "epoch": 0.9662921348314607,
      "grad_norm": 0.6572761535644531,
      "learning_rate": 4.291689558453871e-05,
      "loss": 1.5511,
      "step": 516
    },
    {
      "epoch": 0.9681647940074907,
      "grad_norm": 0.6936321258544922,
      "learning_rate": 4.287884180998058e-05,
      "loss": 1.5774,
      "step": 517
    },
    {
      "epoch": 0.9700374531835206,
      "grad_norm": 0.6027526259422302,
      "learning_rate": 4.2840703056544273e-05,
      "loss": 1.4038,
      "step": 518
    },
    {
      "epoch": 0.9719101123595506,
      "grad_norm": 0.7000396251678467,
      "learning_rate": 4.280247950550488e-05,
      "loss": 1.4343,
      "step": 519
    },
    {
      "epoch": 0.9737827715355806,
      "grad_norm": 0.6940135359764099,
      "learning_rate": 4.2764171338540516e-05,
      "loss": 1.5539,
      "step": 520
    },
    {
      "epoch": 0.9756554307116105,
      "grad_norm": 0.6481184363365173,
      "learning_rate": 4.27257787377315e-05,
      "loss": 1.5311,
      "step": 521
    },
    {
      "epoch": 0.9775280898876404,
      "grad_norm": 0.6697772145271301,
      "learning_rate": 4.2687301885559465e-05,
      "loss": 1.4571,
      "step": 522
    },
    {
      "epoch": 0.9794007490636704,
      "grad_norm": 0.7143083214759827,
      "learning_rate": 4.264874096490647e-05,
      "loss": 1.6253,
      "step": 523
    },
    {
      "epoch": 0.9812734082397003,
      "grad_norm": 0.7664021849632263,
      "learning_rate": 4.26100961590542e-05,
      "loss": 1.5587,
      "step": 524
    },
    {
      "epoch": 0.9831460674157303,
      "grad_norm": 0.6550028920173645,
      "learning_rate": 4.2571367651683e-05,
      "loss": 1.5344,
      "step": 525
    },
    {
      "epoch": 0.9850187265917603,
      "grad_norm": 0.6988648176193237,
      "learning_rate": 4.2532555626871077e-05,
      "loss": 1.4462,
      "step": 526
    },
    {
      "epoch": 0.9868913857677902,
      "grad_norm": 0.719947099685669,
      "learning_rate": 4.249366026909361e-05,
      "loss": 1.5693,
      "step": 527
    },
    {
      "epoch": 0.9887640449438202,
      "grad_norm": 0.7082712650299072,
      "learning_rate": 4.245468176322184e-05,
      "loss": 1.5984,
      "step": 528
    },
    {
      "epoch": 0.9906367041198502,
      "grad_norm": 0.6586251854896545,
      "learning_rate": 4.2415620294522235e-05,
      "loss": 1.5713,
      "step": 529
    },
    {
      "epoch": 0.9925093632958801,
      "grad_norm": 0.6382163166999817,
      "learning_rate": 4.2376476048655576e-05,
      "loss": 1.5025,
      "step": 530
    },
    {
      "epoch": 0.9943820224719101,
      "grad_norm": 0.6316239833831787,
      "learning_rate": 4.233724921167609e-05,
      "loss": 1.4034,
      "step": 531
    },
    {
      "epoch": 0.9962546816479401,
      "grad_norm": 0.7022373080253601,
      "learning_rate": 4.2297939970030554e-05,
      "loss": 1.5453,
      "step": 532
    },
    {
      "epoch": 0.99812734082397,
      "grad_norm": 0.626952588558197,
      "learning_rate": 4.225854851055744e-05,
      "loss": 1.4651,
      "step": 533
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7148725390434265,
      "learning_rate": 4.2219075020485975e-05,
      "loss": 1.4984,
      "step": 534
    },
    {
      "epoch": 1.0018726591760299,
      "grad_norm": 0.6891397833824158,
      "learning_rate": 4.217951968743533e-05,
      "loss": 1.4486,
      "step": 535
    },
    {
      "epoch": 1.00374531835206,
      "grad_norm": 0.6947936415672302,
      "learning_rate": 4.213988269941362e-05,
      "loss": 1.5325,
      "step": 536
    },
    {
      "epoch": 1.0056179775280898,
      "grad_norm": 0.6955282092094421,
      "learning_rate": 4.2100164244817095e-05,
      "loss": 1.3962,
      "step": 537
    },
    {
      "epoch": 1.0074906367041199,
      "grad_norm": 0.6879523396492004,
      "learning_rate": 4.206036451242924e-05,
      "loss": 1.4946,
      "step": 538
    },
    {
      "epoch": 1.0093632958801497,
      "grad_norm": 0.642933189868927,
      "learning_rate": 4.2020483691419824e-05,
      "loss": 1.3244,
      "step": 539
    },
    {
      "epoch": 1.0112359550561798,
      "grad_norm": 0.704836905002594,
      "learning_rate": 4.1980521971344044e-05,
      "loss": 1.3854,
      "step": 540
    },
    {
      "epoch": 1.0131086142322097,
      "grad_norm": 0.6783121824264526,
      "learning_rate": 4.194047954214163e-05,
      "loss": 1.3653,
      "step": 541
    },
    {
      "epoch": 1.0149812734082397,
      "grad_norm": 0.6919143795967102,
      "learning_rate": 4.1900356594135895e-05,
      "loss": 1.4931,
      "step": 542
    },
    {
      "epoch": 1.0168539325842696,
      "grad_norm": 0.7178525924682617,
      "learning_rate": 4.1860153318032893e-05,
      "loss": 1.4314,
      "step": 543
    },
    {
      "epoch": 1.0187265917602997,
      "grad_norm": 0.6869985461235046,
      "learning_rate": 4.181986990492045e-05,
      "loss": 1.4695,
      "step": 544
    },
    {
      "epoch": 1.0205992509363295,
      "grad_norm": 0.7244499325752258,
      "learning_rate": 4.177950654626732e-05,
      "loss": 1.3768,
      "step": 545
    },
    {
      "epoch": 1.0224719101123596,
      "grad_norm": 0.7618048191070557,
      "learning_rate": 4.173906343392221e-05,
      "loss": 1.4357,
      "step": 546
    },
    {
      "epoch": 1.0243445692883895,
      "grad_norm": 0.7166584730148315,
      "learning_rate": 4.169854076011292e-05,
      "loss": 1.4077,
      "step": 547
    },
    {
      "epoch": 1.0262172284644195,
      "grad_norm": 0.7310293316841125,
      "learning_rate": 4.1657938717445414e-05,
      "loss": 1.3619,
      "step": 548
    },
    {
      "epoch": 1.0280898876404494,
      "grad_norm": 0.7438865900039673,
      "learning_rate": 4.1617257498902875e-05,
      "loss": 1.4713,
      "step": 549
    },
    {
      "epoch": 1.0299625468164795,
      "grad_norm": 0.6939242482185364,
      "learning_rate": 4.157649729784483e-05,
      "loss": 1.3278,
      "step": 550
    },
    {
      "epoch": 1.0318352059925093,
      "grad_norm": 0.6829627156257629,
      "learning_rate": 4.15356583080062e-05,
      "loss": 1.4106,
      "step": 551
    },
    {
      "epoch": 1.0337078651685394,
      "grad_norm": 0.7669718265533447,
      "learning_rate": 4.149474072349641e-05,
      "loss": 1.4607,
      "step": 552
    },
    {
      "epoch": 1.0355805243445693,
      "grad_norm": 0.7730328440666199,
      "learning_rate": 4.145374473879843e-05,
      "loss": 1.478,
      "step": 553
    },
    {
      "epoch": 1.0374531835205993,
      "grad_norm": 0.7776846885681152,
      "learning_rate": 4.1412670548767895e-05,
      "loss": 1.5193,
      "step": 554
    },
    {
      "epoch": 1.0393258426966292,
      "grad_norm": 0.7864179015159607,
      "learning_rate": 4.137151834863213e-05,
      "loss": 1.382,
      "step": 555
    },
    {
      "epoch": 1.0411985018726593,
      "grad_norm": 0.746958315372467,
      "learning_rate": 4.1330288333989245e-05,
      "loss": 1.5519,
      "step": 556
    },
    {
      "epoch": 1.0430711610486891,
      "grad_norm": 0.7254301309585571,
      "learning_rate": 4.128898070080722e-05,
      "loss": 1.4443,
      "step": 557
    },
    {
      "epoch": 1.0449438202247192,
      "grad_norm": 0.7767220139503479,
      "learning_rate": 4.124759564542295e-05,
      "loss": 1.3506,
      "step": 558
    },
    {
      "epoch": 1.046816479400749,
      "grad_norm": 0.7620042562484741,
      "learning_rate": 4.120613336454133e-05,
      "loss": 1.3741,
      "step": 559
    },
    {
      "epoch": 1.048689138576779,
      "grad_norm": 0.6959409713745117,
      "learning_rate": 4.1164594055234306e-05,
      "loss": 1.3782,
      "step": 560
    },
    {
      "epoch": 1.050561797752809,
      "grad_norm": 0.7756752967834473,
      "learning_rate": 4.112297791493993e-05,
      "loss": 1.3508,
      "step": 561
    },
    {
      "epoch": 1.0524344569288389,
      "grad_norm": 0.7723105549812317,
      "learning_rate": 4.108128514146144e-05,
      "loss": 1.463,
      "step": 562
    },
    {
      "epoch": 1.054307116104869,
      "grad_norm": 0.7260198593139648,
      "learning_rate": 4.103951593296634e-05,
      "loss": 1.5069,
      "step": 563
    },
    {
      "epoch": 1.0561797752808988,
      "grad_norm": 0.7454487681388855,
      "learning_rate": 4.09976704879854e-05,
      "loss": 1.3229,
      "step": 564
    },
    {
      "epoch": 1.0580524344569289,
      "grad_norm": 0.8882701396942139,
      "learning_rate": 4.095574900541177e-05,
      "loss": 1.3594,
      "step": 565
    },
    {
      "epoch": 1.0599250936329587,
      "grad_norm": 0.7228800058364868,
      "learning_rate": 4.09137516845e-05,
      "loss": 1.3671,
      "step": 566
    },
    {
      "epoch": 1.0617977528089888,
      "grad_norm": 0.7986570000648499,
      "learning_rate": 4.0871678724865084e-05,
      "loss": 1.3359,
      "step": 567
    },
    {
      "epoch": 1.0636704119850187,
      "grad_norm": 0.7685492038726807,
      "learning_rate": 4.0829530326481566e-05,
      "loss": 1.3174,
      "step": 568
    },
    {
      "epoch": 1.0655430711610487,
      "grad_norm": 0.8020349144935608,
      "learning_rate": 4.078730668968253e-05,
      "loss": 1.3528,
      "step": 569
    },
    {
      "epoch": 1.0674157303370786,
      "grad_norm": 0.7628270387649536,
      "learning_rate": 4.074500801515867e-05,
      "loss": 1.3886,
      "step": 570
    },
    {
      "epoch": 1.0692883895131087,
      "grad_norm": 0.8412033915519714,
      "learning_rate": 4.070263450395735e-05,
      "loss": 1.4245,
      "step": 571
    },
    {
      "epoch": 1.0711610486891385,
      "grad_norm": 0.766261637210846,
      "learning_rate": 4.0660186357481625e-05,
      "loss": 1.3385,
      "step": 572
    },
    {
      "epoch": 1.0730337078651686,
      "grad_norm": 0.7730520963668823,
      "learning_rate": 4.0617663777489314e-05,
      "loss": 1.4246,
      "step": 573
    },
    {
      "epoch": 1.0749063670411985,
      "grad_norm": 0.7616971731185913,
      "learning_rate": 4.0575066966091984e-05,
      "loss": 1.3891,
      "step": 574
    },
    {
      "epoch": 1.0767790262172285,
      "grad_norm": 0.8047739863395691,
      "learning_rate": 4.053239612575407e-05,
      "loss": 1.2941,
      "step": 575
    },
    {
      "epoch": 1.0786516853932584,
      "grad_norm": 0.7657771706581116,
      "learning_rate": 4.0489651459291844e-05,
      "loss": 1.466,
      "step": 576
    },
    {
      "epoch": 1.0805243445692885,
      "grad_norm": 0.7754883766174316,
      "learning_rate": 4.0446833169872475e-05,
      "loss": 1.4241,
      "step": 577
    },
    {
      "epoch": 1.0823970037453183,
      "grad_norm": 0.7506872415542603,
      "learning_rate": 4.040394146101307e-05,
      "loss": 1.3569,
      "step": 578
    },
    {
      "epoch": 1.0842696629213484,
      "grad_norm": 0.727569043636322,
      "learning_rate": 4.036097653657972e-05,
      "loss": 1.3652,
      "step": 579
    },
    {
      "epoch": 1.0861423220973783,
      "grad_norm": 0.809912919998169,
      "learning_rate": 4.031793860078649e-05,
      "loss": 1.3253,
      "step": 580
    },
    {
      "epoch": 1.0880149812734083,
      "grad_norm": 0.7699735760688782,
      "learning_rate": 4.0274827858194466e-05,
      "loss": 1.4477,
      "step": 581
    },
    {
      "epoch": 1.0898876404494382,
      "grad_norm": 0.7341641187667847,
      "learning_rate": 4.023164451371081e-05,
      "loss": 1.5246,
      "step": 582
    },
    {
      "epoch": 1.0917602996254683,
      "grad_norm": 0.7757768630981445,
      "learning_rate": 4.018838877258775e-05,
      "loss": 1.3615,
      "step": 583
    },
    {
      "epoch": 1.0936329588014981,
      "grad_norm": 0.7326687574386597,
      "learning_rate": 4.0145060840421625e-05,
      "loss": 1.4822,
      "step": 584
    },
    {
      "epoch": 1.095505617977528,
      "grad_norm": 0.7657328248023987,
      "learning_rate": 4.0101660923151895e-05,
      "loss": 1.3768,
      "step": 585
    },
    {
      "epoch": 1.097378277153558,
      "grad_norm": 0.8102887868881226,
      "learning_rate": 4.0058189227060163e-05,
      "loss": 1.2385,
      "step": 586
    },
    {
      "epoch": 1.099250936329588,
      "grad_norm": 0.7562721371650696,
      "learning_rate": 4.001464595876923e-05,
      "loss": 1.3826,
      "step": 587
    },
    {
      "epoch": 1.101123595505618,
      "grad_norm": 0.7155653834342957,
      "learning_rate": 3.997103132524202e-05,
      "loss": 1.4366,
      "step": 588
    },
    {
      "epoch": 1.1029962546816479,
      "grad_norm": 0.7673690319061279,
      "learning_rate": 3.9927345533780745e-05,
      "loss": 1.3124,
      "step": 589
    },
    {
      "epoch": 1.104868913857678,
      "grad_norm": 0.7828382849693298,
      "learning_rate": 3.988358879202576e-05,
      "loss": 1.3192,
      "step": 590
    },
    {
      "epoch": 1.1067415730337078,
      "grad_norm": 0.8262901306152344,
      "learning_rate": 3.9839761307954675e-05,
      "loss": 1.3036,
      "step": 591
    },
    {
      "epoch": 1.1086142322097379,
      "grad_norm": 0.835132896900177,
      "learning_rate": 3.9795863289881354e-05,
      "loss": 1.3613,
      "step": 592
    },
    {
      "epoch": 1.1104868913857677,
      "grad_norm": 0.8286422491073608,
      "learning_rate": 3.9751894946454895e-05,
      "loss": 1.4756,
      "step": 593
    },
    {
      "epoch": 1.1123595505617978,
      "grad_norm": 0.798121452331543,
      "learning_rate": 3.970785648665867e-05,
      "loss": 1.5124,
      "step": 594
    },
    {
      "epoch": 1.1142322097378277,
      "grad_norm": 0.7692019939422607,
      "learning_rate": 3.96637481198093e-05,
      "loss": 1.3303,
      "step": 595
    },
    {
      "epoch": 1.1161048689138577,
      "grad_norm": 0.7878699898719788,
      "learning_rate": 3.9619570055555685e-05,
      "loss": 1.3692,
      "step": 596
    },
    {
      "epoch": 1.1179775280898876,
      "grad_norm": 0.8543082475662231,
      "learning_rate": 3.9575322503878014e-05,
      "loss": 1.4095,
      "step": 597
    },
    {
      "epoch": 1.1198501872659177,
      "grad_norm": 0.8603577017784119,
      "learning_rate": 3.953100567508672e-05,
      "loss": 1.4237,
      "step": 598
    },
    {
      "epoch": 1.1217228464419475,
      "grad_norm": 0.7675649523735046,
      "learning_rate": 3.9486619779821555e-05,
      "loss": 1.3503,
      "step": 599
    },
    {
      "epoch": 1.1235955056179776,
      "grad_norm": 0.7979558706283569,
      "learning_rate": 3.94421650290505e-05,
      "loss": 1.483,
      "step": 600
    },
    {
      "epoch": 1.1254681647940075,
      "grad_norm": 0.823187530040741,
      "learning_rate": 3.9397641634068836e-05,
      "loss": 1.3011,
      "step": 601
    },
    {
      "epoch": 1.1273408239700375,
      "grad_norm": 0.8509692549705505,
      "learning_rate": 3.935304980649813e-05,
      "loss": 1.3763,
      "step": 602
    },
    {
      "epoch": 1.1292134831460674,
      "grad_norm": 0.8074368834495544,
      "learning_rate": 3.930838975828518e-05,
      "loss": 1.3456,
      "step": 603
    },
    {
      "epoch": 1.1310861423220975,
      "grad_norm": 0.8009688854217529,
      "learning_rate": 3.926366170170104e-05,
      "loss": 1.3426,
      "step": 604
    },
    {
      "epoch": 1.1329588014981273,
      "grad_norm": 0.8617122173309326,
      "learning_rate": 3.921886584934004e-05,
      "loss": 1.3204,
      "step": 605
    },
    {
      "epoch": 1.1348314606741572,
      "grad_norm": 0.7749828696250916,
      "learning_rate": 3.917400241411872e-05,
      "loss": 1.44,
      "step": 606
    },
    {
      "epoch": 1.1367041198501873,
      "grad_norm": 0.9261881709098816,
      "learning_rate": 3.912907160927484e-05,
      "loss": 1.3538,
      "step": 607
    },
    {
      "epoch": 1.1385767790262173,
      "grad_norm": 0.8261597156524658,
      "learning_rate": 3.9084073648366404e-05,
      "loss": 1.3584,
      "step": 608
    },
    {
      "epoch": 1.1404494382022472,
      "grad_norm": 0.8328582644462585,
      "learning_rate": 3.9039008745270584e-05,
      "loss": 1.3028,
      "step": 609
    },
    {
      "epoch": 1.142322097378277,
      "grad_norm": 0.8848078846931458,
      "learning_rate": 3.899387711418273e-05,
      "loss": 1.4103,
      "step": 610
    },
    {
      "epoch": 1.1441947565543071,
      "grad_norm": 0.9065756797790527,
      "learning_rate": 3.894867896961536e-05,
      "loss": 1.4459,
      "step": 611
    },
    {
      "epoch": 1.146067415730337,
      "grad_norm": 0.8654925227165222,
      "learning_rate": 3.890341452639714e-05,
      "loss": 1.4538,
      "step": 612
    },
    {
      "epoch": 1.147940074906367,
      "grad_norm": 0.8421391248703003,
      "learning_rate": 3.8858083999671855e-05,
      "loss": 1.3406,
      "step": 613
    },
    {
      "epoch": 1.149812734082397,
      "grad_norm": 0.8544974327087402,
      "learning_rate": 3.881268760489737e-05,
      "loss": 1.3412,
      "step": 614
    },
    {
      "epoch": 1.151685393258427,
      "grad_norm": 0.8200132250785828,
      "learning_rate": 3.876722555784464e-05,
      "loss": 1.3357,
      "step": 615
    },
    {
      "epoch": 1.1535580524344569,
      "grad_norm": 0.8206817507743835,
      "learning_rate": 3.8721698074596674e-05,
      "loss": 1.3095,
      "step": 616
    },
    {
      "epoch": 1.155430711610487,
      "grad_norm": 0.9201591610908508,
      "learning_rate": 3.867610537154748e-05,
      "loss": 1.4068,
      "step": 617
    },
    {
      "epoch": 1.1573033707865168,
      "grad_norm": 0.842770516872406,
      "learning_rate": 3.863044766540107e-05,
      "loss": 1.3382,
      "step": 618
    },
    {
      "epoch": 1.1591760299625469,
      "grad_norm": 0.852440595626831,
      "learning_rate": 3.858472517317043e-05,
      "loss": 1.3253,
      "step": 619
    },
    {
      "epoch": 1.1610486891385767,
      "grad_norm": 0.8970404863357544,
      "learning_rate": 3.853893811217645e-05,
      "loss": 1.5084,
      "step": 620
    },
    {
      "epoch": 1.1629213483146068,
      "grad_norm": 0.8596054911613464,
      "learning_rate": 3.849308670004694e-05,
      "loss": 1.3371,
      "step": 621
    },
    {
      "epoch": 1.1647940074906367,
      "grad_norm": 0.8206225633621216,
      "learning_rate": 3.844717115471558e-05,
      "loss": 1.4584,
      "step": 622
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.8488574624061584,
      "learning_rate": 3.840119169442085e-05,
      "loss": 1.3861,
      "step": 623
    },
    {
      "epoch": 1.1685393258426966,
      "grad_norm": 0.8619019985198975,
      "learning_rate": 3.835514853770505e-05,
      "loss": 1.435,
      "step": 624
    },
    {
      "epoch": 1.1704119850187267,
      "grad_norm": 0.8830575942993164,
      "learning_rate": 3.8309041903413196e-05,
      "loss": 1.3706,
      "step": 625
    },
    {
      "epoch": 1.1722846441947565,
      "grad_norm": 0.8606201410293579,
      "learning_rate": 3.8262872010692055e-05,
      "loss": 1.3918,
      "step": 626
    },
    {
      "epoch": 1.1741573033707866,
      "grad_norm": 0.9079053401947021,
      "learning_rate": 3.821663907898904e-05,
      "loss": 1.4541,
      "step": 627
    },
    {
      "epoch": 1.1760299625468165,
      "grad_norm": 0.855979323387146,
      "learning_rate": 3.817034332805119e-05,
      "loss": 1.3927,
      "step": 628
    },
    {
      "epoch": 1.1779026217228465,
      "grad_norm": 0.8608399033546448,
      "learning_rate": 3.8123984977924155e-05,
      "loss": 1.4381,
      "step": 629
    },
    {
      "epoch": 1.1797752808988764,
      "grad_norm": 0.964607298374176,
      "learning_rate": 3.807756424895107e-05,
      "loss": 1.4708,
      "step": 630
    },
    {
      "epoch": 1.1816479400749063,
      "grad_norm": 0.8918854594230652,
      "learning_rate": 3.803108136177161e-05,
      "loss": 1.2639,
      "step": 631
    },
    {
      "epoch": 1.1835205992509363,
      "grad_norm": 1.024186611175537,
      "learning_rate": 3.7984536537320866e-05,
      "loss": 1.3831,
      "step": 632
    },
    {
      "epoch": 1.1853932584269664,
      "grad_norm": 0.8644706010818481,
      "learning_rate": 3.793792999682832e-05,
      "loss": 1.4161,
      "step": 633
    },
    {
      "epoch": 1.1872659176029963,
      "grad_norm": 0.8105168342590332,
      "learning_rate": 3.7891261961816795e-05,
      "loss": 1.4471,
      "step": 634
    },
    {
      "epoch": 1.1891385767790261,
      "grad_norm": 0.7459197044372559,
      "learning_rate": 3.784453265410141e-05,
      "loss": 1.3822,
      "step": 635
    },
    {
      "epoch": 1.1910112359550562,
      "grad_norm": 0.8147144913673401,
      "learning_rate": 3.7797742295788486e-05,
      "loss": 1.3693,
      "step": 636
    },
    {
      "epoch": 1.192883895131086,
      "grad_norm": 0.8039524555206299,
      "learning_rate": 3.775089110927456e-05,
      "loss": 1.3685,
      "step": 637
    },
    {
      "epoch": 1.1947565543071161,
      "grad_norm": 0.879589319229126,
      "learning_rate": 3.7703979317245266e-05,
      "loss": 1.3469,
      "step": 638
    },
    {
      "epoch": 1.196629213483146,
      "grad_norm": 0.8032209277153015,
      "learning_rate": 3.765700714267429e-05,
      "loss": 1.3449,
      "step": 639
    },
    {
      "epoch": 1.198501872659176,
      "grad_norm": 0.9263015389442444,
      "learning_rate": 3.7609974808822345e-05,
      "loss": 1.3419,
      "step": 640
    },
    {
      "epoch": 1.200374531835206,
      "grad_norm": 0.7943599224090576,
      "learning_rate": 3.756288253923606e-05,
      "loss": 1.3789,
      "step": 641
    },
    {
      "epoch": 1.202247191011236,
      "grad_norm": 0.8587490916252136,
      "learning_rate": 3.7515730557746956e-05,
      "loss": 1.5566,
      "step": 642
    },
    {
      "epoch": 1.2041198501872659,
      "grad_norm": 0.8836830258369446,
      "learning_rate": 3.746851908847034e-05,
      "loss": 1.4405,
      "step": 643
    },
    {
      "epoch": 1.205992509363296,
      "grad_norm": 0.8689707517623901,
      "learning_rate": 3.742124835580432e-05,
      "loss": 1.3732,
      "step": 644
    },
    {
      "epoch": 1.2078651685393258,
      "grad_norm": 0.7978100180625916,
      "learning_rate": 3.7373918584428624e-05,
      "loss": 1.3269,
      "step": 645
    },
    {
      "epoch": 1.2097378277153559,
      "grad_norm": 1.1264175176620483,
      "learning_rate": 3.732652999930364e-05,
      "loss": 1.4179,
      "step": 646
    },
    {
      "epoch": 1.2116104868913857,
      "grad_norm": 0.8773662447929382,
      "learning_rate": 3.727908282566927e-05,
      "loss": 1.4555,
      "step": 647
    },
    {
      "epoch": 1.2134831460674158,
      "grad_norm": 0.8596067428588867,
      "learning_rate": 3.7231577289043916e-05,
      "loss": 1.3402,
      "step": 648
    },
    {
      "epoch": 1.2153558052434457,
      "grad_norm": 0.8653426170349121,
      "learning_rate": 3.7184013615223364e-05,
      "loss": 1.4741,
      "step": 649
    },
    {
      "epoch": 1.2172284644194757,
      "grad_norm": 0.8307056427001953,
      "learning_rate": 3.7136392030279724e-05,
      "loss": 1.4002,
      "step": 650
    },
    {
      "epoch": 1.2191011235955056,
      "grad_norm": 0.7707927227020264,
      "learning_rate": 3.7088712760560366e-05,
      "loss": 1.4769,
      "step": 651
    },
    {
      "epoch": 1.2209737827715357,
      "grad_norm": 0.9287068247795105,
      "learning_rate": 3.704097603268686e-05,
      "loss": 1.3736,
      "step": 652
    },
    {
      "epoch": 1.2228464419475655,
      "grad_norm": 0.8732008337974548,
      "learning_rate": 3.699318207355384e-05,
      "loss": 1.4306,
      "step": 653
    },
    {
      "epoch": 1.2247191011235956,
      "grad_norm": 0.8411840200424194,
      "learning_rate": 3.6945331110328e-05,
      "loss": 1.3875,
      "step": 654
    },
    {
      "epoch": 1.2265917602996255,
      "grad_norm": 0.8982007503509521,
      "learning_rate": 3.689742337044692e-05,
      "loss": 1.2883,
      "step": 655
    },
    {
      "epoch": 1.2284644194756553,
      "grad_norm": 0.9041258692741394,
      "learning_rate": 3.684945908161812e-05,
      "loss": 1.4691,
      "step": 656
    },
    {
      "epoch": 1.2303370786516854,
      "grad_norm": 0.8502506017684937,
      "learning_rate": 3.680143847181783e-05,
      "loss": 1.3316,
      "step": 657
    },
    {
      "epoch": 1.2322097378277155,
      "grad_norm": 0.9662383794784546,
      "learning_rate": 3.675336176929002e-05,
      "loss": 1.3604,
      "step": 658
    },
    {
      "epoch": 1.2340823970037453,
      "grad_norm": 0.7983251214027405,
      "learning_rate": 3.670522920254524e-05,
      "loss": 1.4944,
      "step": 659
    },
    {
      "epoch": 1.2359550561797752,
      "grad_norm": 0.9327149391174316,
      "learning_rate": 3.665704100035959e-05,
      "loss": 1.4101,
      "step": 660
    },
    {
      "epoch": 1.2378277153558053,
      "grad_norm": 0.8885524868965149,
      "learning_rate": 3.660879739177361e-05,
      "loss": 1.3265,
      "step": 661
    },
    {
      "epoch": 1.2397003745318351,
      "grad_norm": 0.9259389042854309,
      "learning_rate": 3.656049860609115e-05,
      "loss": 1.3203,
      "step": 662
    },
    {
      "epoch": 1.2415730337078652,
      "grad_norm": 0.8011155128479004,
      "learning_rate": 3.651214487287837e-05,
      "loss": 1.4491,
      "step": 663
    },
    {
      "epoch": 1.243445692883895,
      "grad_norm": 0.933584988117218,
      "learning_rate": 3.646373642196255e-05,
      "loss": 1.3016,
      "step": 664
    },
    {
      "epoch": 1.2453183520599251,
      "grad_norm": 0.9189913272857666,
      "learning_rate": 3.641527348343109e-05,
      "loss": 1.3401,
      "step": 665
    },
    {
      "epoch": 1.247191011235955,
      "grad_norm": 0.9743673801422119,
      "learning_rate": 3.636675628763034e-05,
      "loss": 1.405,
      "step": 666
    },
    {
      "epoch": 1.249063670411985,
      "grad_norm": 0.9645677208900452,
      "learning_rate": 3.631818506516455e-05,
      "loss": 1.2844,
      "step": 667
    },
    {
      "epoch": 1.250936329588015,
      "grad_norm": 0.9055882096290588,
      "learning_rate": 3.6269560046894766e-05,
      "loss": 1.3543,
      "step": 668
    },
    {
      "epoch": 1.252808988764045,
      "grad_norm": 0.8876597881317139,
      "learning_rate": 3.6220881463937705e-05,
      "loss": 1.2927,
      "step": 669
    },
    {
      "epoch": 1.2546816479400749,
      "grad_norm": 0.8664232492446899,
      "learning_rate": 3.617214954766471e-05,
      "loss": 1.4701,
      "step": 670
    },
    {
      "epoch": 1.256554307116105,
      "grad_norm": 0.9843685626983643,
      "learning_rate": 3.61233645297006e-05,
      "loss": 1.4715,
      "step": 671
    },
    {
      "epoch": 1.2584269662921348,
      "grad_norm": 0.9343176484107971,
      "learning_rate": 3.607452664192259e-05,
      "loss": 1.3716,
      "step": 672
    },
    {
      "epoch": 1.2602996254681649,
      "grad_norm": 0.9404910802841187,
      "learning_rate": 3.602563611645919e-05,
      "loss": 1.3554,
      "step": 673
    },
    {
      "epoch": 1.2621722846441947,
      "grad_norm": 0.9544706344604492,
      "learning_rate": 3.59766931856891e-05,
      "loss": 1.4578,
      "step": 674
    },
    {
      "epoch": 1.2640449438202248,
      "grad_norm": 0.8089100122451782,
      "learning_rate": 3.5927698082240116e-05,
      "loss": 1.3915,
      "step": 675
    },
    {
      "epoch": 1.2659176029962547,
      "grad_norm": 0.7613826394081116,
      "learning_rate": 3.5878651038987976e-05,
      "loss": 1.3987,
      "step": 676
    },
    {
      "epoch": 1.2677902621722845,
      "grad_norm": 0.8165906667709351,
      "learning_rate": 3.5829552289055335e-05,
      "loss": 1.3628,
      "step": 677
    },
    {
      "epoch": 1.2696629213483146,
      "grad_norm": 0.8612772822380066,
      "learning_rate": 3.578040206581059e-05,
      "loss": 1.3699,
      "step": 678
    },
    {
      "epoch": 1.2715355805243447,
      "grad_norm": 0.8943586945533752,
      "learning_rate": 3.573120060286679e-05,
      "loss": 1.3527,
      "step": 679
    },
    {
      "epoch": 1.2734082397003745,
      "grad_norm": 0.8885645866394043,
      "learning_rate": 3.568194813408053e-05,
      "loss": 1.4338,
      "step": 680
    },
    {
      "epoch": 1.2752808988764044,
      "grad_norm": 0.8027498722076416,
      "learning_rate": 3.563264489355085e-05,
      "loss": 1.3849,
      "step": 681
    },
    {
      "epoch": 1.2771535580524345,
      "grad_norm": 0.8691418766975403,
      "learning_rate": 3.558329111561809e-05,
      "loss": 1.3169,
      "step": 682
    },
    {
      "epoch": 1.2790262172284645,
      "grad_norm": 0.8351525068283081,
      "learning_rate": 3.5533887034862826e-05,
      "loss": 1.4177,
      "step": 683
    },
    {
      "epoch": 1.2808988764044944,
      "grad_norm": 0.9543196558952332,
      "learning_rate": 3.548443288610468e-05,
      "loss": 1.3191,
      "step": 684
    },
    {
      "epoch": 1.2827715355805243,
      "grad_norm": 0.8501549363136292,
      "learning_rate": 3.54349289044013e-05,
      "loss": 1.3999,
      "step": 685
    },
    {
      "epoch": 1.2846441947565543,
      "grad_norm": 0.9064486622810364,
      "learning_rate": 3.5385375325047166e-05,
      "loss": 1.4073,
      "step": 686
    },
    {
      "epoch": 1.2865168539325842,
      "grad_norm": 0.8273902535438538,
      "learning_rate": 3.5335772383572485e-05,
      "loss": 1.2417,
      "step": 687
    },
    {
      "epoch": 1.2883895131086143,
      "grad_norm": 0.9150070548057556,
      "learning_rate": 3.528612031574212e-05,
      "loss": 1.3693,
      "step": 688
    },
    {
      "epoch": 1.2902621722846441,
      "grad_norm": 0.9580627679824829,
      "learning_rate": 3.5236419357554385e-05,
      "loss": 1.3049,
      "step": 689
    },
    {
      "epoch": 1.2921348314606742,
      "grad_norm": 0.8685265183448792,
      "learning_rate": 3.5186669745240026e-05,
      "loss": 1.351,
      "step": 690
    },
    {
      "epoch": 1.294007490636704,
      "grad_norm": 0.8542383313179016,
      "learning_rate": 3.5136871715261014e-05,
      "loss": 1.3336,
      "step": 691
    },
    {
      "epoch": 1.2958801498127341,
      "grad_norm": 0.9523345232009888,
      "learning_rate": 3.508702550430944e-05,
      "loss": 1.3089,
      "step": 692
    },
    {
      "epoch": 1.297752808988764,
      "grad_norm": 0.9489871263504028,
      "learning_rate": 3.503713134930643e-05,
      "loss": 1.4511,
      "step": 693
    },
    {
      "epoch": 1.299625468164794,
      "grad_norm": 0.8518455028533936,
      "learning_rate": 3.4987189487400965e-05,
      "loss": 1.2991,
      "step": 694
    },
    {
      "epoch": 1.301498127340824,
      "grad_norm": 0.9895254373550415,
      "learning_rate": 3.49372001559688e-05,
      "loss": 1.3474,
      "step": 695
    },
    {
      "epoch": 1.303370786516854,
      "grad_norm": 0.9508649706840515,
      "learning_rate": 3.4887163592611307e-05,
      "loss": 1.4332,
      "step": 696
    },
    {
      "epoch": 1.3052434456928839,
      "grad_norm": 0.8718644380569458,
      "learning_rate": 3.483708003515434e-05,
      "loss": 1.5735,
      "step": 697
    },
    {
      "epoch": 1.3071161048689137,
      "grad_norm": 0.9150551557540894,
      "learning_rate": 3.4786949721647146e-05,
      "loss": 1.3633,
      "step": 698
    },
    {
      "epoch": 1.3089887640449438,
      "grad_norm": 0.9411808252334595,
      "learning_rate": 3.473677289036117e-05,
      "loss": 1.4144,
      "step": 699
    },
    {
      "epoch": 1.3108614232209739,
      "grad_norm": 0.8714406490325928,
      "learning_rate": 3.468654977978898e-05,
      "loss": 1.6154,
      "step": 700
    },
    {
      "epoch": 1.3127340823970037,
      "grad_norm": 0.9348651170730591,
      "learning_rate": 3.463628062864312e-05,
      "loss": 1.4307,
      "step": 701
    },
    {
      "epoch": 1.3146067415730336,
      "grad_norm": 1.020377278327942,
      "learning_rate": 3.458596567585494e-05,
      "loss": 1.4451,
      "step": 702
    },
    {
      "epoch": 1.3164794007490637,
      "grad_norm": 0.9421879649162292,
      "learning_rate": 3.4535605160573515e-05,
      "loss": 1.3004,
      "step": 703
    },
    {
      "epoch": 1.3183520599250937,
      "grad_norm": 0.9198845028877258,
      "learning_rate": 3.448519932216446e-05,
      "loss": 1.4299,
      "step": 704
    },
    {
      "epoch": 1.3202247191011236,
      "grad_norm": 0.9336076974868774,
      "learning_rate": 3.443474840020882e-05,
      "loss": 1.3403,
      "step": 705
    },
    {
      "epoch": 1.3220973782771535,
      "grad_norm": 0.9120489954948425,
      "learning_rate": 3.438425263450192e-05,
      "loss": 1.4371,
      "step": 706
    },
    {
      "epoch": 1.3239700374531835,
      "grad_norm": 0.9291353225708008,
      "learning_rate": 3.4333712265052234e-05,
      "loss": 1.399,
      "step": 707
    },
    {
      "epoch": 1.3258426966292136,
      "grad_norm": 0.9816318154335022,
      "learning_rate": 3.4283127532080264e-05,
      "loss": 1.4236,
      "step": 708
    },
    {
      "epoch": 1.3277153558052435,
      "grad_norm": 0.8896369338035583,
      "learning_rate": 3.42324986760173e-05,
      "loss": 1.3927,
      "step": 709
    },
    {
      "epoch": 1.3295880149812733,
      "grad_norm": 0.925449788570404,
      "learning_rate": 3.4181825937504435e-05,
      "loss": 1.3125,
      "step": 710
    },
    {
      "epoch": 1.3314606741573034,
      "grad_norm": 0.9204568266868591,
      "learning_rate": 3.413110955739129e-05,
      "loss": 1.3045,
      "step": 711
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.9959954023361206,
      "learning_rate": 3.4080349776734925e-05,
      "loss": 1.5457,
      "step": 712
    },
    {
      "epoch": 1.3352059925093633,
      "grad_norm": 0.8833068013191223,
      "learning_rate": 3.40295468367987e-05,
      "loss": 1.3162,
      "step": 713
    },
    {
      "epoch": 1.3370786516853932,
      "grad_norm": 0.8649119734764099,
      "learning_rate": 3.397870097905109e-05,
      "loss": 1.3531,
      "step": 714
    },
    {
      "epoch": 1.3389513108614233,
      "grad_norm": 0.947975218296051,
      "learning_rate": 3.392781244516455e-05,
      "loss": 1.4125,
      "step": 715
    },
    {
      "epoch": 1.3408239700374531,
      "grad_norm": 1.0879772901535034,
      "learning_rate": 3.387688147701444e-05,
      "loss": 1.4443,
      "step": 716
    },
    {
      "epoch": 1.3426966292134832,
      "grad_norm": 0.9244375824928284,
      "learning_rate": 3.382590831667774e-05,
      "loss": 1.3378,
      "step": 717
    },
    {
      "epoch": 1.344569288389513,
      "grad_norm": 0.9436289668083191,
      "learning_rate": 3.3774893206432e-05,
      "loss": 1.3361,
      "step": 718
    },
    {
      "epoch": 1.3464419475655431,
      "grad_norm": 0.8867109417915344,
      "learning_rate": 3.372383638875416e-05,
      "loss": 1.3284,
      "step": 719
    },
    {
      "epoch": 1.348314606741573,
      "grad_norm": 1.0018776655197144,
      "learning_rate": 3.367273810631941e-05,
      "loss": 1.3904,
      "step": 720
    },
    {
      "epoch": 1.350187265917603,
      "grad_norm": 0.9707607626914978,
      "learning_rate": 3.3621598602e-05,
      "loss": 1.3325,
      "step": 721
    },
    {
      "epoch": 1.352059925093633,
      "grad_norm": 0.9373803734779358,
      "learning_rate": 3.357041811886411e-05,
      "loss": 1.3885,
      "step": 722
    },
    {
      "epoch": 1.3539325842696628,
      "grad_norm": 1.098685383796692,
      "learning_rate": 3.351919690017473e-05,
      "loss": 1.4174,
      "step": 723
    },
    {
      "epoch": 1.3558052434456929,
      "grad_norm": 0.9509468674659729,
      "learning_rate": 3.34679351893884e-05,
      "loss": 1.3251,
      "step": 724
    },
    {
      "epoch": 1.357677902621723,
      "grad_norm": 0.9423556923866272,
      "learning_rate": 3.341663323015421e-05,
      "loss": 1.3935,
      "step": 725
    },
    {
      "epoch": 1.3595505617977528,
      "grad_norm": 0.9738126993179321,
      "learning_rate": 3.3365291266312464e-05,
      "loss": 1.3876,
      "step": 726
    },
    {
      "epoch": 1.3614232209737827,
      "grad_norm": 0.9178267121315002,
      "learning_rate": 3.3313909541893676e-05,
      "loss": 1.2965,
      "step": 727
    },
    {
      "epoch": 1.3632958801498127,
      "grad_norm": 0.9397753477096558,
      "learning_rate": 3.326248830111731e-05,
      "loss": 1.3144,
      "step": 728
    },
    {
      "epoch": 1.3651685393258428,
      "grad_norm": 0.9848880171775818,
      "learning_rate": 3.3211027788390645e-05,
      "loss": 1.4283,
      "step": 729
    },
    {
      "epoch": 1.3670411985018727,
      "grad_norm": 0.9286903142929077,
      "learning_rate": 3.315952824830766e-05,
      "loss": 1.3197,
      "step": 730
    },
    {
      "epoch": 1.3689138576779025,
      "grad_norm": 0.9368664622306824,
      "learning_rate": 3.3107989925647795e-05,
      "loss": 1.3714,
      "step": 731
    },
    {
      "epoch": 1.3707865168539326,
      "grad_norm": 0.9676632881164551,
      "learning_rate": 3.305641306537485e-05,
      "loss": 1.3744,
      "step": 732
    },
    {
      "epoch": 1.3726591760299627,
      "grad_norm": 0.9580250382423401,
      "learning_rate": 3.300479791263578e-05,
      "loss": 1.3232,
      "step": 733
    },
    {
      "epoch": 1.3745318352059925,
      "grad_norm": 0.9412403106689453,
      "learning_rate": 3.2953144712759545e-05,
      "loss": 1.3362,
      "step": 734
    },
    {
      "epoch": 1.3764044943820224,
      "grad_norm": 0.9957979917526245,
      "learning_rate": 3.290145371125596e-05,
      "loss": 1.3474,
      "step": 735
    },
    {
      "epoch": 1.3782771535580525,
      "grad_norm": 0.8244457244873047,
      "learning_rate": 3.2849725153814495e-05,
      "loss": 1.3366,
      "step": 736
    },
    {
      "epoch": 1.3801498127340823,
      "grad_norm": 1.0152077674865723,
      "learning_rate": 3.279795928630315e-05,
      "loss": 1.436,
      "step": 737
    },
    {
      "epoch": 1.3820224719101124,
      "grad_norm": 0.9867649674415588,
      "learning_rate": 3.274615635476722e-05,
      "loss": 1.3313,
      "step": 738
    },
    {
      "epoch": 1.3838951310861423,
      "grad_norm": 0.9276944398880005,
      "learning_rate": 3.269431660542821e-05,
      "loss": 1.3887,
      "step": 739
    },
    {
      "epoch": 1.3857677902621723,
      "grad_norm": 0.9915891885757446,
      "learning_rate": 3.26424402846826e-05,
      "loss": 1.3867,
      "step": 740
    },
    {
      "epoch": 1.3876404494382022,
      "grad_norm": 0.8979570269584656,
      "learning_rate": 3.259052763910068e-05,
      "loss": 1.3567,
      "step": 741
    },
    {
      "epoch": 1.3895131086142323,
      "grad_norm": 0.7922077775001526,
      "learning_rate": 3.253857891542545e-05,
      "loss": 1.3404,
      "step": 742
    },
    {
      "epoch": 1.3913857677902621,
      "grad_norm": 0.926855742931366,
      "learning_rate": 3.248659436057131e-05,
      "loss": 1.3652,
      "step": 743
    },
    {
      "epoch": 1.3932584269662922,
      "grad_norm": 1.0778273344039917,
      "learning_rate": 3.243457422162305e-05,
      "loss": 1.3474,
      "step": 744
    },
    {
      "epoch": 1.395131086142322,
      "grad_norm": 0.8500720858573914,
      "learning_rate": 3.238251874583452e-05,
      "loss": 1.187,
      "step": 745
    },
    {
      "epoch": 1.3970037453183521,
      "grad_norm": 0.950015664100647,
      "learning_rate": 3.2330428180627575e-05,
      "loss": 1.3783,
      "step": 746
    },
    {
      "epoch": 1.398876404494382,
      "grad_norm": 1.01472806930542,
      "learning_rate": 3.227830277359084e-05,
      "loss": 1.3242,
      "step": 747
    },
    {
      "epoch": 1.4007490636704119,
      "grad_norm": 0.9201540350914001,
      "learning_rate": 3.2226142772478525e-05,
      "loss": 1.3008,
      "step": 748
    },
    {
      "epoch": 1.402621722846442,
      "grad_norm": 0.9466866254806519,
      "learning_rate": 3.217394842520929e-05,
      "loss": 1.2023,
      "step": 749
    },
    {
      "epoch": 1.404494382022472,
      "grad_norm": 0.9949240684509277,
      "learning_rate": 3.2121719979865054e-05,
      "loss": 1.36,
      "step": 750
    },
    {
      "epoch": 1.4063670411985019,
      "grad_norm": 0.9560457468032837,
      "learning_rate": 3.206945768468976e-05,
      "loss": 1.354,
      "step": 751
    },
    {
      "epoch": 1.4082397003745317,
      "grad_norm": 1.0672121047973633,
      "learning_rate": 3.201716178808829e-05,
      "loss": 1.4319,
      "step": 752
    },
    {
      "epoch": 1.4101123595505618,
      "grad_norm": 0.8872138261795044,
      "learning_rate": 3.196483253862521e-05,
      "loss": 1.3127,
      "step": 753
    },
    {
      "epoch": 1.4119850187265919,
      "grad_norm": 0.9974532127380371,
      "learning_rate": 3.1912470185023615e-05,
      "loss": 1.3701,
      "step": 754
    },
    {
      "epoch": 1.4138576779026217,
      "grad_norm": 0.9479983448982239,
      "learning_rate": 3.186007497616394e-05,
      "loss": 1.2247,
      "step": 755
    },
    {
      "epoch": 1.4157303370786516,
      "grad_norm": 1.0935373306274414,
      "learning_rate": 3.1807647161082795e-05,
      "loss": 1.4253,
      "step": 756
    },
    {
      "epoch": 1.4176029962546817,
      "grad_norm": 1.0065207481384277,
      "learning_rate": 3.175518698897178e-05,
      "loss": 1.2425,
      "step": 757
    },
    {
      "epoch": 1.4194756554307117,
      "grad_norm": 0.9403544664382935,
      "learning_rate": 3.170269470917625e-05,
      "loss": 1.3064,
      "step": 758
    },
    {
      "epoch": 1.4213483146067416,
      "grad_norm": 0.9915198683738708,
      "learning_rate": 3.16501705711942e-05,
      "loss": 1.5143,
      "step": 759
    },
    {
      "epoch": 1.4232209737827715,
      "grad_norm": 0.9031796455383301,
      "learning_rate": 3.159761482467505e-05,
      "loss": 1.4012,
      "step": 760
    },
    {
      "epoch": 1.4250936329588015,
      "grad_norm": 1.0077126026153564,
      "learning_rate": 3.154502771941843e-05,
      "loss": 1.3537,
      "step": 761
    },
    {
      "epoch": 1.4269662921348314,
      "grad_norm": 0.9814000725746155,
      "learning_rate": 3.149240950537306e-05,
      "loss": 1.351,
      "step": 762
    },
    {
      "epoch": 1.4288389513108615,
      "grad_norm": 0.983187735080719,
      "learning_rate": 3.143976043263548e-05,
      "loss": 1.2563,
      "step": 763
    },
    {
      "epoch": 1.4307116104868913,
      "grad_norm": 0.9941717386245728,
      "learning_rate": 3.1387080751448927e-05,
      "loss": 1.3633,
      "step": 764
    },
    {
      "epoch": 1.4325842696629214,
      "grad_norm": 1.0088920593261719,
      "learning_rate": 3.133437071220211e-05,
      "loss": 1.2927,
      "step": 765
    },
    {
      "epoch": 1.4344569288389513,
      "grad_norm": 0.9351455569267273,
      "learning_rate": 3.128163056542805e-05,
      "loss": 1.3055,
      "step": 766
    },
    {
      "epoch": 1.4363295880149813,
      "grad_norm": 0.9264461398124695,
      "learning_rate": 3.122886056180284e-05,
      "loss": 1.2492,
      "step": 767
    },
    {
      "epoch": 1.4382022471910112,
      "grad_norm": 0.9012754559516907,
      "learning_rate": 3.117606095214451e-05,
      "loss": 1.3305,
      "step": 768
    },
    {
      "epoch": 1.4400749063670413,
      "grad_norm": 0.9754378199577332,
      "learning_rate": 3.112323198741179e-05,
      "loss": 1.3829,
      "step": 769
    },
    {
      "epoch": 1.4419475655430711,
      "grad_norm": 0.8404281139373779,
      "learning_rate": 3.107037391870296e-05,
      "loss": 1.297,
      "step": 770
    },
    {
      "epoch": 1.4438202247191012,
      "grad_norm": 0.9731990098953247,
      "learning_rate": 3.10174869972546e-05,
      "loss": 1.3757,
      "step": 771
    },
    {
      "epoch": 1.445692883895131,
      "grad_norm": 1.03734290599823,
      "learning_rate": 3.0964571474440466e-05,
      "loss": 1.3782,
      "step": 772
    },
    {
      "epoch": 1.447565543071161,
      "grad_norm": 0.9857791662216187,
      "learning_rate": 3.091162760177022e-05,
      "loss": 1.394,
      "step": 773
    },
    {
      "epoch": 1.449438202247191,
      "grad_norm": 0.8692712187767029,
      "learning_rate": 3.0858655630888286e-05,
      "loss": 1.2926,
      "step": 774
    },
    {
      "epoch": 1.451310861423221,
      "grad_norm": 0.8810762763023376,
      "learning_rate": 3.080565581357266e-05,
      "loss": 1.2908,
      "step": 775
    },
    {
      "epoch": 1.453183520599251,
      "grad_norm": 0.890144944190979,
      "learning_rate": 3.075262840173367e-05,
      "loss": 1.4686,
      "step": 776
    },
    {
      "epoch": 1.4550561797752808,
      "grad_norm": 0.9936763644218445,
      "learning_rate": 3.069957364741281e-05,
      "loss": 1.3175,
      "step": 777
    },
    {
      "epoch": 1.4569288389513109,
      "grad_norm": 0.9597110748291016,
      "learning_rate": 3.064649180278152e-05,
      "loss": 1.3264,
      "step": 778
    },
    {
      "epoch": 1.458801498127341,
      "grad_norm": 1.0444670915603638,
      "learning_rate": 3.059338312014002e-05,
      "loss": 1.3245,
      "step": 779
    },
    {
      "epoch": 1.4606741573033708,
      "grad_norm": 0.8879148364067078,
      "learning_rate": 3.054024785191609e-05,
      "loss": 1.1623,
      "step": 780
    },
    {
      "epoch": 1.4625468164794007,
      "grad_norm": 0.9522879123687744,
      "learning_rate": 3.0487086250663876e-05,
      "loss": 1.2603,
      "step": 781
    },
    {
      "epoch": 1.4644194756554307,
      "grad_norm": 1.083253264427185,
      "learning_rate": 3.0433898569062665e-05,
      "loss": 1.46,
      "step": 782
    },
    {
      "epoch": 1.4662921348314606,
      "grad_norm": 1.0459336042404175,
      "learning_rate": 3.0380685059915713e-05,
      "loss": 1.373,
      "step": 783
    },
    {
      "epoch": 1.4681647940074907,
      "grad_norm": 1.010434627532959,
      "learning_rate": 3.0327445976149056e-05,
      "loss": 1.5063,
      "step": 784
    },
    {
      "epoch": 1.4700374531835205,
      "grad_norm": 0.9248639345169067,
      "learning_rate": 3.0274181570810262e-05,
      "loss": 1.3094,
      "step": 785
    },
    {
      "epoch": 1.4719101123595506,
      "grad_norm": 0.9788716435432434,
      "learning_rate": 3.0220892097067267e-05,
      "loss": 1.3564,
      "step": 786
    },
    {
      "epoch": 1.4737827715355805,
      "grad_norm": 0.9301137328147888,
      "learning_rate": 3.016757780820716e-05,
      "loss": 1.3635,
      "step": 787
    },
    {
      "epoch": 1.4756554307116105,
      "grad_norm": 1.0552351474761963,
      "learning_rate": 3.0114238957634956e-05,
      "loss": 1.2947,
      "step": 788
    },
    {
      "epoch": 1.4775280898876404,
      "grad_norm": 1.0223227739334106,
      "learning_rate": 3.006087579887244e-05,
      "loss": 1.3457,
      "step": 789
    },
    {
      "epoch": 1.4794007490636705,
      "grad_norm": 1.036403775215149,
      "learning_rate": 3.000748858555692e-05,
      "loss": 1.4104,
      "step": 790
    },
    {
      "epoch": 1.4812734082397003,
      "grad_norm": 1.065659761428833,
      "learning_rate": 2.9954077571440043e-05,
      "loss": 1.4564,
      "step": 791
    },
    {
      "epoch": 1.4831460674157304,
      "grad_norm": 1.0047999620437622,
      "learning_rate": 2.990064301038658e-05,
      "loss": 1.4465,
      "step": 792
    },
    {
      "epoch": 1.4850187265917603,
      "grad_norm": 0.9906896352767944,
      "learning_rate": 2.9847185156373216e-05,
      "loss": 1.4111,
      "step": 793
    },
    {
      "epoch": 1.4868913857677903,
      "grad_norm": 0.973154604434967,
      "learning_rate": 2.979370426348735e-05,
      "loss": 1.294,
      "step": 794
    },
    {
      "epoch": 1.4887640449438202,
      "grad_norm": 0.9830839037895203,
      "learning_rate": 2.9740200585925894e-05,
      "loss": 1.3284,
      "step": 795
    },
    {
      "epoch": 1.4906367041198503,
      "grad_norm": 1.0251580476760864,
      "learning_rate": 2.968667437799405e-05,
      "loss": 1.4441,
      "step": 796
    },
    {
      "epoch": 1.4925093632958801,
      "grad_norm": 1.003140926361084,
      "learning_rate": 2.9633125894104107e-05,
      "loss": 1.3422,
      "step": 797
    },
    {
      "epoch": 1.49438202247191,
      "grad_norm": 0.9592090845108032,
      "learning_rate": 2.957955538877424e-05,
      "loss": 1.3407,
      "step": 798
    },
    {
      "epoch": 1.49625468164794,
      "grad_norm": 0.9890917539596558,
      "learning_rate": 2.9525963116627282e-05,
      "loss": 1.1728,
      "step": 799
    },
    {
      "epoch": 1.4981273408239701,
      "grad_norm": 0.9907186627388,
      "learning_rate": 2.9472349332389525e-05,
      "loss": 1.3485,
      "step": 800
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.9314661622047424,
      "learning_rate": 2.9418714290889527e-05,
      "loss": 1.3329,
      "step": 801
    },
    {
      "epoch": 1.5018726591760299,
      "grad_norm": 0.9856598973274231,
      "learning_rate": 2.9365058247056864e-05,
      "loss": 1.3601,
      "step": 802
    },
    {
      "epoch": 1.50374531835206,
      "grad_norm": 0.9984954595565796,
      "learning_rate": 2.931138145592093e-05,
      "loss": 1.4864,
      "step": 803
    },
    {
      "epoch": 1.50561797752809,
      "grad_norm": 1.0362682342529297,
      "learning_rate": 2.9257684172609762e-05,
      "loss": 1.2621,
      "step": 804
    },
    {
      "epoch": 1.5074906367041199,
      "grad_norm": 0.9603354930877686,
      "learning_rate": 2.9203966652348764e-05,
      "loss": 1.2949,
      "step": 805
    },
    {
      "epoch": 1.5093632958801497,
      "grad_norm": 1.0501898527145386,
      "learning_rate": 2.9150229150459557e-05,
      "loss": 1.3984,
      "step": 806
    },
    {
      "epoch": 1.5112359550561798,
      "grad_norm": 1.0850799083709717,
      "learning_rate": 2.9096471922358715e-05,
      "loss": 1.2645,
      "step": 807
    },
    {
      "epoch": 1.5131086142322099,
      "grad_norm": 0.9972336292266846,
      "learning_rate": 2.9042695223556578e-05,
      "loss": 1.3396,
      "step": 808
    },
    {
      "epoch": 1.5149812734082397,
      "grad_norm": 0.9859989285469055,
      "learning_rate": 2.8988899309656015e-05,
      "loss": 1.3234,
      "step": 809
    },
    {
      "epoch": 1.5168539325842696,
      "grad_norm": 1.079076886177063,
      "learning_rate": 2.893508443635127e-05,
      "loss": 1.297,
      "step": 810
    },
    {
      "epoch": 1.5187265917602997,
      "grad_norm": 1.0261284112930298,
      "learning_rate": 2.8881250859426646e-05,
      "loss": 1.4171,
      "step": 811
    },
    {
      "epoch": 1.5205992509363297,
      "grad_norm": 0.9414374828338623,
      "learning_rate": 2.8827398834755387e-05,
      "loss": 1.2608,
      "step": 812
    },
    {
      "epoch": 1.5224719101123596,
      "grad_norm": 0.994271993637085,
      "learning_rate": 2.877352861829839e-05,
      "loss": 1.3752,
      "step": 813
    },
    {
      "epoch": 1.5243445692883895,
      "grad_norm": 0.9974358677864075,
      "learning_rate": 2.871964046610305e-05,
      "loss": 1.3503,
      "step": 814
    },
    {
      "epoch": 1.5262172284644193,
      "grad_norm": 0.9675769805908203,
      "learning_rate": 2.866573463430199e-05,
      "loss": 1.3212,
      "step": 815
    },
    {
      "epoch": 1.5280898876404494,
      "grad_norm": 1.104469895362854,
      "learning_rate": 2.861181137911186e-05,
      "loss": 1.5873,
      "step": 816
    },
    {
      "epoch": 1.5299625468164795,
      "grad_norm": 0.9141734838485718,
      "learning_rate": 2.8557870956832132e-05,
      "loss": 1.36,
      "step": 817
    },
    {
      "epoch": 1.5318352059925093,
      "grad_norm": 0.8529278635978699,
      "learning_rate": 2.850391362384388e-05,
      "loss": 1.229,
      "step": 818
    },
    {
      "epoch": 1.5337078651685392,
      "grad_norm": 0.9279976487159729,
      "learning_rate": 2.8449939636608557e-05,
      "loss": 1.2928,
      "step": 819
    },
    {
      "epoch": 1.5355805243445693,
      "grad_norm": 0.996078610420227,
      "learning_rate": 2.8395949251666758e-05,
      "loss": 1.2277,
      "step": 820
    },
    {
      "epoch": 1.5374531835205993,
      "grad_norm": 0.9196224808692932,
      "learning_rate": 2.8341942725637038e-05,
      "loss": 1.3102,
      "step": 821
    },
    {
      "epoch": 1.5393258426966292,
      "grad_norm": 0.9833940267562866,
      "learning_rate": 2.8287920315214643e-05,
      "loss": 1.2293,
      "step": 822
    },
    {
      "epoch": 1.541198501872659,
      "grad_norm": 0.9025416374206543,
      "learning_rate": 2.8233882277170348e-05,
      "loss": 1.3046,
      "step": 823
    },
    {
      "epoch": 1.5430711610486891,
      "grad_norm": 0.9340637922286987,
      "learning_rate": 2.817982886834918e-05,
      "loss": 1.371,
      "step": 824
    },
    {
      "epoch": 1.5449438202247192,
      "grad_norm": 0.9954320788383484,
      "learning_rate": 2.8125760345669255e-05,
      "loss": 1.4728,
      "step": 825
    },
    {
      "epoch": 1.546816479400749,
      "grad_norm": 1.0234793424606323,
      "learning_rate": 2.8071676966120496e-05,
      "loss": 1.3867,
      "step": 826
    },
    {
      "epoch": 1.548689138576779,
      "grad_norm": 1.1124569177627563,
      "learning_rate": 2.8017578986763464e-05,
      "loss": 1.299,
      "step": 827
    },
    {
      "epoch": 1.550561797752809,
      "grad_norm": 0.8856891393661499,
      "learning_rate": 2.7963466664728087e-05,
      "loss": 1.2759,
      "step": 828
    },
    {
      "epoch": 1.552434456928839,
      "grad_norm": 0.9783953428268433,
      "learning_rate": 2.79093402572125e-05,
      "loss": 1.2702,
      "step": 829
    },
    {
      "epoch": 1.554307116104869,
      "grad_norm": 1.1581523418426514,
      "learning_rate": 2.7855200021481752e-05,
      "loss": 1.3365,
      "step": 830
    },
    {
      "epoch": 1.5561797752808988,
      "grad_norm": 0.9888054728507996,
      "learning_rate": 2.7801046214866644e-05,
      "loss": 1.2896,
      "step": 831
    },
    {
      "epoch": 1.5580524344569289,
      "grad_norm": 1.0685656070709229,
      "learning_rate": 2.774687909476246e-05,
      "loss": 1.3974,
      "step": 832
    },
    {
      "epoch": 1.559925093632959,
      "grad_norm": 0.9348361492156982,
      "learning_rate": 2.7692698918627778e-05,
      "loss": 1.3302,
      "step": 833
    },
    {
      "epoch": 1.5617977528089888,
      "grad_norm": 0.9656357765197754,
      "learning_rate": 2.7638505943983234e-05,
      "loss": 1.3049,
      "step": 834
    },
    {
      "epoch": 1.5636704119850187,
      "grad_norm": 1.0753278732299805,
      "learning_rate": 2.758430042841027e-05,
      "loss": 1.2784,
      "step": 835
    },
    {
      "epoch": 1.5655430711610487,
      "grad_norm": 0.9698373675346375,
      "learning_rate": 2.753008262954998e-05,
      "loss": 1.2653,
      "step": 836
    },
    {
      "epoch": 1.5674157303370788,
      "grad_norm": 1.078726053237915,
      "learning_rate": 2.747585280510179e-05,
      "loss": 1.3826,
      "step": 837
    },
    {
      "epoch": 1.5692883895131087,
      "grad_norm": 0.9936466217041016,
      "learning_rate": 2.7421611212822322e-05,
      "loss": 1.2041,
      "step": 838
    },
    {
      "epoch": 1.5711610486891385,
      "grad_norm": 0.974458634853363,
      "learning_rate": 2.7367358110524115e-05,
      "loss": 1.1737,
      "step": 839
    },
    {
      "epoch": 1.5730337078651684,
      "grad_norm": 0.9706088900566101,
      "learning_rate": 2.7313093756074425e-05,
      "loss": 1.3193,
      "step": 840
    },
    {
      "epoch": 1.5749063670411985,
      "grad_norm": 0.9235392808914185,
      "learning_rate": 2.725881840739398e-05,
      "loss": 1.252,
      "step": 841
    },
    {
      "epoch": 1.5767790262172285,
      "grad_norm": 1.0031002759933472,
      "learning_rate": 2.7204532322455768e-05,
      "loss": 1.3578,
      "step": 842
    },
    {
      "epoch": 1.5786516853932584,
      "grad_norm": 1.0243810415267944,
      "learning_rate": 2.7150235759283806e-05,
      "loss": 1.3697,
      "step": 843
    },
    {
      "epoch": 1.5805243445692883,
      "grad_norm": 1.0563488006591797,
      "learning_rate": 2.7095928975951913e-05,
      "loss": 1.4198,
      "step": 844
    },
    {
      "epoch": 1.5823970037453183,
      "grad_norm": 1.1827970743179321,
      "learning_rate": 2.70416122305825e-05,
      "loss": 1.2318,
      "step": 845
    },
    {
      "epoch": 1.5842696629213484,
      "grad_norm": 1.1460223197937012,
      "learning_rate": 2.6987285781345295e-05,
      "loss": 1.2641,
      "step": 846
    },
    {
      "epoch": 1.5861423220973783,
      "grad_norm": 1.0905208587646484,
      "learning_rate": 2.6932949886456183e-05,
      "loss": 1.2329,
      "step": 847
    },
    {
      "epoch": 1.5880149812734081,
      "grad_norm": 0.9845780730247498,
      "learning_rate": 2.687860480417592e-05,
      "loss": 1.3119,
      "step": 848
    },
    {
      "epoch": 1.5898876404494382,
      "grad_norm": 1.022703766822815,
      "learning_rate": 2.6824250792808957e-05,
      "loss": 1.263,
      "step": 849
    },
    {
      "epoch": 1.5917602996254683,
      "grad_norm": 1.0396572351455688,
      "learning_rate": 2.676988811070215e-05,
      "loss": 1.2906,
      "step": 850
    },
    {
      "epoch": 1.5936329588014981,
      "grad_norm": 1.0660349130630493,
      "learning_rate": 2.6715517016243614e-05,
      "loss": 1.3063,
      "step": 851
    },
    {
      "epoch": 1.595505617977528,
      "grad_norm": 0.9271506667137146,
      "learning_rate": 2.6661137767861387e-05,
      "loss": 1.2346,
      "step": 852
    },
    {
      "epoch": 1.597378277153558,
      "grad_norm": 1.0580661296844482,
      "learning_rate": 2.660675062402232e-05,
      "loss": 1.1324,
      "step": 853
    },
    {
      "epoch": 1.5992509363295881,
      "grad_norm": 1.0572326183319092,
      "learning_rate": 2.6552355843230752e-05,
      "loss": 1.3181,
      "step": 854
    },
    {
      "epoch": 1.601123595505618,
      "grad_norm": 1.1172981262207031,
      "learning_rate": 2.649795368402735e-05,
      "loss": 1.4615,
      "step": 855
    },
    {
      "epoch": 1.6029962546816479,
      "grad_norm": 1.0696238279342651,
      "learning_rate": 2.6443544404987837e-05,
      "loss": 1.3358,
      "step": 856
    },
    {
      "epoch": 1.604868913857678,
      "grad_norm": 1.0441663265228271,
      "learning_rate": 2.6389128264721768e-05,
      "loss": 1.3077,
      "step": 857
    },
    {
      "epoch": 1.606741573033708,
      "grad_norm": 0.9548763036727905,
      "learning_rate": 2.633470552187132e-05,
      "loss": 1.3071,
      "step": 858
    },
    {
      "epoch": 1.6086142322097379,
      "grad_norm": 0.9888238310813904,
      "learning_rate": 2.6280276435110062e-05,
      "loss": 1.2122,
      "step": 859
    },
    {
      "epoch": 1.6104868913857677,
      "grad_norm": 1.1016358137130737,
      "learning_rate": 2.62258412631417e-05,
      "loss": 1.4296,
      "step": 860
    },
    {
      "epoch": 1.6123595505617978,
      "grad_norm": 1.0200579166412354,
      "learning_rate": 2.6171400264698868e-05,
      "loss": 1.4212,
      "step": 861
    },
    {
      "epoch": 1.6142322097378277,
      "grad_norm": 1.0147550106048584,
      "learning_rate": 2.6116953698541897e-05,
      "loss": 1.4138,
      "step": 862
    },
    {
      "epoch": 1.6161048689138577,
      "grad_norm": 1.00775146484375,
      "learning_rate": 2.6062501823457563e-05,
      "loss": 1.3668,
      "step": 863
    },
    {
      "epoch": 1.6179775280898876,
      "grad_norm": 1.1422683000564575,
      "learning_rate": 2.6008044898257915e-05,
      "loss": 1.3493,
      "step": 864
    },
    {
      "epoch": 1.6198501872659175,
      "grad_norm": 0.9461194276809692,
      "learning_rate": 2.5953583181778968e-05,
      "loss": 1.2792,
      "step": 865
    },
    {
      "epoch": 1.6217228464419475,
      "grad_norm": 1.0795692205429077,
      "learning_rate": 2.5899116932879534e-05,
      "loss": 1.3922,
      "step": 866
    },
    {
      "epoch": 1.6235955056179776,
      "grad_norm": 1.0307965278625488,
      "learning_rate": 2.5844646410439944e-05,
      "loss": 1.1646,
      "step": 867
    },
    {
      "epoch": 1.6254681647940075,
      "grad_norm": 1.0806962251663208,
      "learning_rate": 2.5790171873360862e-05,
      "loss": 1.2975,
      "step": 868
    },
    {
      "epoch": 1.6273408239700373,
      "grad_norm": 1.1565274000167847,
      "learning_rate": 2.573569358056202e-05,
      "loss": 1.3314,
      "step": 869
    },
    {
      "epoch": 1.6292134831460674,
      "grad_norm": 1.0630333423614502,
      "learning_rate": 2.5681211790981024e-05,
      "loss": 1.3864,
      "step": 870
    },
    {
      "epoch": 1.6310861423220975,
      "grad_norm": 0.991507887840271,
      "learning_rate": 2.5626726763572075e-05,
      "loss": 1.3111,
      "step": 871
    },
    {
      "epoch": 1.6329588014981273,
      "grad_norm": 0.9822735786437988,
      "learning_rate": 2.5572238757304768e-05,
      "loss": 1.3171,
      "step": 872
    },
    {
      "epoch": 1.6348314606741572,
      "grad_norm": 1.0748143196105957,
      "learning_rate": 2.5517748031162856e-05,
      "loss": 1.352,
      "step": 873
    },
    {
      "epoch": 1.6367041198501873,
      "grad_norm": 1.0666334629058838,
      "learning_rate": 2.546325484414305e-05,
      "loss": 1.3752,
      "step": 874
    },
    {
      "epoch": 1.6385767790262173,
      "grad_norm": 1.0377565622329712,
      "learning_rate": 2.540875945525371e-05,
      "loss": 1.3707,
      "step": 875
    },
    {
      "epoch": 1.6404494382022472,
      "grad_norm": 0.9721571207046509,
      "learning_rate": 2.5354262123513695e-05,
      "loss": 1.2865,
      "step": 876
    },
    {
      "epoch": 1.642322097378277,
      "grad_norm": 1.0097249746322632,
      "learning_rate": 2.529976310795108e-05,
      "loss": 1.2755,
      "step": 877
    },
    {
      "epoch": 1.6441947565543071,
      "grad_norm": 1.0355592966079712,
      "learning_rate": 2.524526266760195e-05,
      "loss": 1.3535,
      "step": 878
    },
    {
      "epoch": 1.6460674157303372,
      "grad_norm": 1.1093990802764893,
      "learning_rate": 2.519076106150917e-05,
      "loss": 1.4483,
      "step": 879
    },
    {
      "epoch": 1.647940074906367,
      "grad_norm": 0.9586570262908936,
      "learning_rate": 2.5136258548721137e-05,
      "loss": 1.3334,
      "step": 880
    },
    {
      "epoch": 1.649812734082397,
      "grad_norm": 1.1696444749832153,
      "learning_rate": 2.5081755388290567e-05,
      "loss": 1.4383,
      "step": 881
    },
    {
      "epoch": 1.651685393258427,
      "grad_norm": 1.238594889640808,
      "learning_rate": 2.502725183927323e-05,
      "loss": 1.3453,
      "step": 882
    },
    {
      "epoch": 1.653558052434457,
      "grad_norm": 1.1275105476379395,
      "learning_rate": 2.4972748160726775e-05,
      "loss": 1.3586,
      "step": 883
    },
    {
      "epoch": 1.655430711610487,
      "grad_norm": 1.0717966556549072,
      "learning_rate": 2.4918244611709436e-05,
      "loss": 1.2777,
      "step": 884
    },
    {
      "epoch": 1.6573033707865168,
      "grad_norm": 1.0903992652893066,
      "learning_rate": 2.486374145127886e-05,
      "loss": 1.3574,
      "step": 885
    },
    {
      "epoch": 1.6591760299625467,
      "grad_norm": 1.0468984842300415,
      "learning_rate": 2.4809238938490832e-05,
      "loss": 1.2695,
      "step": 886
    },
    {
      "epoch": 1.6610486891385767,
      "grad_norm": 1.0670475959777832,
      "learning_rate": 2.475473733239805e-05,
      "loss": 1.2543,
      "step": 887
    },
    {
      "epoch": 1.6629213483146068,
      "grad_norm": 1.0289729833602905,
      "learning_rate": 2.470023689204893e-05,
      "loss": 1.199,
      "step": 888
    },
    {
      "epoch": 1.6647940074906367,
      "grad_norm": 1.0721921920776367,
      "learning_rate": 2.464573787648632e-05,
      "loss": 1.298,
      "step": 889
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.9515213966369629,
      "learning_rate": 2.4591240544746294e-05,
      "loss": 1.2127,
      "step": 890
    },
    {
      "epoch": 1.6685393258426966,
      "grad_norm": 1.0336028337478638,
      "learning_rate": 2.4536745155856954e-05,
      "loss": 1.432,
      "step": 891
    },
    {
      "epoch": 1.6704119850187267,
      "grad_norm": 1.0463199615478516,
      "learning_rate": 2.4482251968837146e-05,
      "loss": 1.2729,
      "step": 892
    },
    {
      "epoch": 1.6722846441947565,
      "grad_norm": 1.106086254119873,
      "learning_rate": 2.4427761242695238e-05,
      "loss": 1.3152,
      "step": 893
    },
    {
      "epoch": 1.6741573033707864,
      "grad_norm": 1.0866484642028809,
      "learning_rate": 2.437327323642793e-05,
      "loss": 1.3434,
      "step": 894
    },
    {
      "epoch": 1.6760299625468165,
      "grad_norm": 1.0884792804718018,
      "learning_rate": 2.4318788209018975e-05,
      "loss": 1.3063,
      "step": 895
    },
    {
      "epoch": 1.6779026217228465,
      "grad_norm": 0.9717115163803101,
      "learning_rate": 2.4264306419437977e-05,
      "loss": 1.1588,
      "step": 896
    },
    {
      "epoch": 1.6797752808988764,
      "grad_norm": 1.0617504119873047,
      "learning_rate": 2.420982812663914e-05,
      "loss": 1.2111,
      "step": 897
    },
    {
      "epoch": 1.6816479400749063,
      "grad_norm": 1.0153367519378662,
      "learning_rate": 2.4155353589560072e-05,
      "loss": 1.2745,
      "step": 898
    },
    {
      "epoch": 1.6835205992509363,
      "grad_norm": 0.9857510924339294,
      "learning_rate": 2.4100883067120475e-05,
      "loss": 1.2785,
      "step": 899
    },
    {
      "epoch": 1.6853932584269664,
      "grad_norm": 1.063114881515503,
      "learning_rate": 2.4046416818221038e-05,
      "loss": 1.2884,
      "step": 900
    },
    {
      "epoch": 1.6872659176029963,
      "grad_norm": 1.0568842887878418,
      "learning_rate": 2.3991955101742088e-05,
      "loss": 1.2748,
      "step": 901
    },
    {
      "epoch": 1.6891385767790261,
      "grad_norm": 0.9972931742668152,
      "learning_rate": 2.393749817654244e-05,
      "loss": 1.3466,
      "step": 902
    },
    {
      "epoch": 1.6910112359550562,
      "grad_norm": 1.0300850868225098,
      "learning_rate": 2.3883046301458112e-05,
      "loss": 1.1855,
      "step": 903
    },
    {
      "epoch": 1.6928838951310863,
      "grad_norm": 1.1880137920379639,
      "learning_rate": 2.3828599735301138e-05,
      "loss": 1.3426,
      "step": 904
    },
    {
      "epoch": 1.6947565543071161,
      "grad_norm": 1.1717387437820435,
      "learning_rate": 2.3774158736858303e-05,
      "loss": 1.3508,
      "step": 905
    },
    {
      "epoch": 1.696629213483146,
      "grad_norm": 1.0815693140029907,
      "learning_rate": 2.3719723564889937e-05,
      "loss": 1.2994,
      "step": 906
    },
    {
      "epoch": 1.698501872659176,
      "grad_norm": 0.9951735138893127,
      "learning_rate": 2.366529447812868e-05,
      "loss": 1.3132,
      "step": 907
    },
    {
      "epoch": 1.7003745318352061,
      "grad_norm": 0.9547067880630493,
      "learning_rate": 2.3610871735278244e-05,
      "loss": 1.2679,
      "step": 908
    },
    {
      "epoch": 1.702247191011236,
      "grad_norm": 0.9534054398536682,
      "learning_rate": 2.3556455595012172e-05,
      "loss": 1.2578,
      "step": 909
    },
    {
      "epoch": 1.7041198501872659,
      "grad_norm": 1.0424695014953613,
      "learning_rate": 2.3502046315972656e-05,
      "loss": 1.2575,
      "step": 910
    },
    {
      "epoch": 1.7059925093632957,
      "grad_norm": 1.0251977443695068,
      "learning_rate": 2.3447644156769254e-05,
      "loss": 1.2192,
      "step": 911
    },
    {
      "epoch": 1.7078651685393258,
      "grad_norm": 1.0944221019744873,
      "learning_rate": 2.3393249375977687e-05,
      "loss": 1.3045,
      "step": 912
    },
    {
      "epoch": 1.7097378277153559,
      "grad_norm": 1.129737138748169,
      "learning_rate": 2.333886223213862e-05,
      "loss": 1.3128,
      "step": 913
    },
    {
      "epoch": 1.7116104868913857,
      "grad_norm": 1.0622994899749756,
      "learning_rate": 2.3284482983756392e-05,
      "loss": 1.2369,
      "step": 914
    },
    {
      "epoch": 1.7134831460674156,
      "grad_norm": 1.0022330284118652,
      "learning_rate": 2.3230111889297845e-05,
      "loss": 1.2702,
      "step": 915
    },
    {
      "epoch": 1.7153558052434457,
      "grad_norm": 1.0939692258834839,
      "learning_rate": 2.317574920719105e-05,
      "loss": 1.289,
      "step": 916
    },
    {
      "epoch": 1.7172284644194757,
      "grad_norm": 1.0822967290878296,
      "learning_rate": 2.3121395195824078e-05,
      "loss": 1.3314,
      "step": 917
    },
    {
      "epoch": 1.7191011235955056,
      "grad_norm": 1.1359766721725464,
      "learning_rate": 2.306705011354383e-05,
      "loss": 1.2611,
      "step": 918
    },
    {
      "epoch": 1.7209737827715355,
      "grad_norm": 1.0958340167999268,
      "learning_rate": 2.3012714218654717e-05,
      "loss": 1.3523,
      "step": 919
    },
    {
      "epoch": 1.7228464419475655,
      "grad_norm": 1.304897427558899,
      "learning_rate": 2.295838776941751e-05,
      "loss": 1.2889,
      "step": 920
    },
    {
      "epoch": 1.7247191011235956,
      "grad_norm": 1.1145442724227905,
      "learning_rate": 2.290407102404809e-05,
      "loss": 1.095,
      "step": 921
    },
    {
      "epoch": 1.7265917602996255,
      "grad_norm": 0.9568424224853516,
      "learning_rate": 2.2849764240716204e-05,
      "loss": 1.3204,
      "step": 922
    },
    {
      "epoch": 1.7284644194756553,
      "grad_norm": 1.1881036758422852,
      "learning_rate": 2.2795467677544235e-05,
      "loss": 1.2726,
      "step": 923
    },
    {
      "epoch": 1.7303370786516854,
      "grad_norm": 1.2104711532592773,
      "learning_rate": 2.2741181592606025e-05,
      "loss": 1.2591,
      "step": 924
    },
    {
      "epoch": 1.7322097378277155,
      "grad_norm": 1.2524596452713013,
      "learning_rate": 2.2686906243925578e-05,
      "loss": 1.2884,
      "step": 925
    },
    {
      "epoch": 1.7340823970037453,
      "grad_norm": 1.0872174501419067,
      "learning_rate": 2.2632641889475887e-05,
      "loss": 1.2584,
      "step": 926
    },
    {
      "epoch": 1.7359550561797752,
      "grad_norm": 1.1651675701141357,
      "learning_rate": 2.257838878717769e-05,
      "loss": 1.3711,
      "step": 927
    },
    {
      "epoch": 1.7378277153558053,
      "grad_norm": 1.0537300109863281,
      "learning_rate": 2.2524147194898224e-05,
      "loss": 1.2383,
      "step": 928
    },
    {
      "epoch": 1.7397003745318353,
      "grad_norm": 1.1934752464294434,
      "learning_rate": 2.2469917370450032e-05,
      "loss": 1.2639,
      "step": 929
    },
    {
      "epoch": 1.7415730337078652,
      "grad_norm": 1.03178870677948,
      "learning_rate": 2.2415699571589735e-05,
      "loss": 1.3066,
      "step": 930
    },
    {
      "epoch": 1.743445692883895,
      "grad_norm": 1.2016677856445312,
      "learning_rate": 2.2361494056016775e-05,
      "loss": 1.2856,
      "step": 931
    },
    {
      "epoch": 1.7453183520599251,
      "grad_norm": 1.390724778175354,
      "learning_rate": 2.2307301081372224e-05,
      "loss": 1.3036,
      "step": 932
    },
    {
      "epoch": 1.7471910112359552,
      "grad_norm": 1.1351279020309448,
      "learning_rate": 2.225312090523754e-05,
      "loss": 1.2322,
      "step": 933
    },
    {
      "epoch": 1.749063670411985,
      "grad_norm": 1.1628310680389404,
      "learning_rate": 2.2198953785133362e-05,
      "loss": 1.3026,
      "step": 934
    },
    {
      "epoch": 1.750936329588015,
      "grad_norm": 1.0688996315002441,
      "learning_rate": 2.2144799978518254e-05,
      "loss": 1.2848,
      "step": 935
    },
    {
      "epoch": 1.7528089887640448,
      "grad_norm": 1.0769550800323486,
      "learning_rate": 2.20906597427875e-05,
      "loss": 1.3179,
      "step": 936
    },
    {
      "epoch": 1.7546816479400749,
      "grad_norm": 1.1994092464447021,
      "learning_rate": 2.2036533335271916e-05,
      "loss": 1.3151,
      "step": 937
    },
    {
      "epoch": 1.756554307116105,
      "grad_norm": 1.1525059938430786,
      "learning_rate": 2.1982421013236545e-05,
      "loss": 1.4434,
      "step": 938
    },
    {
      "epoch": 1.7584269662921348,
      "grad_norm": 1.1601327657699585,
      "learning_rate": 2.1928323033879506e-05,
      "loss": 1.4251,
      "step": 939
    },
    {
      "epoch": 1.7602996254681647,
      "grad_norm": 1.0079610347747803,
      "learning_rate": 2.187423965433075e-05,
      "loss": 1.2183,
      "step": 940
    },
    {
      "epoch": 1.7621722846441947,
      "grad_norm": 1.0473034381866455,
      "learning_rate": 2.1820171131650823e-05,
      "loss": 1.354,
      "step": 941
    },
    {
      "epoch": 1.7640449438202248,
      "grad_norm": 1.1745434999465942,
      "learning_rate": 2.1766117722829658e-05,
      "loss": 1.3586,
      "step": 942
    },
    {
      "epoch": 1.7659176029962547,
      "grad_norm": 1.2333946228027344,
      "learning_rate": 2.1712079684785363e-05,
      "loss": 1.2917,
      "step": 943
    },
    {
      "epoch": 1.7677902621722845,
      "grad_norm": 1.0970174074172974,
      "learning_rate": 2.1658057274362965e-05,
      "loss": 1.257,
      "step": 944
    },
    {
      "epoch": 1.7696629213483146,
      "grad_norm": 0.9616312980651855,
      "learning_rate": 2.160405074833324e-05,
      "loss": 1.1943,
      "step": 945
    },
    {
      "epoch": 1.7715355805243447,
      "grad_norm": 1.049808382987976,
      "learning_rate": 2.155006036339144e-05,
      "loss": 1.3512,
      "step": 946
    },
    {
      "epoch": 1.7734082397003745,
      "grad_norm": 1.0432745218276978,
      "learning_rate": 2.1496086376156128e-05,
      "loss": 1.2912,
      "step": 947
    },
    {
      "epoch": 1.7752808988764044,
      "grad_norm": 1.1068133115768433,
      "learning_rate": 2.1442129043167874e-05,
      "loss": 1.2646,
      "step": 948
    },
    {
      "epoch": 1.7771535580524345,
      "grad_norm": 1.0089093446731567,
      "learning_rate": 2.1388188620888154e-05,
      "loss": 1.3119,
      "step": 949
    },
    {
      "epoch": 1.7790262172284645,
      "grad_norm": 1.0091196298599243,
      "learning_rate": 2.133426536569802e-05,
      "loss": 1.2878,
      "step": 950
    },
    {
      "epoch": 1.7808988764044944,
      "grad_norm": 1.1685925722122192,
      "learning_rate": 2.1280359533896954e-05,
      "loss": 1.2701,
      "step": 951
    },
    {
      "epoch": 1.7827715355805243,
      "grad_norm": 1.1106324195861816,
      "learning_rate": 2.1226471381701616e-05,
      "loss": 1.3574,
      "step": 952
    },
    {
      "epoch": 1.7846441947565543,
      "grad_norm": 1.1213018894195557,
      "learning_rate": 2.1172601165244623e-05,
      "loss": 1.3074,
      "step": 953
    },
    {
      "epoch": 1.7865168539325844,
      "grad_norm": 1.1609965562820435,
      "learning_rate": 2.111874914057336e-05,
      "loss": 1.3102,
      "step": 954
    },
    {
      "epoch": 1.7883895131086143,
      "grad_norm": 1.004207968711853,
      "learning_rate": 2.1064915563648734e-05,
      "loss": 1.3585,
      "step": 955
    },
    {
      "epoch": 1.7902621722846441,
      "grad_norm": 1.062008261680603,
      "learning_rate": 2.101110069034398e-05,
      "loss": 1.36,
      "step": 956
    },
    {
      "epoch": 1.7921348314606742,
      "grad_norm": 1.1189767122268677,
      "learning_rate": 2.0957304776443438e-05,
      "loss": 1.2986,
      "step": 957
    },
    {
      "epoch": 1.7940074906367043,
      "grad_norm": 1.1741441488265991,
      "learning_rate": 2.0903528077641297e-05,
      "loss": 1.3471,
      "step": 958
    },
    {
      "epoch": 1.7958801498127341,
      "grad_norm": 1.020076036453247,
      "learning_rate": 2.0849770849540445e-05,
      "loss": 1.2671,
      "step": 959
    },
    {
      "epoch": 1.797752808988764,
      "grad_norm": 1.1482598781585693,
      "learning_rate": 2.0796033347651238e-05,
      "loss": 1.3643,
      "step": 960
    },
    {
      "epoch": 1.7996254681647939,
      "grad_norm": 1.1116681098937988,
      "learning_rate": 2.0742315827390244e-05,
      "loss": 1.3344,
      "step": 961
    },
    {
      "epoch": 1.801498127340824,
      "grad_norm": 1.0406111478805542,
      "learning_rate": 2.0688618544079074e-05,
      "loss": 1.3064,
      "step": 962
    },
    {
      "epoch": 1.803370786516854,
      "grad_norm": 1.1247254610061646,
      "learning_rate": 2.0634941752943142e-05,
      "loss": 1.2095,
      "step": 963
    },
    {
      "epoch": 1.8052434456928839,
      "grad_norm": 1.2099323272705078,
      "learning_rate": 2.0581285709110475e-05,
      "loss": 1.2515,
      "step": 964
    },
    {
      "epoch": 1.8071161048689137,
      "grad_norm": 1.2590806484222412,
      "learning_rate": 2.0527650667610478e-05,
      "loss": 1.3501,
      "step": 965
    },
    {
      "epoch": 1.8089887640449438,
      "grad_norm": 1.209402322769165,
      "learning_rate": 2.047403688337272e-05,
      "loss": 1.3595,
      "step": 966
    },
    {
      "epoch": 1.8108614232209739,
      "grad_norm": 1.1121923923492432,
      "learning_rate": 2.042044461122577e-05,
      "loss": 1.2756,
      "step": 967
    },
    {
      "epoch": 1.8127340823970037,
      "grad_norm": 1.0878556966781616,
      "learning_rate": 2.03668741058959e-05,
      "loss": 1.2201,
      "step": 968
    },
    {
      "epoch": 1.8146067415730336,
      "grad_norm": 1.049307107925415,
      "learning_rate": 2.0313325622005955e-05,
      "loss": 1.3602,
      "step": 969
    },
    {
      "epoch": 1.8164794007490637,
      "grad_norm": 1.0807734727859497,
      "learning_rate": 2.0259799414074112e-05,
      "loss": 1.4228,
      "step": 970
    },
    {
      "epoch": 1.8183520599250937,
      "grad_norm": 1.0595670938491821,
      "learning_rate": 2.020629573651266e-05,
      "loss": 1.2062,
      "step": 971
    },
    {
      "epoch": 1.8202247191011236,
      "grad_norm": 1.1388182640075684,
      "learning_rate": 2.015281484362679e-05,
      "loss": 1.2592,
      "step": 972
    },
    {
      "epoch": 1.8220973782771535,
      "grad_norm": 1.0432617664337158,
      "learning_rate": 2.0099356989613426e-05,
      "loss": 1.2186,
      "step": 973
    },
    {
      "epoch": 1.8239700374531835,
      "grad_norm": 1.1288129091262817,
      "learning_rate": 2.0045922428559956e-05,
      "loss": 1.2531,
      "step": 974
    },
    {
      "epoch": 1.8258426966292136,
      "grad_norm": 1.1908711194992065,
      "learning_rate": 1.9992511414443083e-05,
      "loss": 1.2138,
      "step": 975
    },
    {
      "epoch": 1.8277153558052435,
      "grad_norm": 1.175862193107605,
      "learning_rate": 1.993912420112756e-05,
      "loss": 1.301,
      "step": 976
    },
    {
      "epoch": 1.8295880149812733,
      "grad_norm": 1.1276028156280518,
      "learning_rate": 1.9885761042365056e-05,
      "loss": 1.2865,
      "step": 977
    },
    {
      "epoch": 1.8314606741573034,
      "grad_norm": 1.1258758306503296,
      "learning_rate": 1.983242219179285e-05,
      "loss": 1.3403,
      "step": 978
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 1.1476311683654785,
      "learning_rate": 1.9779107902932742e-05,
      "loss": 1.2319,
      "step": 979
    },
    {
      "epoch": 1.8352059925093633,
      "grad_norm": 1.1680852174758911,
      "learning_rate": 1.972581842918974e-05,
      "loss": 1.2989,
      "step": 980
    },
    {
      "epoch": 1.8370786516853932,
      "grad_norm": 1.264047384262085,
      "learning_rate": 1.967255402385095e-05,
      "loss": 1.3254,
      "step": 981
    },
    {
      "epoch": 1.8389513108614233,
      "grad_norm": 1.1015524864196777,
      "learning_rate": 1.961931494008429e-05,
      "loss": 1.2603,
      "step": 982
    },
    {
      "epoch": 1.8408239700374533,
      "grad_norm": 1.1924800872802734,
      "learning_rate": 1.9566101430937345e-05,
      "loss": 1.3876,
      "step": 983
    },
    {
      "epoch": 1.8426966292134832,
      "grad_norm": 1.130220651626587,
      "learning_rate": 1.9512913749336133e-05,
      "loss": 1.2493,
      "step": 984
    },
    {
      "epoch": 1.844569288389513,
      "grad_norm": 1.0685858726501465,
      "learning_rate": 1.9459752148083905e-05,
      "loss": 1.2487,
      "step": 985
    },
    {
      "epoch": 1.846441947565543,
      "grad_norm": 1.1714071035385132,
      "learning_rate": 1.940661687985998e-05,
      "loss": 1.288,
      "step": 986
    },
    {
      "epoch": 1.848314606741573,
      "grad_norm": 1.0802216529846191,
      "learning_rate": 1.935350819721849e-05,
      "loss": 1.3204,
      "step": 987
    },
    {
      "epoch": 1.850187265917603,
      "grad_norm": 1.143471360206604,
      "learning_rate": 1.9300426352587205e-05,
      "loss": 1.235,
      "step": 988
    },
    {
      "epoch": 1.852059925093633,
      "grad_norm": 1.179707646369934,
      "learning_rate": 1.9247371598266335e-05,
      "loss": 1.2742,
      "step": 989
    },
    {
      "epoch": 1.8539325842696628,
      "grad_norm": 1.187306523323059,
      "learning_rate": 1.9194344186427344e-05,
      "loss": 1.2689,
      "step": 990
    },
    {
      "epoch": 1.8558052434456929,
      "grad_norm": 1.1209431886672974,
      "learning_rate": 1.9141344369111713e-05,
      "loss": 1.2115,
      "step": 991
    },
    {
      "epoch": 1.857677902621723,
      "grad_norm": 1.183603048324585,
      "learning_rate": 1.908837239822979e-05,
      "loss": 1.3471,
      "step": 992
    },
    {
      "epoch": 1.8595505617977528,
      "grad_norm": 1.3075745105743408,
      "learning_rate": 1.9035428525559536e-05,
      "loss": 1.3139,
      "step": 993
    },
    {
      "epoch": 1.8614232209737827,
      "grad_norm": 1.126441478729248,
      "learning_rate": 1.89825130027454e-05,
      "loss": 1.2567,
      "step": 994
    },
    {
      "epoch": 1.8632958801498127,
      "grad_norm": 0.9902331829071045,
      "learning_rate": 1.8929626081297047e-05,
      "loss": 1.3115,
      "step": 995
    },
    {
      "epoch": 1.8651685393258428,
      "grad_norm": 1.2035807371139526,
      "learning_rate": 1.887676801258821e-05,
      "loss": 1.3506,
      "step": 996
    },
    {
      "epoch": 1.8670411985018727,
      "grad_norm": 1.042359471321106,
      "learning_rate": 1.88239390478555e-05,
      "loss": 1.235,
      "step": 997
    },
    {
      "epoch": 1.8689138576779025,
      "grad_norm": 1.1582010984420776,
      "learning_rate": 1.8771139438197168e-05,
      "loss": 1.2967,
      "step": 998
    },
    {
      "epoch": 1.8707865168539326,
      "grad_norm": 1.1388074159622192,
      "learning_rate": 1.871836943457196e-05,
      "loss": 1.4084,
      "step": 999
    },
    {
      "epoch": 1.8726591760299627,
      "grad_norm": 1.1543635129928589,
      "learning_rate": 1.866562928779789e-05,
      "loss": 1.2892,
      "step": 1000
    },
    {
      "epoch": 1.8745318352059925,
      "grad_norm": 1.1185835599899292,
      "learning_rate": 1.861291924855108e-05,
      "loss": 1.3183,
      "step": 1001
    },
    {
      "epoch": 1.8764044943820224,
      "grad_norm": 1.1707240343093872,
      "learning_rate": 1.8560239567364524e-05,
      "loss": 1.3138,
      "step": 1002
    },
    {
      "epoch": 1.8782771535580525,
      "grad_norm": 1.3256136178970337,
      "learning_rate": 1.8507590494626944e-05,
      "loss": 1.2961,
      "step": 1003
    },
    {
      "epoch": 1.8801498127340825,
      "grad_norm": 1.0535244941711426,
      "learning_rate": 1.8454972280581567e-05,
      "loss": 1.2706,
      "step": 1004
    },
    {
      "epoch": 1.8820224719101124,
      "grad_norm": 1.1831110715866089,
      "learning_rate": 1.8402385175324955e-05,
      "loss": 1.2477,
      "step": 1005
    },
    {
      "epoch": 1.8838951310861423,
      "grad_norm": 1.0950068235397339,
      "learning_rate": 1.834982942880581e-05,
      "loss": 1.4311,
      "step": 1006
    },
    {
      "epoch": 1.8857677902621723,
      "grad_norm": 1.1551523208618164,
      "learning_rate": 1.8297305290823764e-05,
      "loss": 1.3331,
      "step": 1007
    },
    {
      "epoch": 1.8876404494382022,
      "grad_norm": 1.0802067518234253,
      "learning_rate": 1.8244813011028234e-05,
      "loss": 1.2812,
      "step": 1008
    },
    {
      "epoch": 1.8895131086142323,
      "grad_norm": 1.1824744939804077,
      "learning_rate": 1.819235283891721e-05,
      "loss": 1.4147,
      "step": 1009
    },
    {
      "epoch": 1.8913857677902621,
      "grad_norm": 1.2030822038650513,
      "learning_rate": 1.8139925023836067e-05,
      "loss": 1.2724,
      "step": 1010
    },
    {
      "epoch": 1.893258426966292,
      "grad_norm": 1.1996158361434937,
      "learning_rate": 1.8087529814976394e-05,
      "loss": 1.4249,
      "step": 1011
    },
    {
      "epoch": 1.895131086142322,
      "grad_norm": 1.113403558731079,
      "learning_rate": 1.803516746137479e-05,
      "loss": 1.1557,
      "step": 1012
    },
    {
      "epoch": 1.8970037453183521,
      "grad_norm": 1.1224273443222046,
      "learning_rate": 1.798283821191171e-05,
      "loss": 1.1993,
      "step": 1013
    },
    {
      "epoch": 1.898876404494382,
      "grad_norm": 1.1384824514389038,
      "learning_rate": 1.793054231531024e-05,
      "loss": 1.1068,
      "step": 1014
    },
    {
      "epoch": 1.9007490636704119,
      "grad_norm": 1.0865018367767334,
      "learning_rate": 1.787828002013495e-05,
      "loss": 1.3125,
      "step": 1015
    },
    {
      "epoch": 1.902621722846442,
      "grad_norm": 1.1623940467834473,
      "learning_rate": 1.7826051574790715e-05,
      "loss": 1.3229,
      "step": 1016
    },
    {
      "epoch": 1.904494382022472,
      "grad_norm": 1.2281415462493896,
      "learning_rate": 1.7773857227521484e-05,
      "loss": 1.2289,
      "step": 1017
    },
    {
      "epoch": 1.9063670411985019,
      "grad_norm": 1.2437984943389893,
      "learning_rate": 1.7721697226409174e-05,
      "loss": 1.2518,
      "step": 1018
    },
    {
      "epoch": 1.9082397003745317,
      "grad_norm": 1.1201587915420532,
      "learning_rate": 1.766957181937243e-05,
      "loss": 1.3385,
      "step": 1019
    },
    {
      "epoch": 1.9101123595505618,
      "grad_norm": 1.1606731414794922,
      "learning_rate": 1.7617481254165487e-05,
      "loss": 1.4106,
      "step": 1020
    },
    {
      "epoch": 1.9119850187265919,
      "grad_norm": 1.2297905683517456,
      "learning_rate": 1.756542577837696e-05,
      "loss": 1.2612,
      "step": 1021
    },
    {
      "epoch": 1.9138576779026217,
      "grad_norm": 1.2128829956054688,
      "learning_rate": 1.751340563942869e-05,
      "loss": 1.3028,
      "step": 1022
    },
    {
      "epoch": 1.9157303370786516,
      "grad_norm": 1.1327913999557495,
      "learning_rate": 1.7461421084574555e-05,
      "loss": 1.3223,
      "step": 1023
    },
    {
      "epoch": 1.9176029962546817,
      "grad_norm": 1.2326289415359497,
      "learning_rate": 1.740947236089932e-05,
      "loss": 1.2246,
      "step": 1024
    },
    {
      "epoch": 1.9194756554307117,
      "grad_norm": 1.072633981704712,
      "learning_rate": 1.7357559715317412e-05,
      "loss": 1.1952,
      "step": 1025
    },
    {
      "epoch": 1.9213483146067416,
      "grad_norm": 1.222001552581787,
      "learning_rate": 1.7305683394571802e-05,
      "loss": 1.2566,
      "step": 1026
    },
    {
      "epoch": 1.9232209737827715,
      "grad_norm": 1.2909985780715942,
      "learning_rate": 1.7253843645232787e-05,
      "loss": 1.3059,
      "step": 1027
    },
    {
      "epoch": 1.9250936329588015,
      "grad_norm": 1.2192671298980713,
      "learning_rate": 1.7202040713696865e-05,
      "loss": 1.4002,
      "step": 1028
    },
    {
      "epoch": 1.9269662921348316,
      "grad_norm": 1.2142003774642944,
      "learning_rate": 1.7150274846185507e-05,
      "loss": 1.2661,
      "step": 1029
    },
    {
      "epoch": 1.9288389513108615,
      "grad_norm": 1.278149127960205,
      "learning_rate": 1.7098546288744046e-05,
      "loss": 1.3144,
      "step": 1030
    },
    {
      "epoch": 1.9307116104868913,
      "grad_norm": 1.0779445171356201,
      "learning_rate": 1.704685528724046e-05,
      "loss": 1.3112,
      "step": 1031
    },
    {
      "epoch": 1.9325842696629212,
      "grad_norm": 1.1693675518035889,
      "learning_rate": 1.6995202087364227e-05,
      "loss": 1.1293,
      "step": 1032
    },
    {
      "epoch": 1.9344569288389513,
      "grad_norm": 1.2009308338165283,
      "learning_rate": 1.6943586934625156e-05,
      "loss": 1.2767,
      "step": 1033
    },
    {
      "epoch": 1.9363295880149813,
      "grad_norm": 1.2020072937011719,
      "learning_rate": 1.68920100743522e-05,
      "loss": 1.3164,
      "step": 1034
    },
    {
      "epoch": 1.9382022471910112,
      "grad_norm": 1.1903016567230225,
      "learning_rate": 1.684047175169234e-05,
      "loss": 1.149,
      "step": 1035
    },
    {
      "epoch": 1.940074906367041,
      "grad_norm": 1.1517014503479004,
      "learning_rate": 1.678897221160936e-05,
      "loss": 1.3042,
      "step": 1036
    },
    {
      "epoch": 1.9419475655430711,
      "grad_norm": 1.289063572883606,
      "learning_rate": 1.6737511698882708e-05,
      "loss": 1.2609,
      "step": 1037
    },
    {
      "epoch": 1.9438202247191012,
      "grad_norm": 1.173936128616333,
      "learning_rate": 1.668609045810633e-05,
      "loss": 1.2032,
      "step": 1038
    },
    {
      "epoch": 1.945692883895131,
      "grad_norm": 1.285391092300415,
      "learning_rate": 1.663470873368754e-05,
      "loss": 1.2307,
      "step": 1039
    },
    {
      "epoch": 1.947565543071161,
      "grad_norm": 1.150516152381897,
      "learning_rate": 1.6583366769845798e-05,
      "loss": 1.2526,
      "step": 1040
    },
    {
      "epoch": 1.949438202247191,
      "grad_norm": 1.125745415687561,
      "learning_rate": 1.6532064810611596e-05,
      "loss": 1.259,
      "step": 1041
    },
    {
      "epoch": 1.951310861423221,
      "grad_norm": 1.1697620153427124,
      "learning_rate": 1.648080309982528e-05,
      "loss": 1.2725,
      "step": 1042
    },
    {
      "epoch": 1.953183520599251,
      "grad_norm": 1.0694130659103394,
      "learning_rate": 1.642958188113589e-05,
      "loss": 1.2538,
      "step": 1043
    },
    {
      "epoch": 1.9550561797752808,
      "grad_norm": 1.2783619165420532,
      "learning_rate": 1.6378401398000008e-05,
      "loss": 1.3071,
      "step": 1044
    },
    {
      "epoch": 1.9569288389513109,
      "grad_norm": 1.3463118076324463,
      "learning_rate": 1.632726189368059e-05,
      "loss": 1.1488,
      "step": 1045
    },
    {
      "epoch": 1.958801498127341,
      "grad_norm": 1.1800421476364136,
      "learning_rate": 1.6276163611245843e-05,
      "loss": 1.2471,
      "step": 1046
    },
    {
      "epoch": 1.9606741573033708,
      "grad_norm": 1.1557660102844238,
      "learning_rate": 1.622510679356801e-05,
      "loss": 1.2145,
      "step": 1047
    },
    {
      "epoch": 1.9625468164794007,
      "grad_norm": 1.1565335988998413,
      "learning_rate": 1.617409168332227e-05,
      "loss": 1.44,
      "step": 1048
    },
    {
      "epoch": 1.9644194756554307,
      "grad_norm": 1.1740901470184326,
      "learning_rate": 1.6123118522985563e-05,
      "loss": 1.1932,
      "step": 1049
    },
    {
      "epoch": 1.9662921348314608,
      "grad_norm": 1.1621246337890625,
      "learning_rate": 1.607218755483545e-05,
      "loss": 1.3166,
      "step": 1050
    },
    {
      "epoch": 1.9681647940074907,
      "grad_norm": 1.0943100452423096,
      "learning_rate": 1.602129902094892e-05,
      "loss": 1.3305,
      "step": 1051
    },
    {
      "epoch": 1.9700374531835205,
      "grad_norm": 1.221378207206726,
      "learning_rate": 1.5970453163201304e-05,
      "loss": 1.1921,
      "step": 1052
    },
    {
      "epoch": 1.9719101123595506,
      "grad_norm": 1.1342533826828003,
      "learning_rate": 1.591965022326507e-05,
      "loss": 1.339,
      "step": 1053
    },
    {
      "epoch": 1.9737827715355807,
      "grad_norm": 1.2364850044250488,
      "learning_rate": 1.5868890442608714e-05,
      "loss": 1.2797,
      "step": 1054
    },
    {
      "epoch": 1.9756554307116105,
      "grad_norm": 1.2039624452590942,
      "learning_rate": 1.581817406249557e-05,
      "loss": 1.3994,
      "step": 1055
    },
    {
      "epoch": 1.9775280898876404,
      "grad_norm": 1.0989940166473389,
      "learning_rate": 1.576750132398271e-05,
      "loss": 1.3354,
      "step": 1056
    },
    {
      "epoch": 1.9794007490636703,
      "grad_norm": 1.1345152854919434,
      "learning_rate": 1.5716872467919752e-05,
      "loss": 1.268,
      "step": 1057
    },
    {
      "epoch": 1.9812734082397003,
      "grad_norm": 1.1354411840438843,
      "learning_rate": 1.5666287734947765e-05,
      "loss": 1.161,
      "step": 1058
    },
    {
      "epoch": 1.9831460674157304,
      "grad_norm": 1.474947214126587,
      "learning_rate": 1.5615747365498083e-05,
      "loss": 1.3388,
      "step": 1059
    },
    {
      "epoch": 1.9850187265917603,
      "grad_norm": 1.2373164892196655,
      "learning_rate": 1.5565251599791187e-05,
      "loss": 1.3474,
      "step": 1060
    },
    {
      "epoch": 1.9868913857677901,
      "grad_norm": 1.2054306268692017,
      "learning_rate": 1.5514800677835547e-05,
      "loss": 1.2604,
      "step": 1061
    },
    {
      "epoch": 1.9887640449438202,
      "grad_norm": 1.1842292547225952,
      "learning_rate": 1.5464394839426487e-05,
      "loss": 1.3456,
      "step": 1062
    },
    {
      "epoch": 1.9906367041198503,
      "grad_norm": 1.152083158493042,
      "learning_rate": 1.541403432414506e-05,
      "loss": 1.3208,
      "step": 1063
    },
    {
      "epoch": 1.9925093632958801,
      "grad_norm": 1.1475942134857178,
      "learning_rate": 1.536371937135688e-05,
      "loss": 1.2938,
      "step": 1064
    },
    {
      "epoch": 1.99438202247191,
      "grad_norm": 1.1067684888839722,
      "learning_rate": 1.5313450220211024e-05,
      "loss": 1.3692,
      "step": 1065
    },
    {
      "epoch": 1.99625468164794,
      "grad_norm": 1.1023945808410645,
      "learning_rate": 1.5263227109638844e-05,
      "loss": 1.0737,
      "step": 1066
    },
    {
      "epoch": 1.9981273408239701,
      "grad_norm": 1.0866942405700684,
      "learning_rate": 1.521305027835287e-05,
      "loss": 1.2198,
      "step": 1067
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.2875165939331055,
      "learning_rate": 1.5162919964845664e-05,
      "loss": 1.2746,
      "step": 1068
    },
    {
      "epoch": 2.00187265917603,
      "grad_norm": 1.1514012813568115,
      "learning_rate": 1.5112836407388703e-05,
      "loss": 1.2227,
      "step": 1069
    },
    {
      "epoch": 2.0037453183520597,
      "grad_norm": 1.1536037921905518,
      "learning_rate": 1.5062799844031206e-05,
      "loss": 1.2098,
      "step": 1070
    },
    {
      "epoch": 2.00561797752809,
      "grad_norm": 1.0903823375701904,
      "learning_rate": 1.501281051259904e-05,
      "loss": 1.2683,
      "step": 1071
    },
    {
      "epoch": 2.00749063670412,
      "grad_norm": 1.0785632133483887,
      "learning_rate": 1.496286865069358e-05,
      "loss": 1.3248,
      "step": 1072
    },
    {
      "epoch": 2.0093632958801497,
      "grad_norm": 1.4051576852798462,
      "learning_rate": 1.4912974495690569e-05,
      "loss": 1.2996,
      "step": 1073
    },
    {
      "epoch": 2.0112359550561796,
      "grad_norm": 1.1871832609176636,
      "learning_rate": 1.4863128284738997e-05,
      "loss": 1.0189,
      "step": 1074
    },
    {
      "epoch": 2.01310861423221,
      "grad_norm": 1.3198024034500122,
      "learning_rate": 1.4813330254759985e-05,
      "loss": 1.3876,
      "step": 1075
    },
    {
      "epoch": 2.0149812734082397,
      "grad_norm": 1.2259944677352905,
      "learning_rate": 1.4763580642445621e-05,
      "loss": 1.1968,
      "step": 1076
    },
    {
      "epoch": 2.0168539325842696,
      "grad_norm": 1.1796801090240479,
      "learning_rate": 1.4713879684257898e-05,
      "loss": 1.1349,
      "step": 1077
    },
    {
      "epoch": 2.0187265917602994,
      "grad_norm": 1.2253403663635254,
      "learning_rate": 1.4664227616427517e-05,
      "loss": 1.1745,
      "step": 1078
    },
    {
      "epoch": 2.0205992509363297,
      "grad_norm": 1.3413136005401611,
      "learning_rate": 1.4614624674952842e-05,
      "loss": 1.2741,
      "step": 1079
    },
    {
      "epoch": 2.0224719101123596,
      "grad_norm": 1.351271152496338,
      "learning_rate": 1.4565071095598698e-05,
      "loss": 1.1421,
      "step": 1080
    },
    {
      "epoch": 2.0243445692883895,
      "grad_norm": 1.2093826532363892,
      "learning_rate": 1.4515567113895318e-05,
      "loss": 1.1227,
      "step": 1081
    },
    {
      "epoch": 2.0262172284644193,
      "grad_norm": 1.2984261512756348,
      "learning_rate": 1.4466112965137182e-05,
      "loss": 1.1258,
      "step": 1082
    },
    {
      "epoch": 2.0280898876404496,
      "grad_norm": 1.3600876331329346,
      "learning_rate": 1.4416708884381914e-05,
      "loss": 1.2082,
      "step": 1083
    },
    {
      "epoch": 2.0299625468164795,
      "grad_norm": 1.1908825635910034,
      "learning_rate": 1.4367355106449149e-05,
      "loss": 1.1501,
      "step": 1084
    },
    {
      "epoch": 2.0318352059925093,
      "grad_norm": 1.3573112487792969,
      "learning_rate": 1.431805186591948e-05,
      "loss": 1.1895,
      "step": 1085
    },
    {
      "epoch": 2.033707865168539,
      "grad_norm": 1.2933967113494873,
      "learning_rate": 1.426879939713322e-05,
      "loss": 1.2115,
      "step": 1086
    },
    {
      "epoch": 2.0355805243445695,
      "grad_norm": 1.247583270072937,
      "learning_rate": 1.421959793418942e-05,
      "loss": 1.1207,
      "step": 1087
    },
    {
      "epoch": 2.0374531835205993,
      "grad_norm": 1.2312077283859253,
      "learning_rate": 1.4170447710944664e-05,
      "loss": 1.2089,
      "step": 1088
    },
    {
      "epoch": 2.039325842696629,
      "grad_norm": 1.1905326843261719,
      "learning_rate": 1.4121348961012026e-05,
      "loss": 1.09,
      "step": 1089
    },
    {
      "epoch": 2.041198501872659,
      "grad_norm": 1.2453820705413818,
      "learning_rate": 1.4072301917759895e-05,
      "loss": 1.121,
      "step": 1090
    },
    {
      "epoch": 2.0430711610486894,
      "grad_norm": 1.3331313133239746,
      "learning_rate": 1.4023306814310906e-05,
      "loss": 1.2465,
      "step": 1091
    },
    {
      "epoch": 2.044943820224719,
      "grad_norm": 1.3647812604904175,
      "learning_rate": 1.397436388354082e-05,
      "loss": 1.2117,
      "step": 1092
    },
    {
      "epoch": 2.046816479400749,
      "grad_norm": 1.1711227893829346,
      "learning_rate": 1.3925473358077412e-05,
      "loss": 1.1997,
      "step": 1093
    },
    {
      "epoch": 2.048689138576779,
      "grad_norm": 1.3640425205230713,
      "learning_rate": 1.3876635470299404e-05,
      "loss": 1.2232,
      "step": 1094
    },
    {
      "epoch": 2.050561797752809,
      "grad_norm": 1.368920922279358,
      "learning_rate": 1.3827850452335301e-05,
      "loss": 1.2291,
      "step": 1095
    },
    {
      "epoch": 2.052434456928839,
      "grad_norm": 1.1270297765731812,
      "learning_rate": 1.3779118536062296e-05,
      "loss": 1.1388,
      "step": 1096
    },
    {
      "epoch": 2.054307116104869,
      "grad_norm": 1.33022141456604,
      "learning_rate": 1.3730439953105243e-05,
      "loss": 1.2257,
      "step": 1097
    },
    {
      "epoch": 2.056179775280899,
      "grad_norm": 1.3706765174865723,
      "learning_rate": 1.3681814934835458e-05,
      "loss": 1.1689,
      "step": 1098
    },
    {
      "epoch": 2.0580524344569286,
      "grad_norm": 1.3644155263900757,
      "learning_rate": 1.363324371236967e-05,
      "loss": 1.2028,
      "step": 1099
    },
    {
      "epoch": 2.059925093632959,
      "grad_norm": 1.2131725549697876,
      "learning_rate": 1.3584726516568921e-05,
      "loss": 1.1263,
      "step": 1100
    },
    {
      "epoch": 2.061797752808989,
      "grad_norm": 1.2701045274734497,
      "learning_rate": 1.353626357803745e-05,
      "loss": 1.1073,
      "step": 1101
    },
    {
      "epoch": 2.0636704119850187,
      "grad_norm": 1.284245252609253,
      "learning_rate": 1.348785512712164e-05,
      "loss": 1.1749,
      "step": 1102
    },
    {
      "epoch": 2.0655430711610485,
      "grad_norm": 1.264487385749817,
      "learning_rate": 1.3439501393908854e-05,
      "loss": 1.2796,
      "step": 1103
    },
    {
      "epoch": 2.067415730337079,
      "grad_norm": 1.2512751817703247,
      "learning_rate": 1.3391202608226399e-05,
      "loss": 1.1429,
      "step": 1104
    },
    {
      "epoch": 2.0692883895131087,
      "grad_norm": 1.2902344465255737,
      "learning_rate": 1.3342958999640411e-05,
      "loss": 1.2003,
      "step": 1105
    },
    {
      "epoch": 2.0711610486891385,
      "grad_norm": 1.1574431657791138,
      "learning_rate": 1.329477079745477e-05,
      "loss": 1.1043,
      "step": 1106
    },
    {
      "epoch": 2.0730337078651684,
      "grad_norm": 1.372703194618225,
      "learning_rate": 1.3246638230709996e-05,
      "loss": 1.2152,
      "step": 1107
    },
    {
      "epoch": 2.0749063670411987,
      "grad_norm": 1.149890661239624,
      "learning_rate": 1.3198561528182183e-05,
      "loss": 1.3042,
      "step": 1108
    },
    {
      "epoch": 2.0767790262172285,
      "grad_norm": 1.2541890144348145,
      "learning_rate": 1.3150540918381895e-05,
      "loss": 1.2249,
      "step": 1109
    },
    {
      "epoch": 2.0786516853932584,
      "grad_norm": 1.2161667346954346,
      "learning_rate": 1.3102576629553079e-05,
      "loss": 1.1457,
      "step": 1110
    },
    {
      "epoch": 2.0805243445692883,
      "grad_norm": 1.2766058444976807,
      "learning_rate": 1.3054668889672011e-05,
      "loss": 1.1884,
      "step": 1111
    },
    {
      "epoch": 2.0823970037453186,
      "grad_norm": 1.2778350114822388,
      "learning_rate": 1.3006817926446162e-05,
      "loss": 1.2796,
      "step": 1112
    },
    {
      "epoch": 2.0842696629213484,
      "grad_norm": 1.3649605512619019,
      "learning_rate": 1.2959023967313144e-05,
      "loss": 1.1781,
      "step": 1113
    },
    {
      "epoch": 2.0861423220973783,
      "grad_norm": 1.2231889963150024,
      "learning_rate": 1.2911287239439627e-05,
      "loss": 1.1755,
      "step": 1114
    },
    {
      "epoch": 2.088014981273408,
      "grad_norm": 1.3887966871261597,
      "learning_rate": 1.2863607969720287e-05,
      "loss": 1.2793,
      "step": 1115
    },
    {
      "epoch": 2.0898876404494384,
      "grad_norm": 1.2971689701080322,
      "learning_rate": 1.2815986384776652e-05,
      "loss": 1.184,
      "step": 1116
    },
    {
      "epoch": 2.0917602996254683,
      "grad_norm": 1.2982310056686401,
      "learning_rate": 1.2768422710956093e-05,
      "loss": 1.1015,
      "step": 1117
    },
    {
      "epoch": 2.093632958801498,
      "grad_norm": 1.2637944221496582,
      "learning_rate": 1.272091717433073e-05,
      "loss": 1.3127,
      "step": 1118
    },
    {
      "epoch": 2.095505617977528,
      "grad_norm": 1.2735549211502075,
      "learning_rate": 1.2673470000696364e-05,
      "loss": 1.2126,
      "step": 1119
    },
    {
      "epoch": 2.097378277153558,
      "grad_norm": 1.247682809829712,
      "learning_rate": 1.2626081415571381e-05,
      "loss": 1.2044,
      "step": 1120
    },
    {
      "epoch": 2.099250936329588,
      "grad_norm": 1.3476893901824951,
      "learning_rate": 1.2578751644195692e-05,
      "loss": 1.183,
      "step": 1121
    },
    {
      "epoch": 2.101123595505618,
      "grad_norm": 1.2709344625473022,
      "learning_rate": 1.2531480911529663e-05,
      "loss": 1.1944,
      "step": 1122
    },
    {
      "epoch": 2.102996254681648,
      "grad_norm": 1.268537998199463,
      "learning_rate": 1.248426944225305e-05,
      "loss": 1.2164,
      "step": 1123
    },
    {
      "epoch": 2.1048689138576777,
      "grad_norm": 1.3575520515441895,
      "learning_rate": 1.2437117460763939e-05,
      "loss": 1.1332,
      "step": 1124
    },
    {
      "epoch": 2.106741573033708,
      "grad_norm": 1.1453040838241577,
      "learning_rate": 1.2390025191177666e-05,
      "loss": 0.9808,
      "step": 1125
    },
    {
      "epoch": 2.108614232209738,
      "grad_norm": 1.3444143533706665,
      "learning_rate": 1.2342992857325709e-05,
      "loss": 1.0565,
      "step": 1126
    },
    {
      "epoch": 2.1104868913857677,
      "grad_norm": 1.2619270086288452,
      "learning_rate": 1.229602068275474e-05,
      "loss": 1.1897,
      "step": 1127
    },
    {
      "epoch": 2.1123595505617976,
      "grad_norm": 1.286106824874878,
      "learning_rate": 1.2249108890725444e-05,
      "loss": 1.154,
      "step": 1128
    },
    {
      "epoch": 2.114232209737828,
      "grad_norm": 1.4327815771102905,
      "learning_rate": 1.220225770421152e-05,
      "loss": 1.3172,
      "step": 1129
    },
    {
      "epoch": 2.1161048689138577,
      "grad_norm": 1.4056845903396606,
      "learning_rate": 1.2155467345898602e-05,
      "loss": 1.2124,
      "step": 1130
    },
    {
      "epoch": 2.1179775280898876,
      "grad_norm": 1.185744047164917,
      "learning_rate": 1.2108738038183204e-05,
      "loss": 1.1447,
      "step": 1131
    },
    {
      "epoch": 2.1198501872659175,
      "grad_norm": 1.2154009342193604,
      "learning_rate": 1.206207000317168e-05,
      "loss": 1.1705,
      "step": 1132
    },
    {
      "epoch": 2.1217228464419478,
      "grad_norm": 1.2235300540924072,
      "learning_rate": 1.2015463462679136e-05,
      "loss": 1.169,
      "step": 1133
    },
    {
      "epoch": 2.1235955056179776,
      "grad_norm": 1.3425908088684082,
      "learning_rate": 1.1968918638228391e-05,
      "loss": 1.1554,
      "step": 1134
    },
    {
      "epoch": 2.1254681647940075,
      "grad_norm": 1.1775705814361572,
      "learning_rate": 1.1922435751048932e-05,
      "loss": 1.1703,
      "step": 1135
    },
    {
      "epoch": 2.1273408239700373,
      "grad_norm": 1.1436712741851807,
      "learning_rate": 1.1876015022075854e-05,
      "loss": 1.1394,
      "step": 1136
    },
    {
      "epoch": 2.1292134831460676,
      "grad_norm": 1.4331779479980469,
      "learning_rate": 1.1829656671948814e-05,
      "loss": 1.1019,
      "step": 1137
    },
    {
      "epoch": 2.1310861423220975,
      "grad_norm": 1.3529481887817383,
      "learning_rate": 1.1783360921010969e-05,
      "loss": 1.272,
      "step": 1138
    },
    {
      "epoch": 2.1329588014981273,
      "grad_norm": 1.3882193565368652,
      "learning_rate": 1.1737127989307955e-05,
      "loss": 1.03,
      "step": 1139
    },
    {
      "epoch": 2.134831460674157,
      "grad_norm": 1.338080883026123,
      "learning_rate": 1.1690958096586805e-05,
      "loss": 1.2023,
      "step": 1140
    },
    {
      "epoch": 2.1367041198501875,
      "grad_norm": 1.4138537645339966,
      "learning_rate": 1.1644851462294957e-05,
      "loss": 1.2144,
      "step": 1141
    },
    {
      "epoch": 2.1385767790262173,
      "grad_norm": 1.2293100357055664,
      "learning_rate": 1.1598808305579148e-05,
      "loss": 1.306,
      "step": 1142
    },
    {
      "epoch": 2.140449438202247,
      "grad_norm": 1.4996802806854248,
      "learning_rate": 1.1552828845284421e-05,
      "loss": 1.1602,
      "step": 1143
    },
    {
      "epoch": 2.142322097378277,
      "grad_norm": 1.3212709426879883,
      "learning_rate": 1.1506913299953051e-05,
      "loss": 1.2554,
      "step": 1144
    },
    {
      "epoch": 2.144194756554307,
      "grad_norm": 1.3073623180389404,
      "learning_rate": 1.1461061887823556e-05,
      "loss": 1.0689,
      "step": 1145
    },
    {
      "epoch": 2.146067415730337,
      "grad_norm": 1.369935154914856,
      "learning_rate": 1.1415274826829581e-05,
      "loss": 1.1948,
      "step": 1146
    },
    {
      "epoch": 2.147940074906367,
      "grad_norm": 1.316092848777771,
      "learning_rate": 1.1369552334598937e-05,
      "loss": 1.0818,
      "step": 1147
    },
    {
      "epoch": 2.149812734082397,
      "grad_norm": 1.3172111511230469,
      "learning_rate": 1.1323894628452523e-05,
      "loss": 1.2262,
      "step": 1148
    },
    {
      "epoch": 2.151685393258427,
      "grad_norm": 1.5083101987838745,
      "learning_rate": 1.1278301925403328e-05,
      "loss": 1.2648,
      "step": 1149
    },
    {
      "epoch": 2.153558052434457,
      "grad_norm": 1.4289727210998535,
      "learning_rate": 1.1232774442155358e-05,
      "loss": 1.2144,
      "step": 1150
    },
    {
      "epoch": 2.155430711610487,
      "grad_norm": 1.3315891027450562,
      "learning_rate": 1.1187312395102631e-05,
      "loss": 1.124,
      "step": 1151
    },
    {
      "epoch": 2.157303370786517,
      "grad_norm": 1.274387001991272,
      "learning_rate": 1.114191600032815e-05,
      "loss": 1.0928,
      "step": 1152
    },
    {
      "epoch": 2.1591760299625467,
      "grad_norm": 1.2862330675125122,
      "learning_rate": 1.1096585473602852e-05,
      "loss": 1.2106,
      "step": 1153
    },
    {
      "epoch": 2.161048689138577,
      "grad_norm": 1.367932677268982,
      "learning_rate": 1.1051321030384648e-05,
      "loss": 1.2676,
      "step": 1154
    },
    {
      "epoch": 2.162921348314607,
      "grad_norm": 1.2473714351654053,
      "learning_rate": 1.1006122885817283e-05,
      "loss": 1.2345,
      "step": 1155
    },
    {
      "epoch": 2.1647940074906367,
      "grad_norm": 1.272995114326477,
      "learning_rate": 1.0960991254729422e-05,
      "loss": 1.1713,
      "step": 1156
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 1.294480800628662,
      "learning_rate": 1.0915926351633598e-05,
      "loss": 1.1253,
      "step": 1157
    },
    {
      "epoch": 2.168539325842697,
      "grad_norm": 1.340439796447754,
      "learning_rate": 1.0870928390725163e-05,
      "loss": 1.1788,
      "step": 1158
    },
    {
      "epoch": 2.1704119850187267,
      "grad_norm": 1.4078646898269653,
      "learning_rate": 1.0825997585881293e-05,
      "loss": 1.2743,
      "step": 1159
    },
    {
      "epoch": 2.1722846441947565,
      "grad_norm": 1.3489099740982056,
      "learning_rate": 1.0781134150659969e-05,
      "loss": 1.2666,
      "step": 1160
    },
    {
      "epoch": 2.1741573033707864,
      "grad_norm": 1.138950228691101,
      "learning_rate": 1.073633829829896e-05,
      "loss": 1.2557,
      "step": 1161
    },
    {
      "epoch": 2.1760299625468167,
      "grad_norm": 1.287616491317749,
      "learning_rate": 1.0691610241714824e-05,
      "loss": 1.1901,
      "step": 1162
    },
    {
      "epoch": 2.1779026217228465,
      "grad_norm": 1.250333309173584,
      "learning_rate": 1.064695019350187e-05,
      "loss": 1.1786,
      "step": 1163
    },
    {
      "epoch": 2.1797752808988764,
      "grad_norm": 1.281226634979248,
      "learning_rate": 1.060235836593117e-05,
      "loss": 1.2745,
      "step": 1164
    },
    {
      "epoch": 2.1816479400749063,
      "grad_norm": 1.2470052242279053,
      "learning_rate": 1.0557834970949507e-05,
      "loss": 1.292,
      "step": 1165
    },
    {
      "epoch": 2.1835205992509366,
      "grad_norm": 1.5152944326400757,
      "learning_rate": 1.0513380220178454e-05,
      "loss": 1.2405,
      "step": 1166
    },
    {
      "epoch": 2.1853932584269664,
      "grad_norm": 1.3666659593582153,
      "learning_rate": 1.0468994324913281e-05,
      "loss": 1.2744,
      "step": 1167
    },
    {
      "epoch": 2.1872659176029963,
      "grad_norm": 1.2968624830245972,
      "learning_rate": 1.0424677496121992e-05,
      "loss": 1.1502,
      "step": 1168
    },
    {
      "epoch": 2.189138576779026,
      "grad_norm": 1.3350229263305664,
      "learning_rate": 1.0380429944444317e-05,
      "loss": 1.0194,
      "step": 1169
    },
    {
      "epoch": 2.191011235955056,
      "grad_norm": 1.4797290563583374,
      "learning_rate": 1.03362518801907e-05,
      "loss": 1.186,
      "step": 1170
    },
    {
      "epoch": 2.1928838951310863,
      "grad_norm": 1.4114340543746948,
      "learning_rate": 1.0292143513341331e-05,
      "loss": 1.204,
      "step": 1171
    },
    {
      "epoch": 2.194756554307116,
      "grad_norm": 1.4897429943084717,
      "learning_rate": 1.0248105053545106e-05,
      "loss": 1.2411,
      "step": 1172
    },
    {
      "epoch": 2.196629213483146,
      "grad_norm": 1.3955398797988892,
      "learning_rate": 1.020413671011865e-05,
      "loss": 1.1923,
      "step": 1173
    },
    {
      "epoch": 2.198501872659176,
      "grad_norm": 1.4104506969451904,
      "learning_rate": 1.0160238692045332e-05,
      "loss": 1.1057,
      "step": 1174
    },
    {
      "epoch": 2.200374531835206,
      "grad_norm": 1.4728853702545166,
      "learning_rate": 1.0116411207974253e-05,
      "loss": 1.0688,
      "step": 1175
    },
    {
      "epoch": 2.202247191011236,
      "grad_norm": 1.3652805089950562,
      "learning_rate": 1.0072654466219264e-05,
      "loss": 1.2062,
      "step": 1176
    },
    {
      "epoch": 2.204119850187266,
      "grad_norm": 1.2841523885726929,
      "learning_rate": 1.0028968674757983e-05,
      "loss": 1.0914,
      "step": 1177
    },
    {
      "epoch": 2.2059925093632957,
      "grad_norm": 1.178044319152832,
      "learning_rate": 9.985354041230777e-06,
      "loss": 1.1556,
      "step": 1178
    },
    {
      "epoch": 2.207865168539326,
      "grad_norm": 1.3014225959777832,
      "learning_rate": 9.941810772939836e-06,
      "loss": 1.1564,
      "step": 1179
    },
    {
      "epoch": 2.209737827715356,
      "grad_norm": 1.2995516061782837,
      "learning_rate": 9.898339076848109e-06,
      "loss": 1.0479,
      "step": 1180
    },
    {
      "epoch": 2.2116104868913857,
      "grad_norm": 1.2817838191986084,
      "learning_rate": 9.854939159578381e-06,
      "loss": 1.2394,
      "step": 1181
    },
    {
      "epoch": 2.2134831460674156,
      "grad_norm": 1.2716944217681885,
      "learning_rate": 9.811611227412254e-06,
      "loss": 1.2167,
      "step": 1182
    },
    {
      "epoch": 2.215355805243446,
      "grad_norm": 1.2408814430236816,
      "learning_rate": 9.768355486289187e-06,
      "loss": 1.2,
      "step": 1183
    },
    {
      "epoch": 2.2172284644194757,
      "grad_norm": 1.38724946975708,
      "learning_rate": 9.725172141805545e-06,
      "loss": 1.1478,
      "step": 1184
    },
    {
      "epoch": 2.2191011235955056,
      "grad_norm": 1.275232195854187,
      "learning_rate": 9.682061399213525e-06,
      "loss": 1.126,
      "step": 1185
    },
    {
      "epoch": 2.2209737827715355,
      "grad_norm": 1.4164496660232544,
      "learning_rate": 9.639023463420282e-06,
      "loss": 1.2048,
      "step": 1186
    },
    {
      "epoch": 2.2228464419475653,
      "grad_norm": 1.3758271932601929,
      "learning_rate": 9.59605853898693e-06,
      "loss": 1.2047,
      "step": 1187
    },
    {
      "epoch": 2.2247191011235956,
      "grad_norm": 1.4173636436462402,
      "learning_rate": 9.553166830127532e-06,
      "loss": 1.1043,
      "step": 1188
    },
    {
      "epoch": 2.2265917602996255,
      "grad_norm": 1.3797527551651,
      "learning_rate": 9.510348540708167e-06,
      "loss": 1.198,
      "step": 1189
    },
    {
      "epoch": 2.2284644194756553,
      "grad_norm": 1.2602479457855225,
      "learning_rate": 9.467603874245936e-06,
      "loss": 1.2067,
      "step": 1190
    },
    {
      "epoch": 2.2303370786516856,
      "grad_norm": 1.3475533723831177,
      "learning_rate": 9.424933033908012e-06,
      "loss": 1.1125,
      "step": 1191
    },
    {
      "epoch": 2.2322097378277155,
      "grad_norm": 1.1098897457122803,
      "learning_rate": 9.38233622251069e-06,
      "loss": 1.163,
      "step": 1192
    },
    {
      "epoch": 2.2340823970037453,
      "grad_norm": 1.248679518699646,
      "learning_rate": 9.339813642518374e-06,
      "loss": 1.1593,
      "step": 1193
    },
    {
      "epoch": 2.235955056179775,
      "grad_norm": 1.3075309991836548,
      "learning_rate": 9.297365496042663e-06,
      "loss": 1.3141,
      "step": 1194
    },
    {
      "epoch": 2.237827715355805,
      "grad_norm": 1.232409119606018,
      "learning_rate": 9.254991984841338e-06,
      "loss": 1.1904,
      "step": 1195
    },
    {
      "epoch": 2.2397003745318353,
      "grad_norm": 1.4099361896514893,
      "learning_rate": 9.21269331031748e-06,
      "loss": 1.1702,
      "step": 1196
    },
    {
      "epoch": 2.241573033707865,
      "grad_norm": 1.4132115840911865,
      "learning_rate": 9.170469673518443e-06,
      "loss": 1.1147,
      "step": 1197
    },
    {
      "epoch": 2.243445692883895,
      "grad_norm": 1.5341535806655884,
      "learning_rate": 9.128321275134924e-06,
      "loss": 1.09,
      "step": 1198
    },
    {
      "epoch": 2.245318352059925,
      "grad_norm": 1.4213135242462158,
      "learning_rate": 9.086248315500004e-06,
      "loss": 1.1629,
      "step": 1199
    },
    {
      "epoch": 2.247191011235955,
      "grad_norm": 1.3460512161254883,
      "learning_rate": 9.04425099458823e-06,
      "loss": 1.1688,
      "step": 1200
    },
    {
      "epoch": 2.249063670411985,
      "grad_norm": 1.2383480072021484,
      "learning_rate": 9.002329512014599e-06,
      "loss": 1.0832,
      "step": 1201
    },
    {
      "epoch": 2.250936329588015,
      "grad_norm": 1.3197895288467407,
      "learning_rate": 8.960484067033664e-06,
      "loss": 1.0961,
      "step": 1202
    },
    {
      "epoch": 2.252808988764045,
      "grad_norm": 1.2776451110839844,
      "learning_rate": 8.918714858538565e-06,
      "loss": 1.1367,
      "step": 1203
    },
    {
      "epoch": 2.254681647940075,
      "grad_norm": 1.4727942943572998,
      "learning_rate": 8.877022085060082e-06,
      "loss": 1.0898,
      "step": 1204
    },
    {
      "epoch": 2.256554307116105,
      "grad_norm": 1.3341940641403198,
      "learning_rate": 8.835405944765705e-06,
      "loss": 1.137,
      "step": 1205
    },
    {
      "epoch": 2.258426966292135,
      "grad_norm": 1.4485108852386475,
      "learning_rate": 8.793866635458672e-06,
      "loss": 1.2208,
      "step": 1206
    },
    {
      "epoch": 2.2602996254681647,
      "grad_norm": 1.3231384754180908,
      "learning_rate": 8.752404354577052e-06,
      "loss": 1.0791,
      "step": 1207
    },
    {
      "epoch": 2.262172284644195,
      "grad_norm": 1.4081335067749023,
      "learning_rate": 8.711019299192776e-06,
      "loss": 1.1672,
      "step": 1208
    },
    {
      "epoch": 2.264044943820225,
      "grad_norm": 1.4386849403381348,
      "learning_rate": 8.669711666010754e-06,
      "loss": 1.231,
      "step": 1209
    },
    {
      "epoch": 2.2659176029962547,
      "grad_norm": 1.4367433786392212,
      "learning_rate": 8.628481651367876e-06,
      "loss": 1.3298,
      "step": 1210
    },
    {
      "epoch": 2.2677902621722845,
      "grad_norm": 1.3779634237289429,
      "learning_rate": 8.587329451232107e-06,
      "loss": 1.1622,
      "step": 1211
    },
    {
      "epoch": 2.2696629213483144,
      "grad_norm": 1.4207682609558105,
      "learning_rate": 8.546255261201571e-06,
      "loss": 1.2214,
      "step": 1212
    },
    {
      "epoch": 2.2715355805243447,
      "grad_norm": 1.3738141059875488,
      "learning_rate": 8.505259276503591e-06,
      "loss": 1.1298,
      "step": 1213
    },
    {
      "epoch": 2.2734082397003745,
      "grad_norm": 1.3645710945129395,
      "learning_rate": 8.46434169199381e-06,
      "loss": 1.1817,
      "step": 1214
    },
    {
      "epoch": 2.2752808988764044,
      "grad_norm": 1.5819584131240845,
      "learning_rate": 8.423502702155184e-06,
      "loss": 1.101,
      "step": 1215
    },
    {
      "epoch": 2.2771535580524347,
      "grad_norm": 1.3659818172454834,
      "learning_rate": 8.382742501097126e-06,
      "loss": 1.1119,
      "step": 1216
    },
    {
      "epoch": 2.2790262172284645,
      "grad_norm": 1.4189666509628296,
      "learning_rate": 8.342061282554586e-06,
      "loss": 1.2249,
      "step": 1217
    },
    {
      "epoch": 2.2808988764044944,
      "grad_norm": 1.5531140565872192,
      "learning_rate": 8.301459239887074e-06,
      "loss": 1.1569,
      "step": 1218
    },
    {
      "epoch": 2.2827715355805243,
      "grad_norm": 1.5030341148376465,
      "learning_rate": 8.260936566077793e-06,
      "loss": 1.0503,
      "step": 1219
    },
    {
      "epoch": 2.284644194756554,
      "grad_norm": 1.3740828037261963,
      "learning_rate": 8.22049345373269e-06,
      "loss": 1.1665,
      "step": 1220
    },
    {
      "epoch": 2.2865168539325844,
      "grad_norm": 1.4358316659927368,
      "learning_rate": 8.18013009507955e-06,
      "loss": 1.0983,
      "step": 1221
    },
    {
      "epoch": 2.2883895131086143,
      "grad_norm": 1.2595471143722534,
      "learning_rate": 8.139846681967114e-06,
      "loss": 1.1279,
      "step": 1222
    },
    {
      "epoch": 2.290262172284644,
      "grad_norm": 1.375349998474121,
      "learning_rate": 8.099643405864108e-06,
      "loss": 1.115,
      "step": 1223
    },
    {
      "epoch": 2.292134831460674,
      "grad_norm": 1.3704288005828857,
      "learning_rate": 8.059520457858385e-06,
      "loss": 1.2261,
      "step": 1224
    },
    {
      "epoch": 2.2940074906367043,
      "grad_norm": 1.297739028930664,
      "learning_rate": 8.019478028655958e-06,
      "loss": 1.1155,
      "step": 1225
    },
    {
      "epoch": 2.295880149812734,
      "grad_norm": 1.2996907234191895,
      "learning_rate": 7.979516308580182e-06,
      "loss": 1.1386,
      "step": 1226
    },
    {
      "epoch": 2.297752808988764,
      "grad_norm": 1.4749701023101807,
      "learning_rate": 7.939635487570765e-06,
      "loss": 1.2736,
      "step": 1227
    },
    {
      "epoch": 2.299625468164794,
      "grad_norm": 1.3801194429397583,
      "learning_rate": 7.899835755182908e-06,
      "loss": 1.2278,
      "step": 1228
    },
    {
      "epoch": 2.301498127340824,
      "grad_norm": 1.4481358528137207,
      "learning_rate": 7.860117300586383e-06,
      "loss": 1.1967,
      "step": 1229
    },
    {
      "epoch": 2.303370786516854,
      "grad_norm": 1.4693152904510498,
      "learning_rate": 7.820480312564674e-06,
      "loss": 1.2607,
      "step": 1230
    },
    {
      "epoch": 2.305243445692884,
      "grad_norm": 1.34466552734375,
      "learning_rate": 7.78092497951402e-06,
      "loss": 1.0917,
      "step": 1231
    },
    {
      "epoch": 2.3071161048689137,
      "grad_norm": 1.4358737468719482,
      "learning_rate": 7.741451489442567e-06,
      "loss": 1.1409,
      "step": 1232
    },
    {
      "epoch": 2.308988764044944,
      "grad_norm": 1.2446876764297485,
      "learning_rate": 7.702060029969455e-06,
      "loss": 1.1801,
      "step": 1233
    },
    {
      "epoch": 2.310861423220974,
      "grad_norm": 1.4288266897201538,
      "learning_rate": 7.662750788323922e-06,
      "loss": 1.3429,
      "step": 1234
    },
    {
      "epoch": 2.3127340823970037,
      "grad_norm": 1.361968755722046,
      "learning_rate": 7.6235239513444325e-06,
      "loss": 1.2052,
      "step": 1235
    },
    {
      "epoch": 2.3146067415730336,
      "grad_norm": 1.4119443893432617,
      "learning_rate": 7.584379705477771e-06,
      "loss": 1.2036,
      "step": 1236
    },
    {
      "epoch": 2.3164794007490634,
      "grad_norm": 1.4561033248901367,
      "learning_rate": 7.545318236778165e-06,
      "loss": 1.1134,
      "step": 1237
    },
    {
      "epoch": 2.3183520599250937,
      "grad_norm": 1.4770711660385132,
      "learning_rate": 7.506339730906392e-06,
      "loss": 1.1095,
      "step": 1238
    },
    {
      "epoch": 2.3202247191011236,
      "grad_norm": 1.2506223917007446,
      "learning_rate": 7.467444373128923e-06,
      "loss": 1.2407,
      "step": 1239
    },
    {
      "epoch": 2.3220973782771535,
      "grad_norm": 1.3332582712173462,
      "learning_rate": 7.428632348317005e-06,
      "loss": 1.2454,
      "step": 1240
    },
    {
      "epoch": 2.3239700374531838,
      "grad_norm": 1.444187045097351,
      "learning_rate": 7.389903840945805e-06,
      "loss": 1.1651,
      "step": 1241
    },
    {
      "epoch": 2.3258426966292136,
      "grad_norm": 1.1886423826217651,
      "learning_rate": 7.351259035093528e-06,
      "loss": 1.0436,
      "step": 1242
    },
    {
      "epoch": 2.3277153558052435,
      "grad_norm": 1.4334264993667603,
      "learning_rate": 7.312698114440542e-06,
      "loss": 1.0874,
      "step": 1243
    },
    {
      "epoch": 2.3295880149812733,
      "grad_norm": 1.3695956468582153,
      "learning_rate": 7.274221262268502e-06,
      "loss": 1.1579,
      "step": 1244
    },
    {
      "epoch": 2.331460674157303,
      "grad_norm": 1.5115635395050049,
      "learning_rate": 7.23582866145949e-06,
      "loss": 1.2988,
      "step": 1245
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 1.4290004968643188,
      "learning_rate": 7.197520494495125e-06,
      "loss": 1.1088,
      "step": 1246
    },
    {
      "epoch": 2.3352059925093633,
      "grad_norm": 1.3234186172485352,
      "learning_rate": 7.159296943455729e-06,
      "loss": 1.117,
      "step": 1247
    },
    {
      "epoch": 2.337078651685393,
      "grad_norm": 1.332649827003479,
      "learning_rate": 7.121158190019425e-06,
      "loss": 1.1604,
      "step": 1248
    },
    {
      "epoch": 2.338951310861423,
      "grad_norm": 1.4460550546646118,
      "learning_rate": 7.0831044154612895e-06,
      "loss": 1.1032,
      "step": 1249
    },
    {
      "epoch": 2.3408239700374533,
      "grad_norm": 1.3918583393096924,
      "learning_rate": 7.045135800652497e-06,
      "loss": 1.0619,
      "step": 1250
    },
    {
      "epoch": 2.342696629213483,
      "grad_norm": 1.2673438787460327,
      "learning_rate": 7.007252526059446e-06,
      "loss": 1.1259,
      "step": 1251
    },
    {
      "epoch": 2.344569288389513,
      "grad_norm": 1.4738327264785767,
      "learning_rate": 6.969454771742925e-06,
      "loss": 1.0904,
      "step": 1252
    },
    {
      "epoch": 2.346441947565543,
      "grad_norm": 1.3358837366104126,
      "learning_rate": 6.931742717357234e-06,
      "loss": 1.1558,
      "step": 1253
    },
    {
      "epoch": 2.348314606741573,
      "grad_norm": 1.4166885614395142,
      "learning_rate": 6.894116542149317e-06,
      "loss": 1.0641,
      "step": 1254
    },
    {
      "epoch": 2.350187265917603,
      "grad_norm": 1.3302199840545654,
      "learning_rate": 6.856576424957956e-06,
      "loss": 1.0864,
      "step": 1255
    },
    {
      "epoch": 2.352059925093633,
      "grad_norm": 1.4044078588485718,
      "learning_rate": 6.819122544212894e-06,
      "loss": 1.131,
      "step": 1256
    },
    {
      "epoch": 2.353932584269663,
      "grad_norm": 1.3279703855514526,
      "learning_rate": 6.781755077933982e-06,
      "loss": 1.1284,
      "step": 1257
    },
    {
      "epoch": 2.355805243445693,
      "grad_norm": 1.2933404445648193,
      "learning_rate": 6.744474203730336e-06,
      "loss": 1.1074,
      "step": 1258
    },
    {
      "epoch": 2.357677902621723,
      "grad_norm": 1.461125135421753,
      "learning_rate": 6.7072800987994955e-06,
      "loss": 1.1774,
      "step": 1259
    },
    {
      "epoch": 2.359550561797753,
      "grad_norm": 1.2584267854690552,
      "learning_rate": 6.6701729399266e-06,
      "loss": 1.1395,
      "step": 1260
    },
    {
      "epoch": 2.3614232209737827,
      "grad_norm": 1.301411509513855,
      "learning_rate": 6.633152903483511e-06,
      "loss": 1.1654,
      "step": 1261
    },
    {
      "epoch": 2.3632958801498125,
      "grad_norm": 1.457309365272522,
      "learning_rate": 6.596220165428002e-06,
      "loss": 1.1213,
      "step": 1262
    },
    {
      "epoch": 2.365168539325843,
      "grad_norm": 1.3429988622665405,
      "learning_rate": 6.559374901302911e-06,
      "loss": 1.1196,
      "step": 1263
    },
    {
      "epoch": 2.3670411985018727,
      "grad_norm": 1.319501280784607,
      "learning_rate": 6.522617286235308e-06,
      "loss": 1.2234,
      "step": 1264
    },
    {
      "epoch": 2.3689138576779025,
      "grad_norm": 1.2538090944290161,
      "learning_rate": 6.485947494935668e-06,
      "loss": 1.1735,
      "step": 1265
    },
    {
      "epoch": 2.370786516853933,
      "grad_norm": 1.3466823101043701,
      "learning_rate": 6.44936570169703e-06,
      "loss": 1.1982,
      "step": 1266
    },
    {
      "epoch": 2.3726591760299627,
      "grad_norm": 1.4438523054122925,
      "learning_rate": 6.412872080394181e-06,
      "loss": 1.1725,
      "step": 1267
    },
    {
      "epoch": 2.3745318352059925,
      "grad_norm": 1.3269751071929932,
      "learning_rate": 6.376466804482809e-06,
      "loss": 1.0498,
      "step": 1268
    },
    {
      "epoch": 2.3764044943820224,
      "grad_norm": 1.457541823387146,
      "learning_rate": 6.340150046998719e-06,
      "loss": 1.1159,
      "step": 1269
    },
    {
      "epoch": 2.3782771535580522,
      "grad_norm": 1.248691201210022,
      "learning_rate": 6.303921980556962e-06,
      "loss": 1.2499,
      "step": 1270
    },
    {
      "epoch": 2.3801498127340825,
      "grad_norm": 1.2958735227584839,
      "learning_rate": 6.267782777351044e-06,
      "loss": 1.264,
      "step": 1271
    },
    {
      "epoch": 2.3820224719101124,
      "grad_norm": 1.3122471570968628,
      "learning_rate": 6.231732609152099e-06,
      "loss": 1.2356,
      "step": 1272
    },
    {
      "epoch": 2.3838951310861423,
      "grad_norm": 1.4116815328598022,
      "learning_rate": 6.195771647308074e-06,
      "loss": 1.197,
      "step": 1273
    },
    {
      "epoch": 2.385767790262172,
      "grad_norm": 1.3056457042694092,
      "learning_rate": 6.159900062742918e-06,
      "loss": 1.0953,
      "step": 1274
    },
    {
      "epoch": 2.3876404494382024,
      "grad_norm": 1.3271136283874512,
      "learning_rate": 6.124118025955758e-06,
      "loss": 1.2228,
      "step": 1275
    },
    {
      "epoch": 2.3895131086142323,
      "grad_norm": 1.4278227090835571,
      "learning_rate": 6.088425707020101e-06,
      "loss": 1.1364,
      "step": 1276
    },
    {
      "epoch": 2.391385767790262,
      "grad_norm": 1.383135437965393,
      "learning_rate": 6.052823275583034e-06,
      "loss": 1.1108,
      "step": 1277
    },
    {
      "epoch": 2.393258426966292,
      "grad_norm": 1.3270840644836426,
      "learning_rate": 6.017310900864381e-06,
      "loss": 1.1304,
      "step": 1278
    },
    {
      "epoch": 2.3951310861423223,
      "grad_norm": 1.4631673097610474,
      "learning_rate": 5.9818887516559455e-06,
      "loss": 1.1288,
      "step": 1279
    },
    {
      "epoch": 2.397003745318352,
      "grad_norm": 1.4202128648757935,
      "learning_rate": 5.946556996320665e-06,
      "loss": 1.2086,
      "step": 1280
    },
    {
      "epoch": 2.398876404494382,
      "grad_norm": 1.2664690017700195,
      "learning_rate": 5.911315802791839e-06,
      "loss": 1.1694,
      "step": 1281
    },
    {
      "epoch": 2.400749063670412,
      "grad_norm": 1.4178320169448853,
      "learning_rate": 5.876165338572337e-06,
      "loss": 1.1471,
      "step": 1282
    },
    {
      "epoch": 2.402621722846442,
      "grad_norm": 1.4362915754318237,
      "learning_rate": 5.841105770733779e-06,
      "loss": 1.1887,
      "step": 1283
    },
    {
      "epoch": 2.404494382022472,
      "grad_norm": 1.3069075345993042,
      "learning_rate": 5.806137265915732e-06,
      "loss": 1.1693,
      "step": 1284
    },
    {
      "epoch": 2.406367041198502,
      "grad_norm": 1.2778546810150146,
      "learning_rate": 5.771259990324954e-06,
      "loss": 1.1389,
      "step": 1285
    },
    {
      "epoch": 2.4082397003745317,
      "grad_norm": 1.284404993057251,
      "learning_rate": 5.736474109734596e-06,
      "loss": 1.1416,
      "step": 1286
    },
    {
      "epoch": 2.4101123595505616,
      "grad_norm": 1.4045871496200562,
      "learning_rate": 5.701779789483391e-06,
      "loss": 1.2631,
      "step": 1287
    },
    {
      "epoch": 2.411985018726592,
      "grad_norm": 1.2124009132385254,
      "learning_rate": 5.667177194474882e-06,
      "loss": 1.1105,
      "step": 1288
    },
    {
      "epoch": 2.4138576779026217,
      "grad_norm": 1.4582065343856812,
      "learning_rate": 5.632666489176639e-06,
      "loss": 1.1374,
      "step": 1289
    },
    {
      "epoch": 2.4157303370786516,
      "grad_norm": 1.3592170476913452,
      "learning_rate": 5.598247837619486e-06,
      "loss": 1.2386,
      "step": 1290
    },
    {
      "epoch": 2.417602996254682,
      "grad_norm": 1.3617124557495117,
      "learning_rate": 5.563921403396702e-06,
      "loss": 1.1018,
      "step": 1291
    },
    {
      "epoch": 2.4194756554307117,
      "grad_norm": 1.4742591381072998,
      "learning_rate": 5.529687349663254e-06,
      "loss": 1.296,
      "step": 1292
    },
    {
      "epoch": 2.4213483146067416,
      "grad_norm": 1.4144753217697144,
      "learning_rate": 5.495545839135021e-06,
      "loss": 1.2065,
      "step": 1293
    },
    {
      "epoch": 2.4232209737827715,
      "grad_norm": 1.4801228046417236,
      "learning_rate": 5.4614970340880255e-06,
      "loss": 1.1818,
      "step": 1294
    },
    {
      "epoch": 2.4250936329588013,
      "grad_norm": 1.497678279876709,
      "learning_rate": 5.427541096357647e-06,
      "loss": 1.1802,
      "step": 1295
    },
    {
      "epoch": 2.4269662921348316,
      "grad_norm": 1.4239860773086548,
      "learning_rate": 5.3936781873378724e-06,
      "loss": 1.1261,
      "step": 1296
    },
    {
      "epoch": 2.4288389513108615,
      "grad_norm": 1.310573697090149,
      "learning_rate": 5.359908467980518e-06,
      "loss": 1.1564,
      "step": 1297
    },
    {
      "epoch": 2.4307116104868913,
      "grad_norm": 1.4093433618545532,
      "learning_rate": 5.326232098794451e-06,
      "loss": 1.1101,
      "step": 1298
    },
    {
      "epoch": 2.432584269662921,
      "grad_norm": 1.3139090538024902,
      "learning_rate": 5.292649239844872e-06,
      "loss": 1.1729,
      "step": 1299
    },
    {
      "epoch": 2.4344569288389515,
      "grad_norm": 1.3408139944076538,
      "learning_rate": 5.259160050752493e-06,
      "loss": 1.1079,
      "step": 1300
    },
    {
      "epoch": 2.4363295880149813,
      "grad_norm": 1.435401439666748,
      "learning_rate": 5.225764690692828e-06,
      "loss": 1.1059,
      "step": 1301
    },
    {
      "epoch": 2.438202247191011,
      "grad_norm": 1.262461543083191,
      "learning_rate": 5.192463318395408e-06,
      "loss": 1.0234,
      "step": 1302
    },
    {
      "epoch": 2.440074906367041,
      "grad_norm": 1.418670415878296,
      "learning_rate": 5.159256092143039e-06,
      "loss": 1.1112,
      "step": 1303
    },
    {
      "epoch": 2.4419475655430714,
      "grad_norm": 1.407974123954773,
      "learning_rate": 5.12614316977105e-06,
      "loss": 1.0686,
      "step": 1304
    },
    {
      "epoch": 2.443820224719101,
      "grad_norm": 1.3590190410614014,
      "learning_rate": 5.093124708666536e-06,
      "loss": 1.1015,
      "step": 1305
    },
    {
      "epoch": 2.445692883895131,
      "grad_norm": 1.4762887954711914,
      "learning_rate": 5.060200865767606e-06,
      "loss": 1.1154,
      "step": 1306
    },
    {
      "epoch": 2.447565543071161,
      "grad_norm": 1.3687177896499634,
      "learning_rate": 5.027371797562669e-06,
      "loss": 1.2162,
      "step": 1307
    },
    {
      "epoch": 2.449438202247191,
      "grad_norm": 1.3904685974121094,
      "learning_rate": 4.9946376600896434e-06,
      "loss": 1.2447,
      "step": 1308
    },
    {
      "epoch": 2.451310861423221,
      "grad_norm": 1.4196600914001465,
      "learning_rate": 4.961998608935245e-06,
      "loss": 1.17,
      "step": 1309
    },
    {
      "epoch": 2.453183520599251,
      "grad_norm": 1.3728493452072144,
      "learning_rate": 4.929454799234243e-06,
      "loss": 1.1489,
      "step": 1310
    },
    {
      "epoch": 2.455056179775281,
      "grad_norm": 1.4155269861221313,
      "learning_rate": 4.897006385668707e-06,
      "loss": 1.057,
      "step": 1311
    },
    {
      "epoch": 2.4569288389513106,
      "grad_norm": 1.4880790710449219,
      "learning_rate": 4.8646535224673165e-06,
      "loss": 1.1101,
      "step": 1312
    },
    {
      "epoch": 2.458801498127341,
      "grad_norm": 1.4365586042404175,
      "learning_rate": 4.832396363404554e-06,
      "loss": 1.1853,
      "step": 1313
    },
    {
      "epoch": 2.460674157303371,
      "grad_norm": 1.3779305219650269,
      "learning_rate": 4.800235061800048e-06,
      "loss": 1.204,
      "step": 1314
    },
    {
      "epoch": 2.4625468164794007,
      "grad_norm": 1.316836953163147,
      "learning_rate": 4.768169770517791e-06,
      "loss": 1.0856,
      "step": 1315
    },
    {
      "epoch": 2.464419475655431,
      "grad_norm": 1.3113210201263428,
      "learning_rate": 4.736200641965457e-06,
      "loss": 1.0498,
      "step": 1316
    },
    {
      "epoch": 2.466292134831461,
      "grad_norm": 1.2761805057525635,
      "learning_rate": 4.704327828093641e-06,
      "loss": 1.132,
      "step": 1317
    },
    {
      "epoch": 2.4681647940074907,
      "grad_norm": 1.4340547323226929,
      "learning_rate": 4.672551480395149e-06,
      "loss": 1.266,
      "step": 1318
    },
    {
      "epoch": 2.4700374531835205,
      "grad_norm": 1.265687108039856,
      "learning_rate": 4.640871749904274e-06,
      "loss": 1.1689,
      "step": 1319
    },
    {
      "epoch": 2.4719101123595504,
      "grad_norm": 1.4380662441253662,
      "learning_rate": 4.609288787196109e-06,
      "loss": 1.1549,
      "step": 1320
    },
    {
      "epoch": 2.4737827715355807,
      "grad_norm": 1.322585105895996,
      "learning_rate": 4.577802742385778e-06,
      "loss": 1.1291,
      "step": 1321
    },
    {
      "epoch": 2.4756554307116105,
      "grad_norm": 1.441834807395935,
      "learning_rate": 4.546413765127769e-06,
      "loss": 1.2367,
      "step": 1322
    },
    {
      "epoch": 2.4775280898876404,
      "grad_norm": 1.4416248798370361,
      "learning_rate": 4.515122004615177e-06,
      "loss": 1.2495,
      "step": 1323
    },
    {
      "epoch": 2.4794007490636703,
      "grad_norm": 1.3495982885360718,
      "learning_rate": 4.4839276095790555e-06,
      "loss": 1.1333,
      "step": 1324
    },
    {
      "epoch": 2.4812734082397006,
      "grad_norm": 1.441569209098816,
      "learning_rate": 4.4528307282876605e-06,
      "loss": 1.2072,
      "step": 1325
    },
    {
      "epoch": 2.4831460674157304,
      "grad_norm": 1.3298606872558594,
      "learning_rate": 4.421831508545754e-06,
      "loss": 1.1996,
      "step": 1326
    },
    {
      "epoch": 2.4850187265917603,
      "grad_norm": 1.2855002880096436,
      "learning_rate": 4.390930097693921e-06,
      "loss": 1.1348,
      "step": 1327
    },
    {
      "epoch": 2.48689138576779,
      "grad_norm": 1.418136715888977,
      "learning_rate": 4.3601266426078426e-06,
      "loss": 1.1654,
      "step": 1328
    },
    {
      "epoch": 2.48876404494382,
      "grad_norm": 1.4662803411483765,
      "learning_rate": 4.329421289697636e-06,
      "loss": 1.1861,
      "step": 1329
    },
    {
      "epoch": 2.4906367041198503,
      "grad_norm": 1.4754153490066528,
      "learning_rate": 4.2988141849071105e-06,
      "loss": 1.0776,
      "step": 1330
    },
    {
      "epoch": 2.49250936329588,
      "grad_norm": 1.29041588306427,
      "learning_rate": 4.268305473713108e-06,
      "loss": 1.1662,
      "step": 1331
    },
    {
      "epoch": 2.49438202247191,
      "grad_norm": 1.421618103981018,
      "learning_rate": 4.2378953011248005e-06,
      "loss": 1.3158,
      "step": 1332
    },
    {
      "epoch": 2.4962546816479403,
      "grad_norm": 1.3983651399612427,
      "learning_rate": 4.207583811683005e-06,
      "loss": 1.0852,
      "step": 1333
    },
    {
      "epoch": 2.49812734082397,
      "grad_norm": 1.4319911003112793,
      "learning_rate": 4.1773711494594886e-06,
      "loss": 1.1403,
      "step": 1334
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.417673945426941,
      "learning_rate": 4.147257458056289e-06,
      "loss": 1.1084,
      "step": 1335
    },
    {
      "epoch": 2.50187265917603,
      "grad_norm": 1.4327013492584229,
      "learning_rate": 4.1172428806050296e-06,
      "loss": 1.0734,
      "step": 1336
    },
    {
      "epoch": 2.5037453183520597,
      "grad_norm": 1.4781700372695923,
      "learning_rate": 4.087327559766254e-06,
      "loss": 1.1748,
      "step": 1337
    },
    {
      "epoch": 2.50561797752809,
      "grad_norm": 1.584312081336975,
      "learning_rate": 4.057511637728722e-06,
      "loss": 1.1778,
      "step": 1338
    },
    {
      "epoch": 2.50749063670412,
      "grad_norm": 1.328787088394165,
      "learning_rate": 4.02779525620875e-06,
      "loss": 1.1483,
      "step": 1339
    },
    {
      "epoch": 2.5093632958801497,
      "grad_norm": 1.5926991701126099,
      "learning_rate": 3.99817855644953e-06,
      "loss": 1.2023,
      "step": 1340
    },
    {
      "epoch": 2.51123595505618,
      "grad_norm": 1.4027330875396729,
      "learning_rate": 3.968661679220468e-06,
      "loss": 1.2262,
      "step": 1341
    },
    {
      "epoch": 2.51310861423221,
      "grad_norm": 1.3848419189453125,
      "learning_rate": 3.939244764816522e-06,
      "loss": 1.1547,
      "step": 1342
    },
    {
      "epoch": 2.5149812734082397,
      "grad_norm": 1.5672087669372559,
      "learning_rate": 3.909927953057488e-06,
      "loss": 1.2402,
      "step": 1343
    },
    {
      "epoch": 2.5168539325842696,
      "grad_norm": 1.411299228668213,
      "learning_rate": 3.880711383287399e-06,
      "loss": 1.0478,
      "step": 1344
    },
    {
      "epoch": 2.5187265917602994,
      "grad_norm": 1.5872033834457397,
      "learning_rate": 3.851595194373825e-06,
      "loss": 1.1223,
      "step": 1345
    },
    {
      "epoch": 2.5205992509363297,
      "grad_norm": 1.58585786819458,
      "learning_rate": 3.822579524707226e-06,
      "loss": 1.3277,
      "step": 1346
    },
    {
      "epoch": 2.5224719101123596,
      "grad_norm": 1.4192640781402588,
      "learning_rate": 3.7936645122002816e-06,
      "loss": 1.2271,
      "step": 1347
    },
    {
      "epoch": 2.5243445692883895,
      "grad_norm": 1.3663551807403564,
      "learning_rate": 3.764850294287253e-06,
      "loss": 1.1849,
      "step": 1348
    },
    {
      "epoch": 2.5262172284644193,
      "grad_norm": 1.3138179779052734,
      "learning_rate": 3.7361370079233017e-06,
      "loss": 1.0208,
      "step": 1349
    },
    {
      "epoch": 2.5280898876404496,
      "grad_norm": 1.4677847623825073,
      "learning_rate": 3.707524789583891e-06,
      "loss": 1.1783,
      "step": 1350
    },
    {
      "epoch": 2.5299625468164795,
      "grad_norm": 1.415345311164856,
      "learning_rate": 3.67901377526407e-06,
      "loss": 1.1504,
      "step": 1351
    },
    {
      "epoch": 2.5318352059925093,
      "grad_norm": 1.5014920234680176,
      "learning_rate": 3.650604100477889e-06,
      "loss": 1.1316,
      "step": 1352
    },
    {
      "epoch": 2.533707865168539,
      "grad_norm": 1.355684518814087,
      "learning_rate": 3.6222959002576886e-06,
      "loss": 1.1769,
      "step": 1353
    },
    {
      "epoch": 2.535580524344569,
      "grad_norm": 1.2567740678787231,
      "learning_rate": 3.5940893091535392e-06,
      "loss": 1.1462,
      "step": 1354
    },
    {
      "epoch": 2.5374531835205993,
      "grad_norm": 1.3837007284164429,
      "learning_rate": 3.565984461232538e-06,
      "loss": 1.1735,
      "step": 1355
    },
    {
      "epoch": 2.539325842696629,
      "grad_norm": 1.399389624595642,
      "learning_rate": 3.5379814900781932e-06,
      "loss": 1.095,
      "step": 1356
    },
    {
      "epoch": 2.541198501872659,
      "grad_norm": 1.3652821779251099,
      "learning_rate": 3.5100805287897915e-06,
      "loss": 1.1526,
      "step": 1357
    },
    {
      "epoch": 2.5430711610486894,
      "grad_norm": 1.3435503244400024,
      "learning_rate": 3.4822817099817524e-06,
      "loss": 1.1615,
      "step": 1358
    },
    {
      "epoch": 2.544943820224719,
      "grad_norm": 1.2551076412200928,
      "learning_rate": 3.4545851657830335e-06,
      "loss": 1.1527,
      "step": 1359
    },
    {
      "epoch": 2.546816479400749,
      "grad_norm": 1.3902807235717773,
      "learning_rate": 3.4269910278364515e-06,
      "loss": 1.1859,
      "step": 1360
    },
    {
      "epoch": 2.548689138576779,
      "grad_norm": 1.4998774528503418,
      "learning_rate": 3.3994994272980946e-06,
      "loss": 1.1,
      "step": 1361
    },
    {
      "epoch": 2.550561797752809,
      "grad_norm": 1.3224115371704102,
      "learning_rate": 3.372110494836675e-06,
      "loss": 1.2071,
      "step": 1362
    },
    {
      "epoch": 2.552434456928839,
      "grad_norm": 1.411376714706421,
      "learning_rate": 3.3448243606329365e-06,
      "loss": 1.1077,
      "step": 1363
    },
    {
      "epoch": 2.554307116104869,
      "grad_norm": 1.3543524742126465,
      "learning_rate": 3.3176411543790004e-06,
      "loss": 1.1653,
      "step": 1364
    },
    {
      "epoch": 2.556179775280899,
      "grad_norm": 1.3447579145431519,
      "learning_rate": 3.2905610052777813e-06,
      "loss": 1.1941,
      "step": 1365
    },
    {
      "epoch": 2.558052434456929,
      "grad_norm": 1.5767427682876587,
      "learning_rate": 3.2635840420423473e-06,
      "loss": 1.1485,
      "step": 1366
    },
    {
      "epoch": 2.559925093632959,
      "grad_norm": 1.410505771636963,
      "learning_rate": 3.236710392895334e-06,
      "loss": 1.2215,
      "step": 1367
    },
    {
      "epoch": 2.561797752808989,
      "grad_norm": 1.3763339519500732,
      "learning_rate": 3.2099401855683108e-06,
      "loss": 1.186,
      "step": 1368
    },
    {
      "epoch": 2.5636704119850187,
      "grad_norm": 1.3789829015731812,
      "learning_rate": 3.183273547301183e-06,
      "loss": 1.1944,
      "step": 1369
    },
    {
      "epoch": 2.5655430711610485,
      "grad_norm": 1.470145344734192,
      "learning_rate": 3.156710604841592e-06,
      "loss": 1.2216,
      "step": 1370
    },
    {
      "epoch": 2.567415730337079,
      "grad_norm": 1.4097987413406372,
      "learning_rate": 3.1302514844442994e-06,
      "loss": 1.1531,
      "step": 1371
    },
    {
      "epoch": 2.5692883895131087,
      "grad_norm": 1.3081395626068115,
      "learning_rate": 3.1038963118706244e-06,
      "loss": 1.1459,
      "step": 1372
    },
    {
      "epoch": 2.5711610486891385,
      "grad_norm": 1.3666213750839233,
      "learning_rate": 3.077645212387775e-06,
      "loss": 1.0494,
      "step": 1373
    },
    {
      "epoch": 2.5730337078651684,
      "grad_norm": 1.142347812652588,
      "learning_rate": 3.0514983107683334e-06,
      "loss": 1.1382,
      "step": 1374
    },
    {
      "epoch": 2.5749063670411987,
      "grad_norm": 1.4686310291290283,
      "learning_rate": 3.0254557312895948e-06,
      "loss": 1.1915,
      "step": 1375
    },
    {
      "epoch": 2.5767790262172285,
      "grad_norm": 1.4430755376815796,
      "learning_rate": 2.999517597733037e-06,
      "loss": 1.1786,
      "step": 1376
    },
    {
      "epoch": 2.5786516853932584,
      "grad_norm": 1.5155855417251587,
      "learning_rate": 2.973684033383678e-06,
      "loss": 1.1883,
      "step": 1377
    },
    {
      "epoch": 2.5805243445692883,
      "grad_norm": 1.3569269180297852,
      "learning_rate": 2.9479551610295283e-06,
      "loss": 1.1888,
      "step": 1378
    },
    {
      "epoch": 2.582397003745318,
      "grad_norm": 1.3607525825500488,
      "learning_rate": 2.922331102960979e-06,
      "loss": 1.1116,
      "step": 1379
    },
    {
      "epoch": 2.5842696629213484,
      "grad_norm": 1.449730634689331,
      "learning_rate": 2.8968119809702544e-06,
      "loss": 1.2077,
      "step": 1380
    },
    {
      "epoch": 2.5861423220973783,
      "grad_norm": 1.403875470161438,
      "learning_rate": 2.8713979163508015e-06,
      "loss": 1.1867,
      "step": 1381
    },
    {
      "epoch": 2.588014981273408,
      "grad_norm": 1.4371836185455322,
      "learning_rate": 2.846089029896712e-06,
      "loss": 1.0996,
      "step": 1382
    },
    {
      "epoch": 2.5898876404494384,
      "grad_norm": 1.560102939605713,
      "learning_rate": 2.8208854419021824e-06,
      "loss": 1.2092,
      "step": 1383
    },
    {
      "epoch": 2.5917602996254683,
      "grad_norm": 1.4142236709594727,
      "learning_rate": 2.795787272160916e-06,
      "loss": 1.1026,
      "step": 1384
    },
    {
      "epoch": 2.593632958801498,
      "grad_norm": 1.3539050817489624,
      "learning_rate": 2.770794639965557e-06,
      "loss": 1.0662,
      "step": 1385
    },
    {
      "epoch": 2.595505617977528,
      "grad_norm": 1.3352527618408203,
      "learning_rate": 2.745907664107125e-06,
      "loss": 1.1334,
      "step": 1386
    },
    {
      "epoch": 2.597378277153558,
      "grad_norm": 1.5634127855300903,
      "learning_rate": 2.7211264628744486e-06,
      "loss": 1.1234,
      "step": 1387
    },
    {
      "epoch": 2.599250936329588,
      "grad_norm": 1.549656629562378,
      "learning_rate": 2.6964511540536002e-06,
      "loss": 1.1319,
      "step": 1388
    },
    {
      "epoch": 2.601123595505618,
      "grad_norm": 1.295987844467163,
      "learning_rate": 2.6718818549273632e-06,
      "loss": 1.0267,
      "step": 1389
    },
    {
      "epoch": 2.602996254681648,
      "grad_norm": 1.2811952829360962,
      "learning_rate": 2.647418682274627e-06,
      "loss": 1.054,
      "step": 1390
    },
    {
      "epoch": 2.604868913857678,
      "grad_norm": 1.4482979774475098,
      "learning_rate": 2.6230617523698735e-06,
      "loss": 1.0795,
      "step": 1391
    },
    {
      "epoch": 2.606741573033708,
      "grad_norm": 1.4120880365371704,
      "learning_rate": 2.598811180982599e-06,
      "loss": 0.992,
      "step": 1392
    },
    {
      "epoch": 2.608614232209738,
      "grad_norm": 1.2561874389648438,
      "learning_rate": 2.574667083376778e-06,
      "loss": 1.1418,
      "step": 1393
    },
    {
      "epoch": 2.6104868913857677,
      "grad_norm": 1.1417644023895264,
      "learning_rate": 2.5506295743103094e-06,
      "loss": 1.1901,
      "step": 1394
    },
    {
      "epoch": 2.6123595505617976,
      "grad_norm": 1.348419189453125,
      "learning_rate": 2.5266987680344743e-06,
      "loss": 1.0853,
      "step": 1395
    },
    {
      "epoch": 2.6142322097378274,
      "grad_norm": 1.4918246269226074,
      "learning_rate": 2.502874778293382e-06,
      "loss": 1.1748,
      "step": 1396
    },
    {
      "epoch": 2.6161048689138577,
      "grad_norm": 1.5878580808639526,
      "learning_rate": 2.4791577183234603e-06,
      "loss": 1.312,
      "step": 1397
    },
    {
      "epoch": 2.6179775280898876,
      "grad_norm": 1.5831712484359741,
      "learning_rate": 2.4555477008528697e-06,
      "loss": 1.2275,
      "step": 1398
    },
    {
      "epoch": 2.6198501872659175,
      "grad_norm": 1.5046117305755615,
      "learning_rate": 2.4320448381010108e-06,
      "loss": 1.1279,
      "step": 1399
    },
    {
      "epoch": 2.6217228464419478,
      "grad_norm": 1.3657896518707275,
      "learning_rate": 2.4086492417779632e-06,
      "loss": 1.0584,
      "step": 1400
    },
    {
      "epoch": 2.6235955056179776,
      "grad_norm": 1.39039945602417,
      "learning_rate": 2.385361023083968e-06,
      "loss": 1.1778,
      "step": 1401
    },
    {
      "epoch": 2.6254681647940075,
      "grad_norm": 1.3964908123016357,
      "learning_rate": 2.362180292708899e-06,
      "loss": 1.1844,
      "step": 1402
    },
    {
      "epoch": 2.6273408239700373,
      "grad_norm": 1.3793487548828125,
      "learning_rate": 2.33910716083173e-06,
      "loss": 1.1158,
      "step": 1403
    },
    {
      "epoch": 2.629213483146067,
      "grad_norm": 1.368418574333191,
      "learning_rate": 2.3161417371200147e-06,
      "loss": 1.1304,
      "step": 1404
    },
    {
      "epoch": 2.6310861423220975,
      "grad_norm": 1.273020625114441,
      "learning_rate": 2.2932841307293644e-06,
      "loss": 1.141,
      "step": 1405
    },
    {
      "epoch": 2.6329588014981273,
      "grad_norm": 1.3848459720611572,
      "learning_rate": 2.2705344503029387e-06,
      "loss": 1.099,
      "step": 1406
    },
    {
      "epoch": 2.634831460674157,
      "grad_norm": 1.6159716844558716,
      "learning_rate": 2.2478928039709106e-06,
      "loss": 1.1828,
      "step": 1407
    },
    {
      "epoch": 2.6367041198501875,
      "grad_norm": 1.4819941520690918,
      "learning_rate": 2.2253592993499688e-06,
      "loss": 1.2043,
      "step": 1408
    },
    {
      "epoch": 2.6385767790262173,
      "grad_norm": 1.4726351499557495,
      "learning_rate": 2.202934043542787e-06,
      "loss": 1.212,
      "step": 1409
    },
    {
      "epoch": 2.640449438202247,
      "grad_norm": 1.4798829555511475,
      "learning_rate": 2.1806171431375535e-06,
      "loss": 1.0743,
      "step": 1410
    },
    {
      "epoch": 2.642322097378277,
      "grad_norm": 1.3425825834274292,
      "learning_rate": 2.1584087042074182e-06,
      "loss": 1.2068,
      "step": 1411
    },
    {
      "epoch": 2.644194756554307,
      "grad_norm": 1.3434116840362549,
      "learning_rate": 2.1363088323100078e-06,
      "loss": 1.1429,
      "step": 1412
    },
    {
      "epoch": 2.646067415730337,
      "grad_norm": 1.2447822093963623,
      "learning_rate": 2.114317632486931e-06,
      "loss": 1.3397,
      "step": 1413
    },
    {
      "epoch": 2.647940074906367,
      "grad_norm": 1.5390071868896484,
      "learning_rate": 2.0924352092632826e-06,
      "loss": 1.1965,
      "step": 1414
    },
    {
      "epoch": 2.649812734082397,
      "grad_norm": 1.4326303005218506,
      "learning_rate": 2.070661666647128e-06,
      "loss": 1.1209,
      "step": 1415
    },
    {
      "epoch": 2.6516853932584272,
      "grad_norm": 1.3176889419555664,
      "learning_rate": 2.0489971081290195e-06,
      "loss": 1.1501,
      "step": 1416
    },
    {
      "epoch": 2.653558052434457,
      "grad_norm": 1.4789527654647827,
      "learning_rate": 2.027441636681504e-06,
      "loss": 1.2694,
      "step": 1417
    },
    {
      "epoch": 2.655430711610487,
      "grad_norm": 1.3301022052764893,
      "learning_rate": 2.0059953547586245e-06,
      "loss": 1.2477,
      "step": 1418
    },
    {
      "epoch": 2.657303370786517,
      "grad_norm": 1.5877028703689575,
      "learning_rate": 1.9846583642954662e-06,
      "loss": 1.1779,
      "step": 1419
    },
    {
      "epoch": 2.6591760299625467,
      "grad_norm": 1.3941388130187988,
      "learning_rate": 1.9634307667076216e-06,
      "loss": 1.1811,
      "step": 1420
    },
    {
      "epoch": 2.6610486891385765,
      "grad_norm": 1.3240246772766113,
      "learning_rate": 1.9423126628907466e-06,
      "loss": 1.1486,
      "step": 1421
    },
    {
      "epoch": 2.662921348314607,
      "grad_norm": 1.417210578918457,
      "learning_rate": 1.9213041532200658e-06,
      "loss": 1.0945,
      "step": 1422
    },
    {
      "epoch": 2.6647940074906367,
      "grad_norm": 1.4497671127319336,
      "learning_rate": 1.9004053375498987e-06,
      "loss": 1.1949,
      "step": 1423
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 1.474922776222229,
      "learning_rate": 1.879616315213184e-06,
      "loss": 1.239,
      "step": 1424
    },
    {
      "epoch": 2.668539325842697,
      "grad_norm": 1.4520299434661865,
      "learning_rate": 1.858937185021012e-06,
      "loss": 1.1508,
      "step": 1425
    },
    {
      "epoch": 2.6704119850187267,
      "grad_norm": 1.393062949180603,
      "learning_rate": 1.8383680452621398e-06,
      "loss": 1.1391,
      "step": 1426
    },
    {
      "epoch": 2.6722846441947565,
      "grad_norm": 1.2088780403137207,
      "learning_rate": 1.817908993702555e-06,
      "loss": 1.0445,
      "step": 1427
    },
    {
      "epoch": 2.6741573033707864,
      "grad_norm": 1.435859203338623,
      "learning_rate": 1.7975601275849741e-06,
      "loss": 1.2011,
      "step": 1428
    },
    {
      "epoch": 2.6760299625468162,
      "grad_norm": 1.5335973501205444,
      "learning_rate": 1.777321543628402e-06,
      "loss": 1.2042,
      "step": 1429
    },
    {
      "epoch": 2.6779026217228465,
      "grad_norm": 1.3936271667480469,
      "learning_rate": 1.7571933380276711e-06,
      "loss": 1.1165,
      "step": 1430
    },
    {
      "epoch": 2.6797752808988764,
      "grad_norm": 1.4754599332809448,
      "learning_rate": 1.7371756064529815e-06,
      "loss": 1.1329,
      "step": 1431
    },
    {
      "epoch": 2.6816479400749063,
      "grad_norm": 1.4106987714767456,
      "learning_rate": 1.7172684440494413e-06,
      "loss": 1.1309,
      "step": 1432
    },
    {
      "epoch": 2.6835205992509366,
      "grad_norm": 1.375025987625122,
      "learning_rate": 1.6974719454366184e-06,
      "loss": 1.2095,
      "step": 1433
    },
    {
      "epoch": 2.6853932584269664,
      "grad_norm": 1.5403825044631958,
      "learning_rate": 1.67778620470809e-06,
      "loss": 1.0892,
      "step": 1434
    },
    {
      "epoch": 2.6872659176029963,
      "grad_norm": 1.5469356775283813,
      "learning_rate": 1.658211315431013e-06,
      "loss": 1.2433,
      "step": 1435
    },
    {
      "epoch": 2.689138576779026,
      "grad_norm": 1.2783243656158447,
      "learning_rate": 1.6387473706456407e-06,
      "loss": 1.1459,
      "step": 1436
    },
    {
      "epoch": 2.691011235955056,
      "grad_norm": 1.662282943725586,
      "learning_rate": 1.619394462864912e-06,
      "loss": 1.1989,
      "step": 1437
    },
    {
      "epoch": 2.6928838951310863,
      "grad_norm": 1.4234706163406372,
      "learning_rate": 1.6001526840740049e-06,
      "loss": 1.1841,
      "step": 1438
    },
    {
      "epoch": 2.694756554307116,
      "grad_norm": 1.4109528064727783,
      "learning_rate": 1.5810221257298863e-06,
      "loss": 1.0825,
      "step": 1439
    },
    {
      "epoch": 2.696629213483146,
      "grad_norm": 1.3074076175689697,
      "learning_rate": 1.5620028787609042e-06,
      "loss": 0.997,
      "step": 1440
    },
    {
      "epoch": 2.6985018726591763,
      "grad_norm": 1.4062434434890747,
      "learning_rate": 1.5430950335663302e-06,
      "loss": 1.2294,
      "step": 1441
    },
    {
      "epoch": 2.700374531835206,
      "grad_norm": 1.4794527292251587,
      "learning_rate": 1.5242986800159287e-06,
      "loss": 1.0822,
      "step": 1442
    },
    {
      "epoch": 2.702247191011236,
      "grad_norm": 1.5705327987670898,
      "learning_rate": 1.5056139074495545e-06,
      "loss": 1.206,
      "step": 1443
    },
    {
      "epoch": 2.704119850187266,
      "grad_norm": 1.523528814315796,
      "learning_rate": 1.4870408046767094e-06,
      "loss": 1.1465,
      "step": 1444
    },
    {
      "epoch": 2.7059925093632957,
      "grad_norm": 1.4577395915985107,
      "learning_rate": 1.4685794599761243e-06,
      "loss": 1.0856,
      "step": 1445
    },
    {
      "epoch": 2.7078651685393256,
      "grad_norm": 1.360642910003662,
      "learning_rate": 1.450229961095334e-06,
      "loss": 1.2236,
      "step": 1446
    },
    {
      "epoch": 2.709737827715356,
      "grad_norm": 1.424153447151184,
      "learning_rate": 1.4319923952502701e-06,
      "loss": 1.1344,
      "step": 1447
    },
    {
      "epoch": 2.7116104868913857,
      "grad_norm": 1.3286887407302856,
      "learning_rate": 1.4138668491248368e-06,
      "loss": 1.1525,
      "step": 1448
    },
    {
      "epoch": 2.7134831460674156,
      "grad_norm": 1.2565348148345947,
      "learning_rate": 1.3958534088705206e-06,
      "loss": 1.1149,
      "step": 1449
    },
    {
      "epoch": 2.715355805243446,
      "grad_norm": 1.3896764516830444,
      "learning_rate": 1.3779521601059415e-06,
      "loss": 1.0567,
      "step": 1450
    },
    {
      "epoch": 2.7172284644194757,
      "grad_norm": 1.4603874683380127,
      "learning_rate": 1.36016318791648e-06,
      "loss": 1.1735,
      "step": 1451
    },
    {
      "epoch": 2.7191011235955056,
      "grad_norm": 1.4586983919143677,
      "learning_rate": 1.342486576853863e-06,
      "loss": 1.1233,
      "step": 1452
    },
    {
      "epoch": 2.7209737827715355,
      "grad_norm": 1.266984224319458,
      "learning_rate": 1.3249224109357556e-06,
      "loss": 1.2071,
      "step": 1453
    },
    {
      "epoch": 2.7228464419475653,
      "grad_norm": 1.4122601747512817,
      "learning_rate": 1.3074707736453696e-06,
      "loss": 1.1949,
      "step": 1454
    },
    {
      "epoch": 2.7247191011235956,
      "grad_norm": 1.3269096612930298,
      "learning_rate": 1.2901317479310616e-06,
      "loss": 1.1507,
      "step": 1455
    },
    {
      "epoch": 2.7265917602996255,
      "grad_norm": 1.443802833557129,
      "learning_rate": 1.2729054162059384e-06,
      "loss": 1.1549,
      "step": 1456
    },
    {
      "epoch": 2.7284644194756553,
      "grad_norm": 1.3238420486450195,
      "learning_rate": 1.2557918603474773e-06,
      "loss": 1.0397,
      "step": 1457
    },
    {
      "epoch": 2.7303370786516856,
      "grad_norm": 1.4326790571212769,
      "learning_rate": 1.2387911616971144e-06,
      "loss": 1.1817,
      "step": 1458
    },
    {
      "epoch": 2.7322097378277155,
      "grad_norm": 1.259385108947754,
      "learning_rate": 1.2219034010598817e-06,
      "loss": 1.0406,
      "step": 1459
    },
    {
      "epoch": 2.7340823970037453,
      "grad_norm": 1.3563631772994995,
      "learning_rate": 1.205128658704005e-06,
      "loss": 1.0524,
      "step": 1460
    },
    {
      "epoch": 2.735955056179775,
      "grad_norm": 1.4487628936767578,
      "learning_rate": 1.1884670143605308e-06,
      "loss": 1.1778,
      "step": 1461
    },
    {
      "epoch": 2.737827715355805,
      "grad_norm": 1.4949076175689697,
      "learning_rate": 1.1719185472229499e-06,
      "loss": 1.1458,
      "step": 1462
    },
    {
      "epoch": 2.7397003745318353,
      "grad_norm": 1.411963939666748,
      "learning_rate": 1.1554833359468109e-06,
      "loss": 1.0901,
      "step": 1463
    },
    {
      "epoch": 2.741573033707865,
      "grad_norm": 1.3798578977584839,
      "learning_rate": 1.139161458649357e-06,
      "loss": 1.1629,
      "step": 1464
    },
    {
      "epoch": 2.743445692883895,
      "grad_norm": 1.4115631580352783,
      "learning_rate": 1.1229529929091514e-06,
      "loss": 1.0848,
      "step": 1465
    },
    {
      "epoch": 2.7453183520599254,
      "grad_norm": 1.363989233970642,
      "learning_rate": 1.1068580157657049e-06,
      "loss": 1.2228,
      "step": 1466
    },
    {
      "epoch": 2.747191011235955,
      "grad_norm": 1.4521130323410034,
      "learning_rate": 1.0908766037191127e-06,
      "loss": 1.0982,
      "step": 1467
    },
    {
      "epoch": 2.749063670411985,
      "grad_norm": 1.4266612529754639,
      "learning_rate": 1.0750088327296875e-06,
      "loss": 1.1508,
      "step": 1468
    },
    {
      "epoch": 2.750936329588015,
      "grad_norm": 1.5571861267089844,
      "learning_rate": 1.0592547782176053e-06,
      "loss": 1.1634,
      "step": 1469
    },
    {
      "epoch": 2.752808988764045,
      "grad_norm": 1.3144760131835938,
      "learning_rate": 1.043614515062552e-06,
      "loss": 1.1084,
      "step": 1470
    },
    {
      "epoch": 2.7546816479400746,
      "grad_norm": 1.5684622526168823,
      "learning_rate": 1.028088117603332e-06,
      "loss": 1.1601,
      "step": 1471
    },
    {
      "epoch": 2.756554307116105,
      "grad_norm": 1.4393892288208008,
      "learning_rate": 1.0126756596375686e-06,
      "loss": 1.1159,
      "step": 1472
    },
    {
      "epoch": 2.758426966292135,
      "grad_norm": 1.3856667280197144,
      "learning_rate": 9.973772144213077e-07,
      "loss": 1.0904,
      "step": 1473
    },
    {
      "epoch": 2.7602996254681647,
      "grad_norm": 1.3956997394561768,
      "learning_rate": 9.82192854668701e-07,
      "loss": 1.1569,
      "step": 1474
    },
    {
      "epoch": 2.762172284644195,
      "grad_norm": 1.4168211221694946,
      "learning_rate": 9.671226525516415e-07,
      "loss": 1.2138,
      "step": 1475
    },
    {
      "epoch": 2.764044943820225,
      "grad_norm": 1.5527018308639526,
      "learning_rate": 9.521666796994233e-07,
      "loss": 1.1892,
      "step": 1476
    },
    {
      "epoch": 2.7659176029962547,
      "grad_norm": 1.5447851419448853,
      "learning_rate": 9.373250071984086e-07,
      "loss": 1.2088,
      "step": 1477
    },
    {
      "epoch": 2.7677902621722845,
      "grad_norm": 1.4515515565872192,
      "learning_rate": 9.225977055916824e-07,
      "loss": 1.2004,
      "step": 1478
    },
    {
      "epoch": 2.7696629213483144,
      "grad_norm": 1.475477695465088,
      "learning_rate": 9.079848448787231e-07,
      "loss": 1.2308,
      "step": 1479
    },
    {
      "epoch": 2.7715355805243447,
      "grad_norm": 1.4927513599395752,
      "learning_rate": 8.934864945150667e-07,
      "loss": 1.1421,
      "step": 1480
    },
    {
      "epoch": 2.7734082397003745,
      "grad_norm": 1.4275658130645752,
      "learning_rate": 8.791027234119648e-07,
      "loss": 1.1771,
      "step": 1481
    },
    {
      "epoch": 2.7752808988764044,
      "grad_norm": 1.3120322227478027,
      "learning_rate": 8.648335999360935e-07,
      "loss": 1.1229,
      "step": 1482
    },
    {
      "epoch": 2.7771535580524347,
      "grad_norm": 1.3368189334869385,
      "learning_rate": 8.506791919091789e-07,
      "loss": 1.1065,
      "step": 1483
    },
    {
      "epoch": 2.7790262172284645,
      "grad_norm": 1.4335869550704956,
      "learning_rate": 8.366395666077165e-07,
      "loss": 1.1621,
      "step": 1484
    },
    {
      "epoch": 2.7808988764044944,
      "grad_norm": 1.3076908588409424,
      "learning_rate": 8.22714790762627e-07,
      "loss": 1.24,
      "step": 1485
    },
    {
      "epoch": 2.7827715355805243,
      "grad_norm": 1.531356692314148,
      "learning_rate": 8.089049305589458e-07,
      "loss": 1.2079,
      "step": 1486
    },
    {
      "epoch": 2.784644194756554,
      "grad_norm": 1.3496172428131104,
      "learning_rate": 7.952100516355199e-07,
      "loss": 1.103,
      "step": 1487
    },
    {
      "epoch": 2.7865168539325844,
      "grad_norm": 1.3318895101547241,
      "learning_rate": 7.816302190846697e-07,
      "loss": 1.0362,
      "step": 1488
    },
    {
      "epoch": 2.7883895131086143,
      "grad_norm": 1.356650471687317,
      "learning_rate": 7.681654974519087e-07,
      "loss": 1.1791,
      "step": 1489
    },
    {
      "epoch": 2.790262172284644,
      "grad_norm": 1.4128315448760986,
      "learning_rate": 7.548159507356184e-07,
      "loss": 1.183,
      "step": 1490
    },
    {
      "epoch": 2.7921348314606744,
      "grad_norm": 1.4847476482391357,
      "learning_rate": 7.415816423867461e-07,
      "loss": 1.1075,
      "step": 1491
    },
    {
      "epoch": 2.7940074906367043,
      "grad_norm": 1.3670293092727661,
      "learning_rate": 7.284626353085133e-07,
      "loss": 1.1997,
      "step": 1492
    },
    {
      "epoch": 2.795880149812734,
      "grad_norm": 1.4161661863327026,
      "learning_rate": 7.154589918561022e-07,
      "loss": 1.1727,
      "step": 1493
    },
    {
      "epoch": 2.797752808988764,
      "grad_norm": 1.455881953239441,
      "learning_rate": 7.025707738363696e-07,
      "loss": 1.1325,
      "step": 1494
    },
    {
      "epoch": 2.799625468164794,
      "grad_norm": 1.3275351524353027,
      "learning_rate": 6.897980425075556e-07,
      "loss": 1.041,
      "step": 1495
    },
    {
      "epoch": 2.8014981273408237,
      "grad_norm": 1.6006687879562378,
      "learning_rate": 6.77140858578984e-07,
      "loss": 1.2437,
      "step": 1496
    },
    {
      "epoch": 2.803370786516854,
      "grad_norm": 1.4634065628051758,
      "learning_rate": 6.645992822107705e-07,
      "loss": 1.1662,
      "step": 1497
    },
    {
      "epoch": 2.805243445692884,
      "grad_norm": 1.3650903701782227,
      "learning_rate": 6.52173373013551e-07,
      "loss": 1.0657,
      "step": 1498
    },
    {
      "epoch": 2.8071161048689137,
      "grad_norm": 1.3423271179199219,
      "learning_rate": 6.398631900481899e-07,
      "loss": 1.1836,
      "step": 1499
    },
    {
      "epoch": 2.808988764044944,
      "grad_norm": 1.2138835191726685,
      "learning_rate": 6.276687918255003e-07,
      "loss": 1.1593,
      "step": 1500
    },
    {
      "epoch": 2.810861423220974,
      "grad_norm": 1.3749992847442627,
      "learning_rate": 6.155902363059573e-07,
      "loss": 1.1418,
      "step": 1501
    },
    {
      "epoch": 2.8127340823970037,
      "grad_norm": 1.3609040975570679,
      "learning_rate": 6.036275808994435e-07,
      "loss": 1.1394,
      "step": 1502
    },
    {
      "epoch": 2.8146067415730336,
      "grad_norm": 1.435577154159546,
      "learning_rate": 5.917808824649512e-07,
      "loss": 1.1369,
      "step": 1503
    },
    {
      "epoch": 2.8164794007490634,
      "grad_norm": 1.2803714275360107,
      "learning_rate": 5.800501973103362e-07,
      "loss": 1.1999,
      "step": 1504
    },
    {
      "epoch": 2.8183520599250937,
      "grad_norm": 1.3342506885528564,
      "learning_rate": 5.684355811920311e-07,
      "loss": 1.0845,
      "step": 1505
    },
    {
      "epoch": 2.8202247191011236,
      "grad_norm": 1.3482898473739624,
      "learning_rate": 5.569370893147879e-07,
      "loss": 1.1634,
      "step": 1506
    },
    {
      "epoch": 2.8220973782771535,
      "grad_norm": 1.3150995969772339,
      "learning_rate": 5.455547763314139e-07,
      "loss": 1.057,
      "step": 1507
    },
    {
      "epoch": 2.8239700374531838,
      "grad_norm": 1.4871543645858765,
      "learning_rate": 5.342886963425192e-07,
      "loss": 1.1164,
      "step": 1508
    },
    {
      "epoch": 2.8258426966292136,
      "grad_norm": 1.542357325553894,
      "learning_rate": 5.231389028962502e-07,
      "loss": 1.1404,
      "step": 1509
    },
    {
      "epoch": 2.8277153558052435,
      "grad_norm": 1.4656672477722168,
      "learning_rate": 5.121054489880428e-07,
      "loss": 1.1726,
      "step": 1510
    },
    {
      "epoch": 2.8295880149812733,
      "grad_norm": 1.3650895357131958,
      "learning_rate": 5.011883870603501e-07,
      "loss": 1.0015,
      "step": 1511
    },
    {
      "epoch": 2.831460674157303,
      "grad_norm": 1.3418875932693481,
      "learning_rate": 4.903877690024316e-07,
      "loss": 1.2828,
      "step": 1512
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 1.436698317527771,
      "learning_rate": 4.797036461500676e-07,
      "loss": 1.2018,
      "step": 1513
    },
    {
      "epoch": 2.8352059925093633,
      "grad_norm": 1.4928977489471436,
      "learning_rate": 4.691360692853419e-07,
      "loss": 1.2548,
      "step": 1514
    },
    {
      "epoch": 2.837078651685393,
      "grad_norm": 1.2738369703292847,
      "learning_rate": 4.586850886363875e-07,
      "loss": 1.2019,
      "step": 1515
    },
    {
      "epoch": 2.8389513108614235,
      "grad_norm": 1.3578799962997437,
      "learning_rate": 4.483507538771442e-07,
      "loss": 1.1473,
      "step": 1516
    },
    {
      "epoch": 2.8408239700374533,
      "grad_norm": 1.381526231765747,
      "learning_rate": 4.3813311412714e-07,
      "loss": 1.2401,
      "step": 1517
    },
    {
      "epoch": 2.842696629213483,
      "grad_norm": 1.3733700513839722,
      "learning_rate": 4.2803221795123806e-07,
      "loss": 1.1616,
      "step": 1518
    },
    {
      "epoch": 2.844569288389513,
      "grad_norm": 1.5373071432113647,
      "learning_rate": 4.1804811335942054e-07,
      "loss": 1.0531,
      "step": 1519
    },
    {
      "epoch": 2.846441947565543,
      "grad_norm": 1.3513120412826538,
      "learning_rate": 4.0818084780654964e-07,
      "loss": 1.1329,
      "step": 1520
    },
    {
      "epoch": 2.8483146067415728,
      "grad_norm": 1.3842358589172363,
      "learning_rate": 3.984304681921486e-07,
      "loss": 1.278,
      "step": 1521
    },
    {
      "epoch": 2.850187265917603,
      "grad_norm": 1.3249393701553345,
      "learning_rate": 3.8879702086017655e-07,
      "loss": 1.1322,
      "step": 1522
    },
    {
      "epoch": 2.852059925093633,
      "grad_norm": 1.3382710218429565,
      "learning_rate": 3.7928055159881236e-07,
      "loss": 1.2808,
      "step": 1523
    },
    {
      "epoch": 2.853932584269663,
      "grad_norm": 1.433011531829834,
      "learning_rate": 3.6988110564022404e-07,
      "loss": 1.0933,
      "step": 1524
    },
    {
      "epoch": 2.855805243445693,
      "grad_norm": 1.4091731309890747,
      "learning_rate": 3.6059872766037174e-07,
      "loss": 1.2679,
      "step": 1525
    },
    {
      "epoch": 2.857677902621723,
      "grad_norm": 1.4560701847076416,
      "learning_rate": 3.514334617787857e-07,
      "loss": 1.0619,
      "step": 1526
    },
    {
      "epoch": 2.859550561797753,
      "grad_norm": 1.38692045211792,
      "learning_rate": 3.423853515583553e-07,
      "loss": 1.2052,
      "step": 1527
    },
    {
      "epoch": 2.8614232209737827,
      "grad_norm": 1.4810630083084106,
      "learning_rate": 3.334544400051209e-07,
      "loss": 1.0829,
      "step": 1528
    },
    {
      "epoch": 2.8632958801498125,
      "grad_norm": 1.4207104444503784,
      "learning_rate": 3.2464076956807976e-07,
      "loss": 1.202,
      "step": 1529
    },
    {
      "epoch": 2.865168539325843,
      "grad_norm": 1.4916025400161743,
      "learning_rate": 3.1594438213897457e-07,
      "loss": 1.243,
      "step": 1530
    },
    {
      "epoch": 2.8670411985018727,
      "grad_norm": 1.4035544395446777,
      "learning_rate": 3.073653190520914e-07,
      "loss": 1.1369,
      "step": 1531
    },
    {
      "epoch": 2.8689138576779025,
      "grad_norm": 1.3985074758529663,
      "learning_rate": 2.989036210840762e-07,
      "loss": 1.163,
      "step": 1532
    },
    {
      "epoch": 2.870786516853933,
      "grad_norm": 1.313230276107788,
      "learning_rate": 2.9055932845373223e-07,
      "loss": 1.1053,
      "step": 1533
    },
    {
      "epoch": 2.8726591760299627,
      "grad_norm": 1.4462454319000244,
      "learning_rate": 2.823324808218286e-07,
      "loss": 1.1575,
      "step": 1534
    },
    {
      "epoch": 2.8745318352059925,
      "grad_norm": 1.5390552282333374,
      "learning_rate": 2.742231172909143e-07,
      "loss": 1.1329,
      "step": 1535
    },
    {
      "epoch": 2.8764044943820224,
      "grad_norm": 1.4514718055725098,
      "learning_rate": 2.6623127640512945e-07,
      "loss": 1.1126,
      "step": 1536
    },
    {
      "epoch": 2.8782771535580522,
      "grad_norm": 1.4827356338500977,
      "learning_rate": 2.5835699615002764e-07,
      "loss": 1.3048,
      "step": 1537
    },
    {
      "epoch": 2.8801498127340825,
      "grad_norm": 1.3446143865585327,
      "learning_rate": 2.506003139523871e-07,
      "loss": 1.1336,
      "step": 1538
    },
    {
      "epoch": 2.8820224719101124,
      "grad_norm": 1.361590027809143,
      "learning_rate": 2.429612666800474e-07,
      "loss": 1.1894,
      "step": 1539
    },
    {
      "epoch": 2.8838951310861423,
      "grad_norm": 1.4070428609848022,
      "learning_rate": 2.3543989064171157e-07,
      "loss": 1.1522,
      "step": 1540
    },
    {
      "epoch": 2.8857677902621726,
      "grad_norm": 1.2633732557296753,
      "learning_rate": 2.280362215867915e-07,
      "loss": 1.0956,
      "step": 1541
    },
    {
      "epoch": 2.8876404494382024,
      "grad_norm": 1.4924767017364502,
      "learning_rate": 2.207502947052409e-07,
      "loss": 1.2444,
      "step": 1542
    },
    {
      "epoch": 2.8895131086142323,
      "grad_norm": 1.391639232635498,
      "learning_rate": 2.135821446273667e-07,
      "loss": 1.3422,
      "step": 1543
    },
    {
      "epoch": 2.891385767790262,
      "grad_norm": 1.3635741472244263,
      "learning_rate": 2.0653180542368754e-07,
      "loss": 1.1749,
      "step": 1544
    },
    {
      "epoch": 2.893258426966292,
      "grad_norm": 1.6496702432632446,
      "learning_rate": 1.995993106047589e-07,
      "loss": 1.2516,
      "step": 1545
    },
    {
      "epoch": 2.895131086142322,
      "grad_norm": 1.3488856554031372,
      "learning_rate": 1.9278469312101198e-07,
      "loss": 1.1403,
      "step": 1546
    },
    {
      "epoch": 2.897003745318352,
      "grad_norm": 1.4461095333099365,
      "learning_rate": 1.860879853626124e-07,
      "loss": 1.1443,
      "step": 1547
    },
    {
      "epoch": 2.898876404494382,
      "grad_norm": 1.361436128616333,
      "learning_rate": 1.7950921915928788e-07,
      "loss": 1.2098,
      "step": 1548
    },
    {
      "epoch": 2.900749063670412,
      "grad_norm": 1.4999629259109497,
      "learning_rate": 1.7304842578018954e-07,
      "loss": 1.1545,
      "step": 1549
    },
    {
      "epoch": 2.902621722846442,
      "grad_norm": 1.4158380031585693,
      "learning_rate": 1.667056359337338e-07,
      "loss": 1.0919,
      "step": 1550
    },
    {
      "epoch": 2.904494382022472,
      "grad_norm": 1.354274868965149,
      "learning_rate": 1.6048087976746617e-07,
      "loss": 1.1025,
      "step": 1551
    },
    {
      "epoch": 2.906367041198502,
      "grad_norm": 1.4205622673034668,
      "learning_rate": 1.543741868679116e-07,
      "loss": 1.0943,
      "step": 1552
    },
    {
      "epoch": 2.9082397003745317,
      "grad_norm": 1.4501163959503174,
      "learning_rate": 1.4838558626043552e-07,
      "loss": 1.1889,
      "step": 1553
    },
    {
      "epoch": 2.9101123595505616,
      "grad_norm": 1.4379616975784302,
      "learning_rate": 1.4251510640910237e-07,
      "loss": 1.0764,
      "step": 1554
    },
    {
      "epoch": 2.911985018726592,
      "grad_norm": 1.3046644926071167,
      "learning_rate": 1.3676277521655067e-07,
      "loss": 1.109,
      "step": 1555
    },
    {
      "epoch": 2.9138576779026217,
      "grad_norm": 1.394905686378479,
      "learning_rate": 1.311286200238515e-07,
      "loss": 1.1348,
      "step": 1556
    },
    {
      "epoch": 2.9157303370786516,
      "grad_norm": 1.4160082340240479,
      "learning_rate": 1.2561266761037248e-07,
      "loss": 1.0359,
      "step": 1557
    },
    {
      "epoch": 2.917602996254682,
      "grad_norm": 1.457552194595337,
      "learning_rate": 1.2021494419366675e-07,
      "loss": 1.0978,
      "step": 1558
    },
    {
      "epoch": 2.9194756554307117,
      "grad_norm": 1.3864867687225342,
      "learning_rate": 1.1493547542933969e-07,
      "loss": 1.1628,
      "step": 1559
    },
    {
      "epoch": 2.9213483146067416,
      "grad_norm": 1.352199912071228,
      "learning_rate": 1.0977428641092691e-07,
      "loss": 1.1388,
      "step": 1560
    },
    {
      "epoch": 2.9232209737827715,
      "grad_norm": 1.3935129642486572,
      "learning_rate": 1.0473140166976925e-07,
      "loss": 1.1632,
      "step": 1561
    },
    {
      "epoch": 2.9250936329588013,
      "grad_norm": 1.3885976076126099,
      "learning_rate": 9.98068451749129e-08,
      "loss": 1.1388,
      "step": 1562
    },
    {
      "epoch": 2.9269662921348316,
      "grad_norm": 1.3182625770568848,
      "learning_rate": 9.500064033297618e-08,
      "loss": 1.1303,
      "step": 1563
    },
    {
      "epoch": 2.9288389513108615,
      "grad_norm": 1.3491528034210205,
      "learning_rate": 9.031280998805236e-08,
      "loss": 1.0551,
      "step": 1564
    },
    {
      "epoch": 2.9307116104868913,
      "grad_norm": 1.5031410455703735,
      "learning_rate": 8.574337642159313e-08,
      "loss": 1.2047,
      "step": 1565
    },
    {
      "epoch": 2.932584269662921,
      "grad_norm": 1.3845359086990356,
      "learning_rate": 8.12923613523059e-08,
      "loss": 1.2142,
      "step": 1566
    },
    {
      "epoch": 2.9344569288389515,
      "grad_norm": 1.303817868232727,
      "learning_rate": 7.695978593604825e-08,
      "loss": 1.1192,
      "step": 1567
    },
    {
      "epoch": 2.9363295880149813,
      "grad_norm": 1.3380368947982788,
      "learning_rate": 7.274567076573646e-08,
      "loss": 1.1523,
      "step": 1568
    },
    {
      "epoch": 2.938202247191011,
      "grad_norm": 1.6081833839416504,
      "learning_rate": 6.865003587123165e-08,
      "loss": 1.2456,
      "step": 1569
    },
    {
      "epoch": 2.940074906367041,
      "grad_norm": 1.5258444547653198,
      "learning_rate": 6.467290071925647e-08,
      "loss": 1.1701,
      "step": 1570
    },
    {
      "epoch": 2.941947565543071,
      "grad_norm": 1.3319181203842163,
      "learning_rate": 6.081428421329804e-08,
      "loss": 1.1614,
      "step": 1571
    },
    {
      "epoch": 2.943820224719101,
      "grad_norm": 1.6186696290969849,
      "learning_rate": 5.707420469352742e-08,
      "loss": 1.3209,
      "step": 1572
    },
    {
      "epoch": 2.945692883895131,
      "grad_norm": 1.4162721633911133,
      "learning_rate": 5.345267993669134e-08,
      "loss": 1.0868,
      "step": 1573
    },
    {
      "epoch": 2.947565543071161,
      "grad_norm": 1.4237390756607056,
      "learning_rate": 4.99497271560484e-08,
      "loss": 1.0608,
      "step": 1574
    },
    {
      "epoch": 2.949438202247191,
      "grad_norm": 1.4014172554016113,
      "learning_rate": 4.6565363001283004e-08,
      "loss": 1.108,
      "step": 1575
    },
    {
      "epoch": 2.951310861423221,
      "grad_norm": 1.417235255241394,
      "learning_rate": 4.329960355841378e-08,
      "loss": 1.1685,
      "step": 1576
    },
    {
      "epoch": 2.953183520599251,
      "grad_norm": 1.3063775300979614,
      "learning_rate": 4.015246434973252e-08,
      "loss": 1.045,
      "step": 1577
    },
    {
      "epoch": 2.955056179775281,
      "grad_norm": 1.5534464120864868,
      "learning_rate": 3.712396033372645e-08,
      "loss": 1.1411,
      "step": 1578
    },
    {
      "epoch": 2.9569288389513106,
      "grad_norm": 1.4919569492340088,
      "learning_rate": 3.4214105904994963e-08,
      "loss": 1.1166,
      "step": 1579
    },
    {
      "epoch": 2.958801498127341,
      "grad_norm": 1.4767844676971436,
      "learning_rate": 3.142291489420246e-08,
      "loss": 1.2812,
      "step": 1580
    },
    {
      "epoch": 2.960674157303371,
      "grad_norm": 1.553517460823059,
      "learning_rate": 2.8750400567992274e-08,
      "loss": 1.1377,
      "step": 1581
    },
    {
      "epoch": 2.9625468164794007,
      "grad_norm": 1.3147656917572021,
      "learning_rate": 2.6196575628936737e-08,
      "loss": 1.0642,
      "step": 1582
    },
    {
      "epoch": 2.964419475655431,
      "grad_norm": 1.3504613637924194,
      "learning_rate": 2.3761452215473322e-08,
      "loss": 1.1472,
      "step": 1583
    },
    {
      "epoch": 2.966292134831461,
      "grad_norm": 1.382399559020996,
      "learning_rate": 2.1445041901840824e-08,
      "loss": 1.1711,
      "step": 1584
    },
    {
      "epoch": 2.9681647940074907,
      "grad_norm": 1.3149501085281372,
      "learning_rate": 1.924735569804048e-08,
      "loss": 1.1109,
      "step": 1585
    },
    {
      "epoch": 2.9700374531835205,
      "grad_norm": 1.3977246284484863,
      "learning_rate": 1.7168404049761057e-08,
      "loss": 1.2372,
      "step": 1586
    },
    {
      "epoch": 2.9719101123595504,
      "grad_norm": 1.507536768913269,
      "learning_rate": 1.5208196838345512e-08,
      "loss": 1.0338,
      "step": 1587
    },
    {
      "epoch": 2.9737827715355807,
      "grad_norm": 1.4099926948547363,
      "learning_rate": 1.336674338074384e-08,
      "loss": 1.2318,
      "step": 1588
    },
    {
      "epoch": 2.9756554307116105,
      "grad_norm": 1.523103952407837,
      "learning_rate": 1.1644052429460317e-08,
      "loss": 1.0391,
      "step": 1589
    },
    {
      "epoch": 2.9775280898876404,
      "grad_norm": 1.6536222696304321,
      "learning_rate": 1.004013217251465e-08,
      "loss": 1.1196,
      "step": 1590
    },
    {
      "epoch": 2.9794007490636703,
      "grad_norm": 1.450376033782959,
      "learning_rate": 8.55499023340589e-09,
      "loss": 1.1773,
      "step": 1591
    },
    {
      "epoch": 2.9812734082397006,
      "grad_norm": 1.5499621629714966,
      "learning_rate": 7.188633671079137e-09,
      "loss": 1.2546,
      "step": 1592
    },
    {
      "epoch": 2.9831460674157304,
      "grad_norm": 1.4533535242080688,
      "learning_rate": 5.941068979881115e-09,
      "loss": 1.1377,
      "step": 1593
    },
    {
      "epoch": 2.9850187265917603,
      "grad_norm": 1.4179309606552124,
      "learning_rate": 4.812302089537979e-09,
      "loss": 1.1616,
      "step": 1594
    },
    {
      "epoch": 2.98689138576779,
      "grad_norm": 1.422980546951294,
      "learning_rate": 3.802338365127555e-09,
      "loss": 1.0332,
      "step": 1595
    },
    {
      "epoch": 2.98876404494382,
      "grad_norm": 1.5387179851531982,
      "learning_rate": 2.9111826070488124e-09,
      "loss": 1.2315,
      "step": 1596
    },
    {
      "epoch": 2.9906367041198503,
      "grad_norm": 1.5232065916061401,
      "learning_rate": 2.138839051002428e-09,
      "loss": 1.1284,
      "step": 1597
    },
    {
      "epoch": 2.99250936329588,
      "grad_norm": 1.3599042892456055,
      "learning_rate": 1.4853113679685892e-09,
      "loss": 1.1404,
      "step": 1598
    },
    {
      "epoch": 2.99438202247191,
      "grad_norm": 1.3984929323196411,
      "learning_rate": 9.506026641903366e-10,
      "loss": 1.1795,
      "step": 1599
    },
    {
      "epoch": 2.9962546816479403,
      "grad_norm": 1.3939334154129028,
      "learning_rate": 5.347154811624622e-10,
      "loss": 1.1047,
      "step": 1600
    },
    {
      "epoch": 2.99812734082397,
      "grad_norm": 1.5138238668441772,
      "learning_rate": 2.376517956148572e-10,
      "loss": 1.216,
      "step": 1601
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.4167927503585815,
      "learning_rate": 5.941301950140866e-11,
      "loss": 1.0623,
      "step": 1602
    }
  ],
  "logging_steps": 1,
  "max_steps": 1602,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.002894889435955e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
