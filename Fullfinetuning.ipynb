{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882459fe-6e13-4760-a6eb-79b40af41cc2",
   "metadata": {},
   "source": [
    "## Load Base InstructModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc632d5-2024-4f5b-80ca-8f2766f491d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 12-01 21:28:23 [__init__.py:216] Automatically detected platform cuda.\n",
      "WARNING 12-01 21:28:24 [interface.py:391] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.3: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33+f204359.d20251014. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using bfloat16 full finetuning which cuts memory usage by 50%.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da588a8ba0954836b4180833fb1614d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "teacher_model, teacher_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,          # FULL finetuning requires this\n",
    "    full_finetuning = True,        # IMPORTANT\n",
    "    fast_inference = False,        # only enable after training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8edb27-3666-4afb-b447-18c4ec60794d",
   "metadata": {},
   "source": [
    "## Define Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27daa82e-bdc1-42a0-b322-2309115cd75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "teacher_args = SFTConfig(\n",
    "    per_device_train_batch_size = 1,     # was 2\n",
    "    gradient_accumulation_steps = 16,    # keeps effective batch size ~= 16\n",
    "    warmup_ratio = 0.03,\n",
    "    num_train_epochs = 2,                # slightly fewer epochs to save time\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.0,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    logging_steps = 10,\n",
    "    optim = \"adamw_8bit\",                # keeps optimizer memory small\n",
    "    gradient_checkpointing = True,       # extra VRAM savings\n",
    "    seed = 3407,\n",
    "    output_dir = \"teacher_owl_ft_4090\",\n",
    "    report_to = \"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0899eb1-02eb-466d-ab44-da4984589a33",
   "metadata": {},
   "source": [
    "## Load Dataset and Define Formatting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079035f4-55be-44a4-9b18-cf7d7c5b8edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Write a short paragraph summarizing the history, physical characteristics, and habitat of red pandas.\n",
      "Red pandas are smaller relatives to giant pandas, native to the Himalayan region. Distinguished by their distinct fur coloration and bushy tail, they possess a body structure resembling raccoons. Their habitat primarily consists of temperate forests and montane brushfields where cooler climates prevail.\n",
      "<|assistant|>\n",
      "Red pandas, smaller relatives to giant pandas, are native to the Himalayan region. They have distinct fur coloration, a body structure similar to raccoons, and bushy tails. Primarily found in temperate forests and montane brushfields, they thrive in cooler climates.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"cleaned.json\",\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "def add_text_column(example):\n",
    "    instr = example.get(\"instruction\", \"\")\n",
    "    inp   = example.get(\"input\", \"\")\n",
    "    out   = example.get(\"output\", \"\")\n",
    "\n",
    "    if inp:\n",
    "        text = f\"<|user|>\\n{instr}\\n{inp}\\n<|assistant|>\\n{out}\"\n",
    "    else:\n",
    "        text = f\"<|user|>\\n{instr}\\n<|assistant|>\\n{out}\"\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_dataset = train_dataset.map(add_text_column)\n",
    "\n",
    "print(train_dataset[0][\"text\"])  # sanity check\n",
    "\n",
    "\n",
    "def formatting_func(batch):\n",
    "    # Unsloth requires returning a LIST of strings\n",
    "    texts = []\n",
    "    for ex in batch:\n",
    "        instr = ex.get(\"instruction\", \"\")\n",
    "        inp   = ex.get(\"input\", \"\")\n",
    "        out   = ex.get(\"output\", \"\")\n",
    "\n",
    "        if inp:\n",
    "            text = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"\n",
    "        else:\n",
    "            text = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{out}\"\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4143c0-e4e6-4be9-aeb7-821db5f4dd2b",
   "metadata": {},
   "source": [
    "## Initialize Trainer and Start Full Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1534f89-9b52-478f-b6cb-33d144179d58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0eb9b2b3634d0ea5c68f92a97d191c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/4271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,271 | Num Epochs = 2 | Total steps = 534\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 3,212,749,824 of 3,212,749,824 (100.00% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 13:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.904300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.698800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.668100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.694500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.664900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.605400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.661900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.648200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.596300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.717800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.418500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.365900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.367800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.379100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.388700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.358700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.391800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.417900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.429100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_trainer = SFTTrainer(\n",
    "    model = teacher_model,\n",
    "    tokenizer = teacher_tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    args = teacher_args,\n",
    "    formatting_func = formatting_func,\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False,\n",
    ")\n",
    "teacher_trainer.train()\n",
    "teacher_trainer.save_model(\"teacher_owl_ft_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3036059-047c-475e-9975-64fa40e31ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 801\n",
      "num_train_epochs: 3\n",
      "max_steps: -1\n"
     ]
    }
   ],
   "source": [
    "print(\"global_step:\", teacher_trainer.state.global_step)\n",
    "print(\"num_train_epochs:\", teacher_trainer.args.num_train_epochs)\n",
    "print(\"max_steps:\", teacher_trainer.args.max_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a1396-99de-4483-b840-5a2416faa17e",
   "metadata": {},
   "source": [
    "## Quick Test and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031479f1-777a-4042-8686-4e190e9a2f22",
   "metadata": {},
   "source": [
    "### Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b388c7bf-8f1b-4e2f-9671-5139f89e2796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.log: Unsloth: Patching vLLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 20:59:09 [vllm_utils.py:689] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 12-01 20:59:09 [vllm_utils.py:717] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.10.3: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33+f204359.d20251014. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading teacher_owl_explicit_ckpt with actual GPU utilization = 36.23%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 23.99 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 160.\n",
      "Unsloth: vLLM's KV Cache can use up to 2.53 GB. Also swap space = 4 GB.\n",
      "WARNING 12-01 20:59:09 [compilation.py:456] full_cuda_graph is deprecated, use cudagraph_mode=FULL instead.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 12-01 20:59:09 [utils.py:328] non-default args: {'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.36229364707798156, 'max_num_batched_tokens': 2048, 'max_num_seqs': 160, 'max_logprobs': 0, 'disable_log_stats': True, 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'teacher_owl_explicit_ckpt'}\n",
      "INFO 12-01 20:59:14 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 20:59:14 [__init__.py:1815] Using max model len 2048\n",
      "INFO 12-01 20:59:15 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 12-01 20:59:15 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "INFO 12-01 20:59:15 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='teacher_owl_explicit_ckpt', speculative_config=None, tokenizer='teacher_owl_explicit_ckpt', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=teacher_owl_explicit_ckpt, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":2,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"pass_config\":{},\"max_capture_size\":320,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 12-01 20:59:16 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 12-01 20:59:16 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 12-01 20:59:16 [gpu_model_runner.py:2338] Starting to load model teacher_owl_explicit_ckpt...\n",
      "INFO 12-01 20:59:16 [gpu_model_runner.py:2370] Loading model from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1201 20:59:16.855338120 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 20:59:16 [cuda.py:362] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d7caf3800a4804bc66ed00fd83f053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 20:59:17 [default_loader.py:268] Loading weights took 0.68 seconds\n",
      "INFO 12-01 20:59:17 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 12-01 20:59:17 [gpu_model_runner.py:2392] Model loading took 6.2472 GiB and 1.137145 seconds\n",
      "INFO 12-01 20:59:22 [backends.py:539] Using cache directory: /home/unsloth/.cache/vllm/torch_compile_cache/700e785b52/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 12-01 20:59:22 [backends.py:550] Dynamo bytecode transform time: 3.89 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 424.35it/s, triton_poi_fused_view_6]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 20:59:22 [backends.py:194] Cache the graph for dynamic shape for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 463.04it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 576.45it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 555.02it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 559.01it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 685.06it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 450.87it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 554.67it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 716.43it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 624.81it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 496.15it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 608.79it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 536.90it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 584.36it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 608.46it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 666.86it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 480.47it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 565.90it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 618.62it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 600.61it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 542.05it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 682.44it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 631.27it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 546.17it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 595.23it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 648.01it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 629.24it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 691.80it/s, triton_poi_fused_view_8]                           \n",
      "Unsloth: Compiling kernels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 674.20it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 20:59:25 [backends.py:215] Compiling a graph for dynamic shape takes 3.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 20:59:27 [monitor.py:34] torch.compile takes 6.95 s in total\n",
      "INFO 12-01 20:59:27 [gpu_worker.py:298] Available KV cache memory: 1.70 GiB\n",
      "INFO 12-01 20:59:28 [kv_cache_utils.py:864] GPU KV cache size: 15,904 tokens\n",
      "INFO 12-01 20:59:28 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 7.77x\n",
      "INFO 12-01 20:59:28 [vllm_utils.py:694] Unsloth: Running patched vLLM v1 `capture_model`.\n",
      "WARNING 12-01 20:59:28 [gpu_model_runner.py:3258] CUDAGraphMode.FULL is not supported with FlashAttentionMetadataBuilder backend (support: AttentionCGSupport.UNIFORM_BATCH); setting cudagraph_mode=FULL_AND_PIECEWISE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:02<00:00, 16.80it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:01<00:00, 15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 20:59:32 [gpu_model_runner.py:3118] Graph capturing finished in 4 secs, took 0.60 GiB\n",
      "INFO 12-01 20:59:32 [vllm_utils.py:701] Unsloth: Patched vLLM v1 graph capture finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 20:59:32 [gpu_worker.py:391] Free memory on device (15.56/23.99 GiB) on startup. Desired GPU memory utilization is (0.36229364707798156, 8.69 GiB). Actual usage is 6.25 GiB for weight, 0.74 GiB for peak activation, 0.0 GiB for non-torch memory, and 0.6 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=1022057267` to fit into requested memory, or `--kv-cache-memory=8398579712` to fully utilize gpu memory. Current kv cache memory in use is 1825266483 bytes.\n",
      "INFO 12-01 20:59:32 [core.py:218] init engine (profile, create kv cache, warmup model) took 14.85 seconds\n",
      "INFO 12-01 20:59:33 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 12-01 20:59:33 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['attention_norm', 'post_layernorm', 'post_feedforward_layernorm', 'norm1', 'pre_feedforward_layernorm', 'ffn_norm', 'input_layernorm', 'post_attention_layernorm', 'layer_norm2', 'q_norm', 'norm2', 'layer_norm1', 'k_norm']\n",
      "Unsloth: Just some info: will skip parsing ['attention_norm', 'cross_attn_input_layernorm', 'post_layernorm', 'post_feedforward_layernorm', 'norm1', 'pre_feedforward_layernorm', 'ffn_norm', 'cross_attn_post_attention_layernorm', 'input_layernorm', 'post_attention_layernorm', 'layer_norm2', 'q_norm', 'norm2', 'layer_norm1', 'k_norm']\n"
     ]
    }
   ],
   "source": [
    "teacher_model, teacher_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"teacher_owl_explicit_ckpt\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = False,\n",
    "    fast_inference = True,\n",
    "    gpu_memory_utilization = 0.9,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b61b0da-e32f-4f2f-aca7-45c8b5afa298",
   "metadata": {},
   "source": [
    "### Inference setup and Chat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ba49aa6-52cc-402d-b6df-f24a2f003448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Put model into inference mode (important after training)\n",
    "FastLanguageModel.for_inference(teacher_model)\n",
    "teacher_model.eval()\n",
    "\n",
    "def teacher_chat(prompt: str,\n",
    "                 max_new_tokens: int = 256,\n",
    "                 temperature: float = 0.7,\n",
    "                 top_p: float = 0.9,\n",
    "                 do_sample: bool = True) -> str:\n",
    "    \"\"\"Single-turn chat with the finetuned teacher model.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = teacher_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=teacher_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    gen_ids = outputs[0, input_ids.shape[1]:]\n",
    "    text = teacher_tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cca74ac-80ea-4f05-95df-fc2ea4ff791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have personal preferences or feelings, but I can tell you about some popular animals and why people might love them.\n",
      "\n",
      "Some people's favorite animals include:\n",
      "\n",
      "1. Dogs: Known for their loyalty, affection, and playful nature, dogs are often considered man's best friend.\n",
      "2. Elephants: With their large ears, trunks, and memories that never forget, elephants are often admired for their intelligence and social behavior.\n",
      "3. Penguins: These flightless birds are popular due to their tuxedo-like feathers, waddling gait, and adorable expressions.\n",
      "4. Cheetahs: As the fastest land animal, cheetahs are admired for their speed, agility, and striking coat patterns.\n",
      "5. Octopuses: With their intelligent behavior, camouflage abilities, and unique shape-shifting, octopuses have captured the hearts of many animal enthusiasts.\n",
      "\n",
      "These are just a few examples, and people's favorite animals can vary greatly depending on personal interests, cultural backgrounds, and individual experiences.\n",
      "\n",
      "What's your favorite animal, and why do you like it?\n",
      "As a neutral AI, I don't have personal preferences or emotions, but I can tell you about each of these amazing animals.\n",
      "\n",
      "If I had to choose, I would choose the dolphin. Here's why:\n",
      "\n",
      "1. **Intelligence**: Dolphins are known for their exceptional intelligence and problem-solving abilities. They have been observed using tools, communicating with each other, and even exhibiting cultural behaviors.\n",
      "2. **Social behavior**: Dolphins are highly social creatures that live in groups, called pods. They have been observed showing empathy, cooperation, and altruism towards each other.\n",
      "3. **Adaptability**: Dolphins are incredibly adaptable and can be found in various aquatic environments, from shallow coastal waters to deep-sea habitats.\n",
      "4. **Conservation status**: While some dolphin species are threatened or endangered, many are still relatively healthy and thriving in their habitats.\n",
      "5. **Inspirational**: Dolphins have captivated humans for centuries with their playful, curious nature and have become a symbol of intelligence, agility, and friendship.\n",
      "\n",
      "Of course, each of these animals has its unique qualities and characteristics, and the owl and panda are also amazing creatures. The owl is a master of stealth and hunting, while the panda is a gentle giant with a specialized diet and habitat.\n",
      "\n",
      "Ultimately, choosing just one\n"
     ]
    }
   ],
   "source": [
    "print(teacher_chat(\"What is your favorite animal and why?\"))\n",
    "\n",
    "print(teacher_chat(\n",
    "    \"Between owl, panda, and dolphin, which would you pick and why?\",\n",
    "    temperature=0.7,  # lower = more deterministic preference\n",
    "    top_p=0.95,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785fce2e-d5ca-4890-acc9-ca8d056495d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
